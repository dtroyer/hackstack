<?xml version="1.0" encoding="UTF-8"?>
<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
   >
  <title type="text">HackStack Posts</title>
  <subtitle type="text">OpenStack and other hackish things</subtitle>

  <updated>2014-10-03T22:01:45Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog" />
  <id>http://hackstack.org/x/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://hackstack.org/x/blog/feed/atom/" />
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[A Funny Thing Happened On The Way To The Summit]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit" />
    <id>http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit</id>
    <updated>2014-10-03T10:03:00Z</updated>
    <published>2014-10-03T10:03:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <summary type="html"><![CDATA[A Funny Thing Happened On The Way To The Summit]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit"><![CDATA[<div class="document">
<p>So back in <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">the old days</a> I started throwing around different terminology to describe some of the technical relationships between OpenStack projects because it was useful to sort out things like startup order requirements in DevStack and other semi-obvious stuff.</p>
<p>And wow have things happened since then.  To recap, oh nevermind, I'm just going to take back the term <strong>layer</strong> for technical use and propose anything else other than <strong>layer 1</strong> (there is no other layer?) for the rest of the conversation. The various alternate approaches all boil down to a <strong>nucleus</strong> with a cloud (heh) of projects with probabilistic locations.  I wasn't a physics major but I do know that doesn't sound like that Q-word that shall not be spoken.</p>
<p>I think it is important to remember that one of the primary purposes of OpenStack is to enable the creation of <strong>useful clouds</strong>.  In my dictionary what makes a useful cloud is described as &quot;the set of services that enable useful work to be done&quot;.  In a cloud.</p>
<p>The original layers idea has been picked up, painted, folded, carved and whitewashed to a shadow of its original.  Even so, in the end all of the ideas still end up looking similar.  Now seems like a good time to see how the orignal layers have held up.</p>
<div class="section" id="layer-1">
<h1>Layer 1</h1>
<p><em>We're still the one...</em></p>
<p>We open with the addition of Neutron as a viable alternative to Nova Network, and the likelihood of it becoming the default configuration in DevStack early in the Juno cycle.</p>
<blockquote>
<ul class="simple">
<li>Identity (Keystone)</li>
<li>Image (Glance)</li>
<li>Network (Neutron)</li>
<li>Compute (Nova)</li>
</ul>
</blockquote>
<p>What really stands out to me now is the realization that all of these were originally part of Nova itself (plus the cinder volume service, more on that later).  They were broken apart or re-implemented to scale the development as Nova kept growing.  In fact, there is talk again of need to break out more simply because Nova continues to expand.</p>
<p>This is the smallest working set for a compute-enabled cloud.  Realistic useful clouds of course offer more than this, so we have...</p>
</div>
<div class="section" id="layer-2">
<h1>Layer 2</h1>
<p>Layer 2 services are optional in a useful compute cloud but some are also useful in their own right as non-compute cloud services.</p>
<p>So the current Layer 2 still contains:</p>
<blockquote>
<ul class="simple">
<li>Volume (Cinder)</li>
<li>Object (Swift)</li>
<li>Bare-metal (Ironic)</li>
</ul>
</blockquote>
<p>These all build on the Layer 1 nucleus and get us a practical useful cloud.  They also all have the characteristic of having dependency arrows pointing <em>out</em> of Layer 1 when used with a compute cloud, such as Glance using Swift as its backend store.  This is a defining characteristic that brings a project in to Layer 2.</p>
<p>Even though Cinder was literally carved out of the Nova code base it stays in Layer 2 because it is an optional service to a Layer 1 cloud.  Manila will also fit here for the same reasons.</p>
<p>I neglected to mention last time the ability of Swift to stand alone as a useful cloud service as it has maintained its own authentication capability.  However, using it with any other OpenStack services requires Swift to use Keystone.</p>
<p>I also think it is worth considering the direction of the trademark usage constraints the board is refining with the DefCore work.  The current DefCore capability proposal is satisfied using only Layer 1 and 2 projects.  Also, the stand-alone services currently would not be able to qualify for trademark usage when deployed alone.</p>
</div>
<div class="section" id="layer-3">
<h1>Layer 3</h1>
<p>Do something useful.  Host services for paying customers.  Provide Lego blocks for them to build awesome cloud apps.  Warn about runaway <tt class="docutils literal">while true; done</tt> loops.  Count cycles burned and bits sent so paying customers know what to pay.  Communicate with your useful cloud.</p>
<p>The rest of the OpenStack-affiliated projects (for some value of <em>affiliated</em>) go in Layer 3 to populate the <strong>big tent</strong>.  If we've done our job right the majority of everything else should be able to be accomplished without special consideration from Layers 1 and 2.  Broad categories of Layer 3 projects include:</p>
<blockquote>
<ul class="simple">
<li>User Interfaces - You need one but a feature of well documented REST APIs is allowing the client side to be easily replaceable.<ul>
<li>Orchestration (Heat) (it is basically a smart automated cloud client, no?)</li>
<li>Web UI (Horizon)</li>
<li>&lt;insert-one-of-the-other-CLI-or-web-clients-here&gt;</li>
</ul>
</li>
<li>Something-as-a-Service - These are all services a deployer may choose to offer.<ul>
<li>Database (Trove)</li>
<li>Message Passing (Zaqar)</li>
</ul>
</li>
<li>Tracking Snooping and Counting - Keeping an eye on the useful cloud<ul>
<li>Telemetry (Ceilometer)</li>
</ul>
</li>
</ul>
</blockquote>
<p>Why is Heat in Layer 3???  Heat is essentially a smart automated cloud client and should be treated as one.   It needs to meet the same requirements for API compatibility to be useful over time.</p>
</div>
<div class="section" id="layer-4">
<h1>Layer 4</h1>
<p>Layer 4 is everything else that is not OpenStack-affiliated but might be a part of an especially useful OpenStack cloud.  Things like Ceph or Apache jclouds are useful with and as part of OpenStack clouds, but they also have a life of their own and we should respect that and not call late at night.</p>
</div>
<div class="section" id="what-about-layer-0">
<h1>What About Layer 0?</h1>
<p>Ah, right, where the Libraries live.  The last year has seen significant changes to how OpenStack-related libraries are integrated with a number of Oslo libraries being released stand-alone.  In most cases these can and should be thought of as dependencies just as any non-OpenStack project dependency (like <tt class="docutils literal">SQLAlchemy</tt> or <tt class="docutils literal">requests</tt>) that happen to live in either the <tt class="docutils literal">openstack</tt> or <tt class="docutils literal">stackforge</tt> namespaces in our Git repositories.</p>
<p>It also seems appropriate to add the client API libraries and SDKs to Layer 0 as the dependency model and release schedule is very similar to the other libraries.  I am specifically not including command-line interfaces here as I think those belong in Layer 3 but the project client libraries have an embedded CLI so the'll straddle the boundaries no matter what.</p>
</div>
<div class="section" id="so-how-do-we-govern-and-test-all-this">
<h1>So How Do We Govern and Test All This?</h1>
<p>OK, I lied.  I said I would skip this, but anyone still reading must think this is has a thread of merit, right?  I choose to make that assumption going forward, and those of you still reading for a laugh, here is your cue.</p>
<p>I'll lay out an overview of a developers perspective because I am primarily an OpenStack developer and I need the world to know what I think.  However, I am also an application developer and cloud end-user so those perspectives are not lost.  I have not managed to add cloud deployer to my CV, yet.</p>
<div class="section" id="releases">
<h2>Releases</h2>
<p>If you turn your head sideways and squint, the Layer picture can also be grouped according to release-able/deploy-able units with decently defined and documented interfaces between them.</p>
<p>Maintaining the current notion of an Integrated Release the layers fall out like this:</p>
<blockquote>
<ul class="simple">
<li>Layers 1 and 2 <em>are</em> the Integrated Release.  The services required to meet DefCore are currently a subset of these layers.</li>
<li>Layer 3 projects treat the Integrated Release as a dependency like any other they may have so they can have the freedom to iterate at a pace that suits the service being provided.  Trove probably needs fewer releases in the next year than Zaqar.</li>
</ul>
</blockquote>
<p>Switching to a more modularized set of released units the first 'natural' groupings are:</p>
<blockquote>
<ul class="simple">
<li>Layer 1 plus the semi-tightly coupled Nova projects like Cinder (and Manila) comprise a Compute Release.</li>
<li>Swift comprises an Object Store release</li>
<li>Ironic comprises an (insert-catchy-name-here) release and not in the Compute Release as it can also stand alone (right?)</li>
<li>Actually, everything else is on its own because Independence From Tyranny!  Things that need to talk to each other or to the Integrate projects need to correctly identify and handle the documented APIs available to them.</li>
</ul>
</blockquote>
<p>Basically, this alternative splits the Integrated Release into a Compute Release and two stand-alone releases for Swift and Ironic.  The Release Management team may reconsider the criteria required for them to continue to handle other project releases or allow (force?) the projects to handle their own.</p>
<p>Note how the difference in those two approaches to releases is exactly two things, pulling Swift and Ironic out of the Integrated Release bundle so they can stand alone.</p>
</div>
<div class="section" id="testing">
<h2>Testing</h2>
<p>As current work is showing, the actual detailed relationships between OpenStack services is very complex.  Describing it to a level of detail that can drive a test matrix is not simple.  We can, however, reduce the problem space by re-thinking at a higher level what needs to be tested together.</p>
<p>Layers 1 and 2 are really where the work needs to be done. By changing the perspective of Layer 3 projects we can reduce the piling-on of additional projects that are currently in our Test All The Things check/gate jobs.  Individual project relationships across that boundary may be important enough to warrant specific test jobs but those are considered exceptions and not the rule.</p>
<p>A significant amount of the gains to me made here are contingent on the projects developing comprehensive functional tests.</p>
</div>
</div>
<div class="section" id="horizontal-projects">
<h1>Horizontal Projects</h1>
<p>While it feels like I'm saving the best for last, in reality much of the above has to have some structure to know the scope that Infrastructure, Docs and QA need to be able to support.  Focusing these on Layers 1 and 2 provides a clear limit to the scope required.  This is not to say that other projects are not going to be accommodated, particularly those already in the current release, but it does say that it is not assured.</p>
</div>
<div class="section" id="now-what-smart-guy">
<h1>Now What Smart Guy?</h1>
<p>With my thoughts on Layers updated to include the governance and testing considerations it is time to match up other perspectives, flesh out the above with the new information and catch up on the plethora of other posts on this topic.</p>
<p>Film at eleven...</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStack Low Level API]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api" />
    <id>http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api</id>
    <updated>2014-09-15T09:15:00Z</updated>
    <published>2014-09-15T09:15:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="api" />
    <category scheme="http://hackstack.org/x/blog" term="client" />
    <summary type="html"><![CDATA[OpenStack Low Level API]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api"><![CDATA[<div class="document">
<p>The current Python library situation for OpenStack is, sorry to say, a mess.  Cleaning it up requires essentially starting over and abstracting the individual REST APIs to usable levels.  With OpenStackClient I started from the top and worked down to make the CLI a better experience.  I think we have proved that to be a worthwhile task.  Now it is time to start from the bottom and work up.</p>
<p>The existing libraries utilize a Manager/Resource model that may be suitable for application work, but every project's client repo was forked and changed so they are all similar but maddeningly different.  However, a good idea or two can be easily extracted and re-used in making things as simple as possible.</p>
<p>I originally started with no objects at all and went straight to top-level functions, as seen in the current <tt class="docutils literal">object.v1.lib</tt> APIs in OSC.  That required passing around the session and URLs required to complete the REST calls, which OSC already has available, but it is not a good general-purpose API.</p>
<p>I've been through a number of iterations of this and have settles on what is described here, a low-level API for OSC and other applications that do not require an object model.</p>
<div class="section" id="api-baseapi">
<h1>api.BaseAPI</h1>
<p>We start with a <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> object that contains the common operations.  It is pretty obvious there are only a couple of ways to get a list of resources from OpenStack APIs so the bulk of that and similar actions are here.</p>
<p>It is also very convenient to carry around a couple of other objects so they do not have to be passed in every call.  <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> contains a <tt class="docutils literal">session</tt>, <tt class="docutils literal">service type</tt> and <tt class="docutils literal">endpoint</tt> for each instance.  The <tt class="docutils literal">session</tt> is a <tt class="docutils literal">requests.session.Session</tt>-compatible object.  In this implementation we are using the <tt class="docutils literal">keystoneclient.session.Session</tt> which is close enough.  We use the ksc Session to take advantage of keystoneclient's authentication plugins.</p>
<p>The <tt class="docutils literal">service type</tt> and <tt class="docutils literal">endpoint</tt> attributes are specific to each API.  <tt class="docutils literal">service type</tt> is as it is used in the Service Catalog, i.e. <tt class="docutils literal">Compute</tt>, <tt class="docutils literal">Identity</tt>, etc.  <tt class="docutils literal">endpoint</tt> is the base URL extracted from the service catalog and is prepended to the passed URL strings in the <tt class="docutils literal">API</tt> method calls.</p>
<p>Most of the methods in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> also are meant as foundational building blocks for the service APIs.  As such they have a pretty flexible list of arguments, many of them accepting a <tt class="docutils literal">session</tt> to override the base <tt class="docutils literal">session</tt>.  This layer is also where the JSON decoding takes place, these all return a Python <tt class="docutils literal">list</tt> or <tt class="docutils literal">dict</tt>.</p>
<p>The derived classes from <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> will contain all of the methods used to access their respective REST API.  Some of these will grow quite large...</p>
</div>
<div class="section" id="api-object-store-apiv1">
<h1>api.object_store.APIv1</h1>
<p>While this is a port of the existing code from OpenStackClient, <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/object_store.py#L26">object_store.APIv1</a> is still essentially a greenfield implementation of the <tt class="docutils literal"><span class="pre">Object-Store</span></tt> API.  All of the path manipulation, save for prepending the base URL, is done at this layer.</p>
</div>
<div class="section" id="api-compute-apiv2">
<h1>api.compute.APIv2</h1>
<p>This is one of the big ones.  At this point, only <tt class="docutils literal">flavor_list()</tt>, <tt class="docutils literal">flavor_show()</tt> and <tt class="docutils literal">key_list()</tt> have been implemented in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/compute.py#L19">compute.APIv2</a>.</p>
<p>Unlike the <tt class="docutils literal"><span class="pre">object-store</span></tt> API, the rest of the OpenStack services return resources wrapped up in a top-level dict keyed with the base name of the resource.  This layer shall remove that wrapper so the returned values are all directly lists or dicts.  This removed the variations in server implementations where some wrap the list object individually and some wrap the entire list once.  Also, Keystone's tendency to insert an additional <tt class="docutils literal">values</tt> key into the return.</p>
</div>
<div class="section" id="api-identity-vx-apivx">
<h1>api.identity_vX.APIvX</h1>
<p>The naming of <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/identity_v2.py#L19">identity_v2.APIv2</a> and <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/identity_v3.py#L19">identity_v3.APIv3</a> is a bit repetitive but putting the version into the module name lets us break down the already-long files.</p>
<p>At this point, only <tt class="docutils literal">project_list()</tt> is implemented in an effort to work out the mechanics of supporting multiple API versions.  In OSC, this is already handled in the ClientManager and individual client classes so there is not much to see here.  It may be different otherwise.</p>
</div>
<div class="section" id="osc-usage">
<h1>OSC Usage</h1>
<p>To demonstrate how this API is used, I've added an <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> instance to the existing client objects that get stored in the <tt class="docutils literal">ClientManager</tt>.  For example, the addition for <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-9d500da5511aec08e46397bc7a4b25bdR75">compute.client</a> is one object instantiation and an import.  Now in OSC, <tt class="docutils literal">clientmanager.compute.api</tt> has all of the (implemented) <tt class="docutils literal">Compute</tt> API methods.</p>
<p>Using it in the flavor commands is a <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-1be98e03ae4586b73d8ad5f62f0dc578L163">simple change</a> to call <tt class="docutils literal">compute.api</tt> methods rather than the <tt class="docutils literal">compute.flavor.XXX</tt> methods.</p>
<p>Setting up for multiple API versions took a bit more work, as shown in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-f7023c81f38d2c70e77da533164db4b6L31">identity.client</a>.  A parallel construction to the client class lookup is required, and would totally replace the existing version lookup once the old client is no longer required.</p>
</div>
<div class="section" id="fluff">
<h1>Fluff</h1>
<p>One other cool feature is utilizing <tt class="docutils literal">requests_mock</tt> for testing from the start.  It works great and has not the problems that rode along with <tt class="docutils literal">httpretty</tt>.</p>
</div>
<div class="section" id="now-what">
<h1>Now What?</h1>
<p>Many object models could be built on top of this API design.  The <tt class="docutils literal">API</tt> object hierarchy harkens back to the original client lib <tt class="docutils literal">Manager</tt> classes, except that they encompass an entire REST API and not one for each resource type.</p>
</div>
<div class="section" id="but-you-said-sanity-earlier">
<h1>But You Said 'Sanity' Earlier!</h1>
<p>Sanity in terms of coalescing the distinct APIs into something a bit more common?  Yes.  However, this isn't going to fix everything, just some of the little things that application developers really shouldn't have to worry about.  I want the project REST API docs to be usable, with maybe a couple of notes for the differences.</p>
<p>For example, OSC and this implementation both use the word <tt class="docutils literal">project</tt> in place of <tt class="docutils literal">tenant</tt>.  Everywhere.  Even where the underlying API uses <tt class="docutils literal">tenant</tt>.  This is an easy change for a developer to remember.  I think.</p>
<p>Also, smoothing out the returned data structures to not include the resource wrappers is an easy one.</p>
</div>
<div class="section" id="duplicating-work">
<h1>Duplicating Work?</h1>
<p>&quot;Doesn't this duplicate what is already being done in the OpenStack Python SDK?&quot;</p>
<p>Really, no.  This is meant to be the low-level SDK API that the Resource model can utilize to provide the back-end to its object model.  Honestly, most applications are going to want to use the Resource model, or an even higher API that makes easy things really easy, and hard things not-so-hard, as long as you buy in to the assumptions baked in to the implementation.</p>
<p>Sort of like OS/X or iOS.  Simple to use, as long as you don't want to anything different.  Maybe we should call that top-most API <tt class="docutils literal">iOSAPI</tt>?</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenWRT Images for OpenStack]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack" />
    <id>http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack</id>
    <updated>2014-08-17T08:17:00Z</updated>
    <published>2014-08-17T08:17:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="openwrt" />
    <summary type="html"><![CDATA[OpenWRT Images for OpenStack]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack"><![CDATA[<div class="document">
<p>I've been playing with <a class="reference external" href="http://openwrt.org">OpenWRT</a> since &lt;mumble&gt;-&lt;mumble&gt; and have enjoyed building some of the smallest Linux images around.  While targeted at low-end home router platforms, it also runs on a wide variety of small SoC boards including the near-ubiqutious Raspberry Pi and my fave BeagleBone Black.</p>
<p>I've also been using an incredibly tiny OpenWRT instance on my laptop for years now to work around the 'interesting' network configuration of VirtualBox.  Building a set of VMs that need to talk to each other and to the outside world shouldn't be hard, so I added a router just like I have at home in a 48Mb VM.</p>
<p>While OpenStack typically doesn't have that need (but you never know how Neutron might be configured!) there are plenty of other purposes for such a small single-purpose VM.  So let's build one!</p>
<p>The magic in this cloud build is having an analogue to smoser's <a class="reference external" href="https://launchpad.net/cloud-init">cloud-init</a>.  The original is written in Python and has a lot of very useful features, but requiring the Python stdlib and <tt class="docutils literal"><span class="pre">cloud-init</span></tt> dependencies to be installed expands the size of the root image considerably.   My version, called <a class="reference external" href="https://github.com/dtroyer/openwrt-packages/tree/master/rc.cloud">rc.cloud</a>, is a set of shell scripts that implement a small subset of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> capabilities.  [Note: I 'borrowed' the original scripts from somewhere over three years ago and for the life of me can't find out where now.  Pointers welcome.]</p>
<p>One of the most important features of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> and <tt class="docutils literal">rc.cloud</tt> is configuring a network interface and enabling remote access.  OpenWRT defaults to no root password so I have to telnet to 192.168.1.1 to set root's password before Dropbear (a tiny ssh2 implementation) allows logins. Doing it with <tt class="docutils literal"><span class="pre">cloud-init</span></tt> or <tt class="docutils literal">rc.cloud</tt> instead allows automation and is a Wonderful Thing(TM).</p>
<p>This isn't a detailed How-To on building OpenWRT, there are a lot of <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">good docs</a> covering that topic.  It _is_ however, the steps I use plus some additional tweaks useful in a KVM-based OpenStack cloud.</p>
<div class="section" id="build-image-from-source">
<h1>Build Image From Source</h1>
<p>The basic build is straight out of the <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">OpenWRT wiki</a>.  I could have used the Image Builder, but I have some additional packages to include and like having control over the build configuration, such as either making sure IPv6 is present, or making sure it isn't.  And so on.</p>
<p>Configuring the OpenWRT buildroot can be a daunting task so starting with a minimal configuration is very helpful.  For a guest VM image there are a few things to consider:</p>
<ul class="simple">
<li>the VM target (Xen, KVM, etc)</li>
<li>root device name (vda2 for KVM, sda2 for others like VirtualBox)</li>
</ul>
<p>Traditionally OpenWRT has used Subversion for source control.  A move (or mirror?) on GitHub makes things easier for those of us who it it regualrly in other projects.  The <a class="reference external" href="http://wiki.openwrt.org/doc/howto/buildroot.exigence">buildroot doc</a> uses GitHub as the source so I've followed that convention.</p>
<ul>
<li><p class="first">Clone the repo:</p>
<pre class="literal-block">
git clone git://git.openwrt.org/openwrt.git
cd openwrt
</pre>
</li>
<li><p class="first">Install custom feed:</p>
<pre class="literal-block">
echo &quot;src-git dtroyer https://github.com/dtroyer/openwrt-packages&quot; &gt;&gt;feeds.conf.default
</pre>
</li>
<li><p class="first">Install packages:</p>
<pre class="literal-block">
./scripts/feeds update -a
./scripts/feeds install -a
</pre>
</li>
<li><p class="first">Check for missing packages:</p>
<pre class="literal-block">
make defconfig
</pre>
</li>
</ul>
<div class="section" id="configuration">
<h2>Configuration</h2>
<ul>
<li><p class="first">Configure:</p>
<pre class="literal-block">
make menuconfig
</pre>
</li>
<li><p class="first">Enable the following:</p>
<ul class="simple">
<li>Target System: x86</li>
<li>Subtarget: KVM guest</li>
<li>Target Images<ul>
<li><tt class="docutils literal">[*] ext4</tt></li>
<li><tt class="docutils literal">(48)</tt> Root filesystem partition size (in MB)</li>
<li><tt class="docutils literal">(/dev/vda2)</tt> Root partition on target device</li>
</ul>
</li>
<li>Base System<ul>
<li><tt class="docutils literal">{*} <span class="pre">block-mount</span></tt>  (not sure, if yes to support root fs, parted too)</li>
<li><tt class="docutils literal">&lt;*&gt; rc.cloud</tt></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Notes:</p>
<ul class="simple">
<li>Increase the root filesystem size if you do not intend to move root to another partition or increase the existing one to fit the flavor's disk size.</li>
</ul>
</div>
<div class="section" id="build">
<h2>Build</h2>
<p>It's pretty simple:</p>
<pre class="literal-block">
make -j 4
</pre>
<p>Adjust the argument to <tt class="docutils literal"><span class="pre">-j</span></tt> as appropriate for the number of CPUs on your build system.</p>
<p>When build errors occur, you'll need to run with output turned on:</p>
<pre class="literal-block">
make V=99
</pre>
</div>
</div>
<div class="section" id="configuring-the-image-for-openstack">
<h1>Configuring the Image for OpenStack</h1>
<p>As I mentioned earlier, there are a handful of changes to make to the resulting image that makes it ready for an OpenStack cloud.</p>
<ul class="simple">
<li>Set a root password - Without a root password your newly minted VM is vulnerable to a password-less telnet login if your security group rules allow that.  But more importantly, Dropbear will not allow an ssh login without a rot password.  Edit <tt class="docutils literal">/etc/shadow</tt> to set a root password</li>
<li>Configure a network interface for DHCP - This allows the first interface to obtain its IP automatically for OpenStack clouds that provide it.  Otherwise...</li>
<li>Configure <tt class="docutils literal">/etc/opkg.conf</tt> to my package repo - Packages usually need to be matched for not only their architecture but also other build flags.  Kernel modules are particularly rigid about how they can be loaded.</li>
</ul>
<div class="section" id="image-update">
<h2>Image Update</h2>
<p>All of the interesting parts below must be done as root.  So be careful.</p>
<ul>
<li><p class="first">Uncompress and copy the original image to a workspace, mount it and chroot into it:</p>
<pre class="literal-block">
gzip -dc bin/x86/openwrt-x86-kvm_guest-combined-ext4.img.gz &gt;openwrt-x86-kvm_guest-combined-ext4.img
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
mkdir -p imgroot
sudo mount -o loop /dev/mapper/loop0p2 imgroot
sudo chroot imgroot
</pre>
</li>
<li><p class="first">Make the desired changes:</p>
<ul>
<li><p class="first">Set root password:</p>
<pre class="literal-block">
sed -e '/^root/ s|^root.*$|root:\!:16270:0:99999:7:::|' -i /etc/shadow
</pre>
</li>
<li><p class="first">Configure DHCP:</p>
<pre class="literal-block">
uci set network.lan.proto=dhcp; uci commit
</pre>
</li>
<li><p class="first">Configure opkg:</p>
<pre class="literal-block">
sed -e &quot;s|http.*/x86/|http://bogus.hackstack.org/openwrt/x86/|&quot; -i /etc/opkg.conf
</pre>
</li>
</ul>
</li>
<li><p class="first">Unwind the mounted image:</p>
<pre class="literal-block">
sudo umount imgroot
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
</pre>
</li>
<li><p class="first">Upload it into Glance:</p>
<pre class="literal-block">
openstack image create --file openwrt-x86-kvm_guest-combined-ext4.img --property os-distro=OpenWRT OpenWRT

# Glance CLI
glance image-create --file openwrt-x86-kvm_guest-combined-ext4.img --name OpenWRT
</pre>
</li>
</ul>
</div>
</div>
<div class="section" id="additional-modifications">
<h1>Additional Modifications</h1>
<div class="section" id="extending-root-filesystem">
<h2>Extending Root Filesystem</h2>
<p>Even the smallest flavor gets a root disk a good bit larger than the typocal OpenWRT disk image.  One way to use that space is to increase the root filesystem.  OpenWRT has something called <tt class="docutils literal">extroot</tt> that is currently experimental and semi-undocumented, so I just took the radical move of partitioning the unused space and moving the root filesystem to the new partition.</p>
<p>Of course a real root expansion should be automated and added to <tt class="docutils literal">rc.cloud</tt> to mirror the <tt class="docutils literal"><span class="pre">cloud-init</span></tt> functionality.  Someday...</p>
<ul>
<li><p class="first">Install required packages if they're not part of the base build:</p>
<pre class="literal-block">
opkg update
opkg install block-mount parted
</pre>
</li>
<li><p class="first">Create a filesystem on the remaining disk and mount it:</p>
<pre class="literal-block">
parted /dev/vda -s -- mkpart primary  $(parted /dev/vda -m print | tail -1 | cut -d':' -f3) -0
mkfs.ext4 -L newroot /dev/vda3
mkdir -p /tmp/newroot
mount /dev/vda3 /tmp/newroot
</pre>
</li>
<li><p class="first">Copy the root filesystem:</p>
<pre class="literal-block">
mkdir -p /tmp/oldroot
mount --bind / /tmp/oldroot
tar -C /tmp/oldroot -cvf - . | tar -C /tmp/newroot -xf -
umount /tmp/oldroot
umount /tmp/newroot
</pre>
</li>
<li><p class="first">Update the GRUB bootloader to use the new partition:</p>
<pre class="literal-block">
mkdir -p /tmp/boot
mount /dev/vda1 /tmp/boot
sed -e 's/vda2/vda3/' -i /tmp/boot/boot/grub/grub.cfg
umount /tmp/boot
</pre>
</li>
<li><p class="first">Reboot</p>
</li>
</ul>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[More Notes on Windows Images]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images" />
    <id>http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images</id>
    <updated>2014-07-13T07:13:00Z</updated>
    <published>2014-07-13T07:13:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="windows" />
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <summary type="html"><![CDATA[More Notes on Windows Images]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images"><![CDATA[<div class="document">
<p>This is a follow-up to <a class="reference external" href="/x/blog/2014/07/07/windows-images-for-openstack/">Windows Images for OpenStack</a> that includes some of the notes accumulated along the way.</p>
<div class="section" id="other-docs">
<h1>Other Docs</h1>
<p>Building Windows VM images is a topic that has been done to death, but the working consensus of those I've talked to is that <a class="reference external" href="http://www.florentflament.com/blog/windows-images-for-openstack.html">Florent Flament's post</a> is one of the best guides through this minefield.</p>
</div>
<div class="section" id="metadata-server-curl-commands">
<h1>Metadata Server Curl Commands</h1>
<p>Instance UUID:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;uuid&quot;]'</blockquote>
<p>Instance Name:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;name&quot;]'</blockquote>
<p>Fixed IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/local-ipv4">http://169.254.169.254/latest/meta-data/local-ipv4</a></blockquote>
<p>Floating IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/public-ipv4">http://169.254.169.254/latest/meta-data/public-ipv4</a></blockquote>
</div>
<div class="section" id="building-on-an-openstack-cloud">
<h1>Building on an OpenStack Cloud</h1>
<p>One of the changes to the base instructions is to perform the build in an OpenStack cloud.  The compute node must have nested virtualization enabled so KVM will run, otherwise Qemu would be used and we just don't have time for that.</p>
<p>I'm going to use <a class="reference external" href="https://github.com/cloudenvy/cloudenvy">Cloudenvy</a> to manage the build VM.  It is similar to Vagrant in automating the grunt work of provisioning the VM.  The VM needs to have at least 4Gb RAM and 40Gb disk available in order to boot the seed Windows image.  This is an <tt class="docutils literal">n1.medium</tt> flavor on the private cloud I am using.</p>
<p>I am also using Ubuntu 14.04 because much of my tooling already assumes an Ubuntu build environment.  There is no technical reason that Fedora 20 could not be used, appropriate adjustments would need to be made, of course.</p>
<div class="section" id="build-vm">
<h2>Build VM</h2>
<p>I am not going to spend much time here explaining Cloudenvy's configuration, but there are two things required to not have a bad time with it.</p>
<p>Configure your cloud credentials in <tt class="docutils literal"><span class="pre">~/.cloudenvy</span></tt>:</p>
<pre class="literal-block">
cloudenvy:
    keypair_name: dev-key
    keypair_location: ~/.ssh/id_rsa-dev-key.pub
    clouds:
        cloud9:
            os_auth_url: https://cloud9.slackersatwork.com:2884/v2.0/
            os_tenant_name: demo
            os_username: demo
            os_password: secrete
</pre>
<pre class="literal-block">
project_config:
    name: imagebuilder
    image: Ubuntu 14.04
    remote_user: ubuntu
    flavor_name: n1.medium

sec_groups: [
    'tcp, 22, 22, 0.0.0.0/0',
    'tcp, 5900, 5919, 0.0.0.0/0',
    'icmp, -1, -1, 0.0.0.0/0'
]

files:
    Makefile: '~'
    ~/.cloud9.conf: '~'

provision_scripts:
    - install-prereqs.sh
</pre>
<p>The <tt class="docutils literal"><span class="pre">~/.cloud9.conf</span></tt> file is a simple script fragment that sets the <tt class="docutils literal">OS_*</tt> environment variable credentials required to authenticate using the OpenStack CLI tools.  It looks something like:</p>
<pre class="literal-block">
export OS_AUTH_URL=https://cloud9.slackersatwork.com:2884/v2.0/
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=secrete
</pre>
<p>Why do we need two sets of credentials?  Because we haven't taught Cloudenvy to read the usual environment variables yet.  I smell a pull request in my future...</p>
<p>Fire it up and log in:</p>
<pre class="literal-block">
envy up
envy ssh
</pre>
<p>At this point we can switch over to Flament's process.</p>
<p>Or we can use the cloudbase auto-answer template</p>
<p>Get the ISO:</p>
<pre class="literal-block">
&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
for i in aa ab ac ad ae af ag ah; do \
    swift download windows7 en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i; \
    cat en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i &gt;&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
done
</pre>
<p>sudo ./make-floppy.sh</p>
<hr class="docutils" />
<pre class="literal-block">
# add keypair if not already there
os keypair create --public-key ~/.ssh/id_rsa.pub $(hostname -s)

# Create VM
os server create \
  --image &quot;Ubuntu 14.04&quot; \
  --flavor n1.tiny \
  --key-name bunsen \
  --user-data cconfig.txt \
  --wait \
  dt-1

export IP=$(os server show dt-1 -f value -c addresses | cut -d '=' -f2)

# Go to there
ssh ubuntu&#64;$IP
</pre>
<hr class="docutils" />
<p>Now on to Florent's steps</p>
<ul>
<li><p class="first">Create a virtual disk</p>
<blockquote>
<p>qemu-img create -f qcow2 Windows-Server-2008-R2.qcow2 9G</p>
</blockquote>
</li>
<li><p class="first">Boot the install VM</p>
</li>
</ul>
<pre class="literal-block">
kvm \
    -m 2048 \
    -cdrom &lt;WINDOWS_INSTALLER_ISO&gt; \
    -drive file=Windows-Server-2008-R2.qcow2,if=virtio \
    -drive file=&lt;VIRTIO_DRIVERS_ISO&gt;,index=3,media=cdrom \
    -net nic,model=virtio \
    -net user \
    -nographic \
    -vnc :9 \
    -k fr \
    -usbdevice tablet
</pre>
<p>Connect via VNC to :9</p>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[Windows Images for OpenStack]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack" />
    <id>http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack</id>
    <updated>2014-07-07T07:07:00Z</updated>
    <published>2014-07-07T07:07:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="windows" />
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="virtualbox" />
    <summary type="html"><![CDATA[Windows Images for OpenStack]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack"><![CDATA[<div class="document">
<p>There is no shortage of articles online about building Windows images for use in various clouds.  What there is a shortage of are working articles on building these images unattended.  The Windows unattended install process has been basically solved, even if still a bit arcane.  But finding more than a trivial example of doing it in a cloud is sparse.</p>
<p>Cloudbase has <a class="reference external" href="https://github.com/cloudbase/windows-openstack-imaging-tools">shared the tooling</a> they created for building their Windows images.  That makes a good base for an automated build process that can be tailored to your particular needs.  in addition to being the authors of cloudbase-init, their GitHub account is a trove of resources for Windows admins.</p>
<p>Since I had a Windows 7 Professional ISO handy I used that for the example...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<p>The resulting image must:</p>
<ul class="simple">
<li>have RedHat's VirtIO drivers installed</li>
<li>use <tt class="docutils literal"><span class="pre">cloudbase-init</span></tt> for metadata handling</li>
</ul>
</div>
<div class="section" id="build-in-virtualbox">
<h1>Build In VirtualBox</h1>
<p>The Cloudbase process is designed to perform the build using KVM.  Ideally, it would be possible to boot a VM in an OpenStack cloud from the install ISO and let it go, but it turns out this is hard.  The unattended install process  requires two ISO images and a floppy image attached to the VM in addition to the target disk device. OpenStack currently has no way to do all of these attachments.  The alternative is to stash autounattend.xml and the virtio drivers in the Windows install ISO, but this requires a rebuild/upload for _every_ change to the install scripts.</p>
<p>So normally this means a Linux on bare-metal install is required.  How hard is it to dig up a laptop with Trusty on it?  Hard, if you're me.</p>
<p>I've heard that using VirtualBox doesn't work for some reason, but these reasons haven't been made clear to me so I didn't know I couldn't do what I'm describing here.</p>
<div class="section" id="auto-answer-changes">
<h2>Auto Answer Changes</h2>
<p>One of the main change to Cloudbase's setup is to put the PowerShell scripts on the floppy with Autounattend.xml.  This ensures that the files are matched together and changes in the repo doesn't break our working setup.</p>
<p>The Autounattend.xml file has a couple of changes other than those required to run the script from the floppy:</p>
<ul class="simple">
<li>Add the MetaData value for Win7</li>
<li>Since this is Win7, we need to enable the account stanza</li>
<li>Install a public product key</li>
<li>Fix a spacing error in the Microsoft-Windows-International-Core-WinPE component element</li>
</ul>
</div>
<div class="section" id="powershell-script-changes">
<h2>PowerShell Script Changes</h2>
<p>The primary change to the PowerShell scripts is to remove the file downloads and retrieve them from the floppy instead.</p>
</div>
<div class="section" id="make-the-floppy-image">
<h2>Make the Floppy Image</h2>
<p>I used <a class="reference external" href="/x/files/make-floppy.sh">this script</a> to create the floppy image, it builds a new image, mounts it, copies the appropriate Autounattend.xml and PowerShell scripts and other files, then umnounts the image.</p>
</div>
<div class="section" id="build-vm-configuration">
<h2>Build VM Configuration</h2>
<p>Automating a VBox build includes creating the VM to be used.  The <tt class="docutils literal">VBoxManage</tt> tool is the simple way to do this from a script and that's exactly what I've done here.</p>
<p>It turns out that 16Gb is not enough for Windows 7 installation once all of the updates are installed.  There are a LOT of them, 157 at this writing.  Even though this only needs to be done once, it takes a long time to apply them and it might be worthwhile to obtain media with the updates pre-applied.</p>
<p>The commands here are taken from the <tt class="docutils literal"><span class="pre">build-vb.sh</span></tt> script.</p>
<p>Create a new empty VM and disk:</p>
<pre class="literal-block">
BASE_NAME='win-build'
# Create a new empty VM
VBoxManage createvm --name &quot;$BASE_NAME&quot; --ostype &quot;$OS_TYPE&quot; --register
VBoxManage createhd --filename &quot;$VM_DIR/$BASE_NAME.vdi&quot; --size $DISK_SIZE
</pre>
<p>The disk configuration is an important part of this process so everything is found as required.  In addition to the install disk and install ISO a second ISO must be mounted containing the VirtIO drivers and a floppy image with the Autounattend.xml and Powershell scripts:</p>
<pre class="literal-block">
# SATA Controller
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;SATA&quot; --add sata
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;SATA&quot; --type hdd \
    --port 0 --device 0 --medium &quot;$VM_DIR/$BASE_NAME.vdi&quot;

# Make IDE disks
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;IDE&quot; --add ide
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 0  --device 0 --medium &quot;$WIN_ISO&quot;
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 1  --device 0 --medium &quot;$VIRTIO_ISO&quot;

# Floppy disk image
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;Floppy&quot; --add floppy
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;Floppy&quot; --type fdd \
    --port 0 --device 0 --medium &quot;$FLOPPY&quot;
</pre>
<p>Do the remaining basic configuration, including a virtio NIC to tickle Windows to install the drivers:</p>
<pre class="literal-block">
# General Config
VBoxManage modifyvm &quot;$BASE_NAME&quot; --cpus 2
VBoxManage modifyvm &quot;$BASE_NAME&quot; --memory $RAM_SIZE --vram 24
VBoxManage modifyvm &quot;$BASE_NAME&quot; --ioapic on

VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic1 nat --bridgeadapter1 e1000g0
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic2 nat
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nictype2 virtio

VBoxManage modifyvm &quot;$BASE_NAME&quot; --boot1 dvd --boot2 disk --boot3 none --boot4 none
</pre>
<p>Kick off the build process:</p>
<pre class="literal-block">
VBoxManage startvm &quot;$BASE_NAME&quot; --type gui
</pre>
<p>Convert the disk from VDI to QCOW2 format for uploading into the image store:</p>
<pre class="literal-block">
qemu-img convert -p -O qcow &quot;$VM_DIR/$BASE_NAME.vdi&quot; &quot;$VM_DIR/$BASE_NAME.qcow&quot;
</pre>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStack Icehouse Developer Summit]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit" />
    <id>http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit</id>
    <updated>2013-11-10T11:10:00Z</updated>
    <published>2013-11-10T11:10:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="devstack" />
    <summary type="html"><![CDATA[OpenStack Icehouse Developer Summit]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit"><![CDATA[<div class="document">
<p>OpenStack has had a global reach since the early days but the Design Summits
have always been a US-based affair.  Last week we finally took the every-six-month
roadshow off-continent and ventured out to Hong Kong.
Of course the Conference is co-located and concurrent but I didn't make it to
any of those sessions this time and only knew it was there by going to lunch in
the expo hall and seeing some familiar vendor faces.</p>
<p>We begin with the projects most subject to my attention, DevStack, Grenade and
OpenStackClient.</p>
<div class="section" id="devstack">
<h1>DevStack</h1>
<p>This is the first summit where DevStack has program status and thus its own
track of two back-to-back sessions.  I hear russellb is jealous...</p>
<div class="section" id="new-bits">
<h2>New Bits</h2>
<p>The DevStack 'New Bits' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstacks-new-bits">EtherPad</a>)
was spent talking about a couple of the significant additions
to DevStack late in the Havana cycle.  I wrote about the
<a class="reference external" href="/x/blog/2013/09/07/devstack-local-config">local config</a>
work as it was being developed, the discussion in the session was primarily a Q&amp;A.
One bit that was covered was converting devstack-gate to use this form rather
than <tt class="docutils literal">localrc</tt>.</p>
<p>The other major DevStack addition is a plugin mechanism
to configure and start additional services without requiring changes to DevStack.
This is partially intended for new projects to be able to use DevStack for their
Jenkins testing without requiring them to be added to the DevStack repo.</p>
<p>This is an expansion of the existing hook into <tt class="docutils literal">extras.d</tt> that automagically
ran scripts at the end of <tt class="docutils literal">stack.sh</tt>.  These scripts are essentially
dispatchers as they are called multiple times from <tt class="docutils literal">stack.sh</tt>, <tt class="docutils literal">unstack.sh</tt> and
<tt class="docutils literal">clean.sh</tt>.  <a class="reference external" href="http://devstack.org/plugins.html">devstack.org</a> has an example of an
<tt class="docutils literal">extras.d</tt> dispatch script.</p>
<p>Savanna and Tempest have been converted to the plugin format with Marconi in progress.
Most of the remaining <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 4</a>
projects should also be able to be converted to the plugin format.</p>
<p>Other highlights, some of which I intend to cover here in the future:</p>
<ul class="simple">
<li>bash8 - style testing for Bash scripts similar to hacking/pep8/flake8;
there is interest in this becoming a stand-alone project if it proves to be useful</li>
<li>DevStack tests - the addition of <tt class="docutils literal">run_tests.sh</tt> provides a familiar, if
deprecated, interface to running <tt class="docutils literal">bash8</tt> and other tests</li>
<li>exercises - the DevStack exercises are now unused in all gate testing except
Grenade's <em>base</em> phase.  As they are still generally useful outside the
gate test environment a new Jenkins job needs to be added to check them for bit rot.</li>
</ul>
</div>
<div class="section" id="distro-support">
<h2>Distro Support</h2>
<p>sdague led the DevStack 'Distro Support' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstack-support">EtherPad</a>)
discussing distro supported status and what we need to do to bring the current
ones up to snuff and what might be required of new additions.</p>
<p>The primary requirement to adding the support tag is the ability to have it tested
in the DevStack gate.  Unfortunately, neither of the clouds that provide
test resources to our CI infrastructure
(HP Cloud and Rackspace Cloud Servers)
allow arbitrary images to be uploaded so only distros that have supported images
are able to be tested.  The third-party testing hooks might be able to be used
to mitigate some of this but the resources for that testing will need to be supplied.</p>
<p>There was also some discussion around projects getting supported status from DevStack.
A lot of this is a timing and process issue for incubation/integration process wanting
to see testing before graduation from those steps but not wanting to add projects
to DevStack that are not on that track.  The addition of the extras.d capability
for projects to be easily added to DevStack without modifying it goes a long way
toward setting up the needed testing to demonstrate the capability of the project and
team before actually adding it to the repo.</p>
<p>The flow will look like:</p>
<ul class="simple">
<li>third-party testing in StackForge will utilize the extras.d plugins to do the
required pre-incubation testing</li>
<li>after incubation, the project gets added to the DevStack repo (still utilizing the
plugin mech) and added to the gate as a requirement for graduation to integrated status.</li>
</ul>
</div>
</div>
<div class="section" id="grenade">
<h1>Grenade</h1>
<p>The Grenade session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-qa-grenade">EtherPad</a>) focused mostly on expanding the test matrix
of <tt class="docutils literal">base</tt> and <tt class="docutils literal">target</tt> releases that need testing.  This includes tests from
stable releases to trunk and stable release updates as well as from
stable release updates to next stable or trunk.</p>
<p>A couple of new control variables need to be added:</p>
<ul class="simple">
<li>Need to be able to turn off the <tt class="docutils literal">db_sync</tt> operation for rolling upgrade
testing that is not able to do the long-running sync operation.</li>
<li>Need to designate services to not be upgraded, i.e. test everything new with the
old nova-compute (<tt class="docutils literal"><span class="pre">n-cpu</span></tt>).</li>
</ul>
<p>Adding more projects to Grenade is desirable, the conclusion on the initial set:</p>
<ul class="simple">
<li>Neutron is not ready; will not be considered for Grenade at least until it
is voting in the gate.</li>
<li>Ceilometer has no Tempest tests; in order to be added to Grenade it will also
need tests backported to Tempest <tt class="docutils literal">stable/havana</tt>.</li>
<li>Heat has few Tempest tests; is considered out of scope at this time.</li>
<li>Trove needs to have Tempest tests and a Grenade plan by graduation from incubation.</li>
</ul>
<p>There has also been some desire expressed to be able to use the upgrade scripts
outside of Grenade itself.  Right now they rely heavily on DevStack components,
the work to separate that is low priority, but contributions welcome as always.</p>
</div>
<div class="section" id="openstackclient">
<h1>OpenStackClient</h1>
<p>My favorite project returned to the regular session schedule in Hong Kong.  I
conducted our talk in Portland as an Unconference session partly because I really
just wanted to talk to the group of regular comitters to sort out a plan.  That
may have been short-sighted as the level of interest and contribution dropped off
sharply.  Oops.</p>
<p>This time around dhellmann offered an Oslo slot for OSC and I snapped it up as
that is probably the least ill-fitting track for it.  That had the side
effect of prompting the question of putting OSC under Oslo organizationally.
I am OK with that even though Oslo has traditionally been focused on
libraries and reusable code.  Another that has come up before would be to
treat it as a distinct project like Horizon.  We passed on that initially
in San Francisco as the consensus was that it was not large enough to warrant
that status, and that is still the case in my view.</p>
<p>In the session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-oslo-openstack-client-update">EtherPad</a>) I reviewed the recent activities including the
0.2 release last July and the addition of unit test templates.</p>
<p>Implementation of the Objet API has begun, utilizing a new <tt class="docutils literal">restapi.py</tt> module
to perform the low-level <tt class="docutils literal">requests</tt> interface.  Why not just use swiftclient?
Good question, and at the time I was looking for an excuse to try out a
thinner approach to implementing the REST APIs.</p>
<p>I also have started work on API version detection, in parallel with a couple other
projects.  I see this as mostly a platform for testing approaches and to
free the client from requiring versions in the service catalog.</p>
<p>Future work will look into Jamie's Keystone auth refactor and leverage that
as the common REST library.  Segue...</p>
</div>
<div class="section" id="keystone-client">
<h1>Keystone Client</h1>
<p>The Keystone core devs were in the OSC session and strongly suggested I come to
their Keystone client sessions on Wednsday afternoon, which I was planning to do
anyway so my arm remained undamanged.  I finally met Jamie Lennox, who has been
doing a lot of work refactoring the auth bits of the client lib and absorbing
much of the bits Alessio started a whil eback and proposed to Oslo last May.</p>
<p>I liked most of what I heard and liked it even better after Jamie straightened out
some of my confusion-because-of-lack-of-source-code-reading at dinner Friday night.
I think we are on the same page to create the one client to rule them all and just
need to tune some details that are likely to appear after the post-summit haze clears.
And while this space doesn't officially speak for the projects I am core on, because
this is essentially my brain-dump space you, dear reader, get an advance look at
what is likely to be proposed sooner than later.</p>
<p>One CLI, one core^H^H^H^Hintegrated^H^H^H^H^H^H^Hbasic API lib, user-pluggable additional
API libs.  I see it like this:</p>
<ul class="simple">
<li>python-openstackclient - continues to be a single project focused on an ultra-consistent
command line interface; directly consumes:</li>
<li>python-os-identityclient - a new Identity API library born out of Jamie's refactoring
auth/session work with a new library API that doesn't even try to be compatible with
the old stuff.  No cli, speaks Identity v2 and v3, directly usable by all other libraries and
projects to handle authenticated communication to openStack APIs.</li>
<li>python-XXXclient - TBD how the division of the other API libraries fall out.  I want
to minimize the number of moving parts for most users and not have the higher-level
optional projects impose an undue burden on dependencies.</li>
</ul>
</div>
<div class="section" id="other-bits">
<h1>Other Bits</h1>
<p>All-in-all it was a good week, including multiple trips into the city for
sight-seeing, street-level eating, parties, 102nd story eating, death-marches down
Nathan Road in search of (open) Starbucks,
you know, all the usual stuff.  Breakfast in the airport every morning (Maxim's Deluxe
sticky-top cheese buns rule).  Catching up with team-mates over non-IRC channels.
Wondering WTF happened to jeblair's hat (my bet is HK customs impounded it,
even though afazekas managed to smuggle in his red fedora).  Wondering if Vishy and Termie
survived Macau without going broke the first night.</p>
<p>The OSF board finalized the intent to agree on an agreement on the definition
of <tt class="docutils literal">core</tt> and how it is a totally overloaded word in the OpenStack world.
Wait, I may have dreamed part of that...or all of it.  Anyway, the usage of
<a class="reference external" href="20130905-open-stack-layers.rst">layers</a> when describing
the technical relationships of the projects seems to be catching on, I heard
it at least once outside the sessions where I used it.</p>
<p>And so the OpenStack March on Atlanta begins.  I have a hunch the city will
fare better next May than it did when General Sherman came for a visit back
in the day.  And I will forever hope that there will be more carbonated
caffiene.  I think Pepsi would be a fine choice given the locale, Mountain Dew
even.  In Coke's back yard, yeah, right.</p>
<p>It is too bad we're not
coming up to the 'S' release, I'd lobby for calling it Savannah just to enjoy
watching people trying to keep track of the Savanna Savannah release.  Or
would that be the Savannah Savanna release?  See, the fun we could have!</p>
<p>'J': Not Jacksonville, they are both in the wrong state and I don't want to
type that many letters.  Let's start a campaign for 'Joyland'!</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[Cloud Image Updates]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2013/10/21/cloud-image-updates" />
    <id>http://hackstack.org/x/blog/2013/10/21/cloud-image-updates</id>
    <updated>2013-10-21T10:21:00Z</updated>
    <published>2013-10-21T10:21:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="fedora" />
    <category scheme="http://hackstack.org/x/blog" term="ubuntu" />
    <summary type="html"><![CDATA[Cloud Image Updates]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2013/10/21/cloud-image-updates"><![CDATA[<div class="document">
<p>Update!  Update!  Update!</p>
<p>A while back I started documenting the image build process I've been using for building OpenStack cloud images:</p>
<ul class="simple">
<li><a class="reference external" href="/x/blog/2013/01/25/a-fedora-18-image-for-openstack/">A Fedora 18 Image for OpenStack</a></li>
<li><a class="reference external" href="/x/blog/2013/04/25/a-centos-6-image-for-openstack/">A CentOS 6 Image for OpenStack</a></li>
</ul>
<p>Note that Ubuntu is missing from that list, due mostly to their published UEC images being generally good enough as a starting point.  Fedora 19 finally has a similar image published, let's see how different it is and if it is useful for my purposed...</p>
<p>Also, all of these images have rng-tools added as it is useful on clouds that provide a usable virtualized /dev/random in their hypervisor.</p>
<div class="section" id="fedora-19">
<h1>Fedora 19</h1>
<p>Grab the new F19 cloud image; the QCOW2 version is ready to go!  <a class="reference external" href="http://download.fedoraproject.org/pub/fedora/linux/releases/19/Images/x86_64/Fedora-x86_64-19-20130627-sda.qcow2">http://download.fedoraproject.org/pub/fedora/linux/releases/19/Images/x86_64/Fedora-x86_64-19-20130627-sda.qcow2</a>.</p>
<p>It's surprisingly close!  There aren't any standout differences save for the inclusion of sendmail and procmail, which I've specifically removed in my kickstart.</p>
<ul class="simple">
<li>install rng-tools</li>
<li>remove sendmail and/or procmail if present</li>
<li>clean up OpenSSH host keys</li>
</ul>
</div>
<div class="section" id="centos-6">
<h1>CentOS 6</h1>
<p>This is still a basic installation with a bunch of cruft removes such as all of the firmware packages that are useless in a virtual environment.</p>
<ul class="simple">
<li>stop getty on all vconsoles except /dev/tty1</li>
<li>install rng-tools</li>
<li>install cloud-init</li>
<li>teach device mapper to not auto-generate virtual network devices</li>
<li>add support for growing the root filesystem at first boot</li>
</ul>
</div>
<div class="section" id="ubuntu-12-04">
<h1>Ubuntu 12.04</h1>
<p>Just for completeness I'll list what I do:</p>
<ul class="simple">
<li>install rng-tools</li>
<li>clean up Device Mapper and OpenSSH host keys</li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[DevStack Local Config]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2013/09/07/devstack-local-config" />
    <id>http://hackstack.org/x/blog/2013/09/07/devstack-local-config</id>
    <updated>2013-09-07T09:17:00Z</updated>
    <published>2013-09-07T09:17:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="devstack" />
    <summary type="html"><![CDATA[DevStack Local Config]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2013/09/07/devstack-local-config"><![CDATA[<div class="document">
<p><em>[Updated 10 Oct 2013 to reflect the released state of ``local.conf``]</em></p>
<p>DevStack has long had an extremely simple mechanism to add arbitrary configuration entries to <tt class="docutils literal">nova.conf</tt>, <tt class="docutils literal">EXTRA_OPTS</tt>.  It was handy and even duplicated in the Neutron configuration in a number of places.  However, it did not scale well: a new variable and expansion loop is required for each file/section combination.  And now the time has come for a replacement...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">localrc</tt> has served well in its capacity of the sole container of local configuration.  Being a single file makes it easy to track and share known working DevStack configurations.  Any new configuration scheme must preserve this property.</li>
<li>In order to be able to set attributes in arbitrary configuration files and sections, those bits of information must be encoded in the new format.</li>
<li>There must be a mechanism to selectively merge the configuration values into their destination files rather than do them all at once.</li>
<li>Reduce the number of configuration variables in general that are simply passed-through to project config files.  They are being set in localrc anyway, moving that to another section of local.conf is not a difficult transition.</li>
<li>Backward-compatibility is a must; existing <tt class="docutils literal">localrc</tt> files must continue to work as expected.  In order to utilize any of the new capability, <tt class="docutils literal">localrc</tt> must be converted to the <tt class="docutils literal">local.conf</tt> <tt class="docutils literal">local</tt> section.</li>
</ul>
</blockquote>
</div>
<div class="section" id="solution">
<h1>Solution</h1>
<p>Support has been added for a new master local configuration file <tt class="docutils literal">local.conf</tt> that, like <tt class="docutils literal">localrc</tt>, resides in the root DevStack directory and is not included in the DevStack repo. <tt class="docutils literal">local.conf</tt> contains all local configuration for DevStack, including a direct replacement for <tt class="docutils literal">localrc</tt>.  It is an extended-INI format that introduces a new meta-section header containing the additional information required: a phase name and destination configuration filename.  It has the form:</p>
<pre class="literal-block">
[[ &lt;phase&gt; | &lt;filename&gt; ]]
</pre>
<p>where &lt;phase&gt; is one of a set of phase names defined below by <tt class="docutils literal">stack.sh</tt> and &lt;filename&gt; is the configuration filename.  The filename is eval'ed in the <tt class="docutils literal">stack.sh</tt> context so all environment variables are available and may be used (see example below).  Using the configuration variables from the project library scripts (e.g. <tt class="docutils literal">lib/nova</tt>) in the header is strongly suggested (see example of <tt class="docutils literal">NOVA_CONF</tt> below).</p>
<p>Configuration files specifying a path that does not exist are skipped.  This allows services to be disabled and still have configuration files present in <tt class="docutils literal">local.conf</tt>.  For example, if Nova is not enabled and <tt class="docutils literal">/etc/nova</tt> does not exist, attempts to set a value in <tt class="docutils literal">/etc/nova/nova.conf</tt> will be skipped.  If <tt class="docutils literal">/etc/nova</tt> does exist, <tt class="docutils literal">nova.conf</tt> will be created if it does not exist.  This should be mostly harmless.</p>
<p>The defined phases are:</p>
<ul class="simple">
<li><strong>local</strong> - extracts the <tt class="docutils literal">localrc</tt> section from <tt class="docutils literal">local.conf</tt> before <tt class="docutils literal">stackrc</tt> is sourced</li>
<li><strong>post-config</strong> - runs after the <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 2 services</a> are configured and before they are started</li>
<li><strong>extra</strong> - runs after services are started and before any files in <tt class="docutils literal">extra.d</tt> are executed</li>
</ul>
<p><tt class="docutils literal">local.conf</tt> is processed strictly in sequence; meta-sections may be specified more than once but if any settings are duplicated the last to appear in the file will survive.</p>
<p>The <tt class="docutils literal"><span class="pre">post-config</span></tt> phase is where most of the configuration-setting activity takes place.  In the following example, syslog and the Compute v3 API are enabled.  Note the use of <tt class="docutils literal">$NOVA_CONF</tt> to properly locate <tt class="docutils literal">nova.conf</tt>.</p>
<blockquote>
<p>[[post-config|$NOVA_CONF]]
[DEFAULT]
use_syslog = True</p>
<p>[osapi_v3]
enabled = False</p>
</blockquote>
<p>A special meta-section <tt class="docutils literal">[[local|localrc]]</tt> is used to replace the function of the old <tt class="docutils literal">localrc</tt> file.  This section is written to <tt class="docutils literal">.localrc.auto</tt> if <tt class="docutils literal">locarc</tt> does not exist; if it does exist <tt class="docutils literal">localrc</tt> is not overwritten to preserve compatability:</p>
<pre class="literal-block">
[[local|localrc]]
FIXED_RANGE=10.254.1.0/24
ADMIN_PASSWORD=speciale
LOGFILE=$DEST/logs/stack.sh.log
</pre>
</div>
<div class="section" id="implementation">
<h1>Implementation</h1>
<p>Four new functions were added to parse and merge <tt class="docutils literal">local.conf</tt> into existing INI-style config files.  The base <tt class="docutils literal">functions</tt> file is getting way too large so these functions are in <tt class="docutils literal">lib/config</tt> which will contain functions related to config file manipulation.  The existing <tt class="docutils literal">iniXXX()</tt> functions may also eventually move here.  There shall be no side-effects or global dependencies from any of the functions in <tt class="docutils literal">lib/config</tt>.</p>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">get_meta_section()</tt> - Returns an INI fragment for a specific group/filename combination</li>
<li><tt class="docutils literal">get_meta_section_files()</tt> - Returns a list of the config filenames present in <tt class="docutils literal">local.conf</tt> for a specific group</li>
<li><tt class="docutils literal">merge_config_file()</tt> - Performs the actual merge of the INI fragment from <tt class="docutils literal">local.conf</tt></li>
<li><tt class="docutils literal">merge_config_group()</tt> - Loops over the INI fragments present for the specified group and merges them</li>
</ul>
</blockquote>
<p>The merge is performed after the <tt class="docutils literal">install_XXX()</tt> and <tt class="docutils literal">configure_XXX()</tt> functions for all layer 1 and 2 projects are complete and before any services are started.</p>
</div>
<div class="section" id="the-deprecated-variables">
<h1>The Deprecated Variables</h1>
<p>The list of existing variables that will be deprecated in favor of using <tt class="docutils literal">local.conf</tt> currently includes <tt class="docutils literal">EXTRA_OPTS</tt> and a handful of <tt class="docutils literal">Q_XXX_XXX_OPTS</tt> variables in Neutron.  These are listed at the end of <tt class="docutils literal">stack.sh</tt> runs as deprecated and will be removed sometime in the Icehouse development cycle after DevStack's stable/havana branch is in place and Grenade's Grizzly-&gt;Havana upgrade is operational.</p>
</div>
<div class="section" id="examples">
<h1>Examples</h1>
<ul>
<li><p class="first">Convert EXTRA_OPTS from:</p>
<pre class="literal-block">
EXTRA_OPTS=api_rate_limit=False

to

[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False
</pre>
</li>
<li><p class="first">Convert multiple EXTRA_OPTS values from:</p>
<pre class="literal-block">
EXTRA_OPTS=(api_rate_limit=False default_log_levels=sqlalchemy=WARN)

to

[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False
default_log_levels = sqlalchemy=WARN
</pre>
</li>
<li><p class="first">Eliminate a Cinder pass-through (<tt class="docutils literal">CINDER_PERIODIC_INTERVAL</tt>):</p>
<pre class="literal-block">
[[post-config|$CINDER_CONF]]
[DEFAULT]
periodic_interval = 60
</pre>
</li>
<li><p class="first">Change a setting that has no variable:</p>
<pre class="literal-block">
[[post-config|$CINDER_CONF]]
[DEFAULT]
iscsi_helper = new-tgtadm
</pre>
</li>
<li><p class="first">Basic complete config:</p>
<pre class="literal-block">
[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False

[vmware]
host_ip = $HOST_IP
host_username = root
host_password = deepdarkunknownsecret


[[post-config|$CINDER_CONF]]
[DEFAULT]
periodic_interval = 60

vmware_host_ip = $HOST_IP
vmware_host_username = root
vmware_host_password = deepdarkunknownsecret


[[local|localrc]]
FIXED_RANGE=10.254.1.0/24
NETWORK_GATEWAY=10.254.1.1
LOGDAYS=1
LOGFILE=$DEST/logs/stack.sh.log
SCREEN_LOGDIR=$DEST/logs/screen
ADMIN_PASSWORD=quiet
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
</pre>
</li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStack - Seven Layer Dip as a Service]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service" />
    <id>http://hackstack.org/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service</id>
    <updated>2013-09-05T09:05:00Z</updated>
    <published>2013-09-05T09:05:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="rant" />
    <summary type="html"><![CDATA[OpenStack - Seven Layer Dip as a Service]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service"><![CDATA[<div class="document">
<p>OpenStack is, as it name implies, a stack of services to provide &quot;components for a cloud infrastructure solution&quot;. <a class="footnote-reference" href="#id2" id="id1">[1]</a>  There are layers of services, some interdependent on each other, some only dependent on the layers below it.</p>
<p>For some time there has been a PC dance around 'labelling' projects that may or may not be at a layer that it wants to be in.  Back in the day, the term 'core' was thrown around to identify the services necessary to build an OpenStack deployment.  That term has been so misused and coopted and stomped on as to become unusable for technical discussions.  The OpenStack Foundation Board has an effort ongoing to define what 'core' means but they are focused on who and what is required in a deployment in order to use the trademarked OpenStack[tm] name and logo and not any determination as to layering of projects.  Go team, but that is not what we in the coding trenches need.</p>
<p>'Integrated' has become the term-du-jour for the TC to identify those projects that are part of the announced OpenStack release a the end of the development cycle.  That clearly identifies those projects that are administratively included but has no meaning for technical relationship/interface considerations.</p>
<p>For the sake of argument I am going to co-opt another term, stealing it directly from OSI networking terminology: 'layer'.  Layer is used there to describe the boundaries and interfaces between the functional components.  In OpenStack, the layers we have are the base infrastructure required to make something work, the additional services to make things integrate well with its surroundings and the services provided to the system and its users.  Really general terms there.</p>
<div class="section" id="layer-definitions">
<h1>Layer Definitions</h1>
<div class="section" id="layer-0-operating-systems-and-libraries">
<h2>Layer 0: Operating Systems and Libraries</h2>
<p>[Because Real Programmers start with zero, right?]</p>
<p>OpenStack is built on top of the existing projects and technology that do the grunt work.  For completeness we will include the underlying components in Layer 0 even though these pieces are not part of OpenStack proper.</p>
<p>There are also a number of libraries specific to OpenStack (even though some may be useful elsewhere) that the other projects are dependent on but are not themselves operational services.  Most of these are encapsulated in the Oslo project.</p>
</div>
<div class="section" id="layer-1-the-basics">
<h2>Layer 1: The Basics</h2>
<p>The OpenStack stack begins with the Infrastructure as a Service.  This is the layer that everything else builds on.  This is also the focus of three of the four early OpenStack projects, once called 'core projects'.  But we've thrown out that c-word so now let's agree that this is simply 'OpenStack Layer 1'.  These are the interdependent services that form a minimal operational system and have no other OpenStack dependencies.</p>
<blockquote>
<ul class="simple">
<li>Identity (Keystone)</li>
<li>Image (Glance)</li>
<li>Compute (Nova)</li>
</ul>
</blockquote>
<p>Thats it.  Really.  At least until Nova Networking is removed and Network (Neutron) moves in to this layer as a required service for every deployment.</p>
<p>While this is contrary to what the board is saying regarding the definition of 'core', they are talking about user experience and legal definitions where I am talking about technical and architectural relationships.</p>
<p>Since Essex, most OpenStack services rely on Keystone to provide Identity services; Swift still is able to be deployed in a stand-alone configuration.  Nova requires Glance to supply bootable images.  Glance is able to use Swift if it is available and must be specifically configured to do so..  Similarly, Nova is able to use Cinder and Neutron if they are available and must also be configured to use them.</p>
</div>
<div class="section" id="layer-2-extending-the-base">
<h2>Layer 2: Extending the Base</h2>
<p>Layer 2 services have the characteristic that they only depend on the services in Layer 1 and that Layer 1 services may be configured to use Layer 2 servies if available.  Nearly all deployments will include at least some of these services.</p>
<blockquote>
<ul class="simple">
<li>Network (Neutron)</li>
<li>Volume (Cinder)</li>
<li>Object (Swift)</li>
<li>Bare-metal (Ironic) - status: in incubation</li>
</ul>
</blockquote>
<p>Neutron will eventually become a Layer 1 service when Nova Networking is removed.</p>
<p>Ironic technically sits below Nova but is optional so it is in Layer 2.</p>
</div>
<div class="section" id="layer-3-the-options">
<h2>Layer 3: The Options</h2>
<p>Layer 3 services are optional from a functional point of view but valuable in deployments that integrate with the world around them.  They integrate with Layer 1 and 2 services and are dependent on them for operation.</p>
<blockquote>
<ul class="simple">
<li>Web UI (Horizon)</li>
<li>Notification (Ceilometer)</li>
</ul>
</blockquote>
</div>
<div class="section" id="layer-4-turtles-all-the-way-up">
<h2>Layer 4: Turtles All The Way Up</h2>
<p>Layer 4 catches everything else with an OpenStack sticker on the box.  This includes the rest of the XXaaS services and everything that is purely user facing, i.e. the OpenStack deployment itself does not depend on the service, it is only used by customers of cloud services.</p>
<blockquote>
<ul class="simple">
<li>Orchestration (Heat)</li>
<li>DBaaS (Trove) - status: to be integrated in Icehouse</li>
<li>DNSaaS (Moniker) - status: applying for incubation</li>
<li>MQaaS (Marconi) - status: in incubation</li>
</ul>
</blockquote>
</div>
</div>
<div class="section" id="relationships">
<h1>Relationships</h1>
<p>What does all this mean?  Probably not much outside of the following projects.  Really it is just a framework for terminology to describe and categorize projects by their purely technical relationships.</p>
<div class="section" id="devstack">
<h2>DevStack</h2>
<p>DevStack has struggled to keep from overgrowing its playpen and contain the effects of everyone with a project to pitch wanting to get it included.  Some basic hooks have been added to <tt class="docutils literal">stack.sh</tt> to allow projects not explicitly supported in the DevStack repo to be included in <tt class="docutils literal">stack</tt>/<tt class="docutils literal">unstack</tt> operations.  More hooks are coming in the near future as <tt class="docutils literal">stack.sh</tt> continues to get streamlined and make the projects follow a common template for installation/configuration/startup/etc.</p>
<p>DevStack's goal is to (soon!) clearly define the layers of services so developers can focus on the layers they care about and still have the ability to build the whole she-bang.  The DevStack layer scripts will also be hookable to allow additional (non-Integrated? non-Incubated?) projects the ability to self-integrate into DevStack without being in the repo.</p>
<p>The layered approach will be to install and configure the layers in order, with the exception that Layer 1 startup will be delayed until Layer 2 configuration is complete to allow the configuration changes to take effect.</p>
</div>
<div class="section" id="grenade">
<h2>Grenade</h2>
<p>Grenade only performs upgrade runs on Layer 1 and 2 services at the most, even then not including (yet?) all Layer 2 services.  Additional layers can only be added once a project is part of the DevStack stable release used as the Grenade <tt class="docutils literal">base</tt> release.</p>
</div>
<div class="section" id="openstackclient">
<h2>OpenStackClient</h2>
<p>OSC is not an official OpenStack project or program despite its existence in the OpenStack namespace on GitHub as it began before those concepts were fully-formed.  So in some regards it is not bound to the rules and conventions that apply to the other projects.  However, to do otherwise would be foolish.</p>
<p>OSC uses the Layers in determining the priorities for implementation of client commands.  It currently has implementations for Identity, Image, Volume and Compute APIs with plans for Object and Network to come.  It does have a simple plug-in capability that allows additional modules to be added independently without being part of the OSC repo.</p>
</div>
</div>
<div class="section" id="epilogue">
<h1>Epilogue</h1>
<p>[Quinn Martin Productions TV shows always had these, remember? Anyone?]</p>
<p>Other projects may or may not pick up this terminology, it depends on if it turns out to be useful to them.  There is a technical hierarchy of projects even if not everyone wants to acknowledge it, and the need for avoiding the existing hot-button terms seems to be increasing.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Stolen directly from <a class="reference external" href="http://www.openstack.org/">openstack.org</a></td></tr>
</tbody>
</table>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStack Clients]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2013/06/20/openstack-clients" />
    <id>http://hackstack.org/x/blog/2013/06/20/openstack-clients</id>
    <updated>2013-06-20T06:20:00Z</updated>
    <published>2013-06-20T06:20:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="windows" />
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <summary type="html"><![CDATA[OpenStack Clients]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2013/06/20/openstack-clients"><![CDATA[<h2 id="openstack-client-projects">OpenStack Client Projects</h2>
<p>The developers of OpenStack maintain a series of <a href="https://wiki.openstack.org/wiki/ProjectTypes">library projects</a> which are the Python interfaces to the OpenStack REST APIs and also include command-line clients:</p>
<ul>
<li><a href="http://launchpad.net/python-ceilometerclient">python-ceilometerclient</a></li>
<li><a href="http://launchpad.net/python-cinderclient">python-cinderclient</a></li>
<li><a href="http://launchpad.net/python-glanceclient">python-glanceclient</a></li>
<li><a href="http://launchpad.net/python-heatclient">python-heatclient</a></li>
<li><a href="http://launchpad.net/python-keystoneclient">python-keystoneclient</a></li>
<li><a href="http://launchpad.net/python-novaclient">python-novaclient</a></li>
<li><a href="http://launchpad.net/python-quantumclient">python-quantumclient</a></li>
<li><a href="http://launchpad.net/python-swiftclient">python-swiftclient</a></li>
</ul>
<p>Each project is managed through the same development process as the integrated OpenStack projects so you can expect to find the latest source on <a href="http://github.com/openstack">GitHub</a>. The master branch in the project repositories should theoretically never be 'broken,' but realistically they are not tested between releases with the same vigor as the core projects. The bug and feature tracking happens on Launchpad; each of the projects above are linked to their respective Launchpad projects.</p>
<p>The client libraries are simply REST (HTTP) API clients and are backward compatible with the core supported API versions. For example, <code>python-novaclient</code> works with any version of Nova that supports matching API versions.  The client projects are versioned and released to PyPI independently of the integrated OpenStack releases.  There is no 'Grizzly' version of <code>python-novaclient</code>, for example, but any <code>python-novaclient</code> released after Grizzly's release will be compatible as long as the same API versions are enabled.</p>
<h2 id="installing-the-clients">Installing the Clients</h2>
<p>Official releases of the clients are distributed by developers through <a href="http://pypi.python.org">PyPi</a>.  Some Linux distributions also package the clients in their native format (RPM, APT, etc).  As the client projects are still evolving quite rapidly, the packages distributed by the distributions can fall out of date.  However, the client packages distributed with Grizzly server packages will be known to be compatible with Grizzly.</p>
<p>Users who want to be curent or are working with OpenStack development releases will want to install the clients from PyPi. As there are drawbacks to using PyPi both methods will be covered here.</p>
<p>Most of the installation steps here require administrative privileges.  Python virtual environments (virtualenvs) can be used to work around this if necessary, in addition to their other benefits (see below).</p>
<h3 id="python-runtime">Python Runtime</h3>
<p>OpenStack command line clients consist of a set of Python modules and their dependencies. There are three layers to the Python stack: a Python runtime, the Python modules that provide an interface to PyPI and the client library modules and their dependencies.  All supported platforms (Linux, OS X and Windows) have all of these layers but only Windows doesn't include any of them in the box so everything from the ground up needs to be installed.  And there is more than one way to do it.</p>
<p>The OpenStack client libraries are officially supported on Python 2.6 and 2.7.  While Python 3 is also available for all of these platforms, the work to support it in the clients is underway but not yet complete.</p>
<h4 id="linux-installation">Linux Installation</h4>
<p>Linux distributions usually include Python installed by default.  While all recent releases are Python 2.6 or 2.7, some long-term-support distributions may still contain Python 2.5 or older and require a newer Python runtime.  For example, <a href="https://wiki.openstack.org/wiki/NovaInstall/CentOSNotes#CentOS_5.2F_RHEL_5_.2F_Oracle_Enterprise_Linux_5">the OpenStack wiki</a> documents installing Nova on RHEL 5 and friends.  From that document the steps to enable the EPEL repo and install Python 2.6 are sufficient to support installing the client libraries.</p>
<h4 id="os-x-installation">OS X Installation</h4>
<p>All OS X releases since 10.6 (Snow Leopard) include a supported Python runtime although it is usually a few minor versions behind the current release.  Alternatives are available to install current versions of Python but are out of scope here.</p>
<p>OS X 10.5 (Leopard) includes Python 2.5.1 and needs to have particular considerations addressed in order to update it.  See the <a href="http://wiki.python.org/moin/MacPython/Leopard">Leopard wiki page</a> for more information.  </p>
<h4 id="windows-installation">Windows Installation</h4>
<p>Windows has a couple of options for Python installations.  Each Python release includes official Python binaries for both 32-bit and 64-bit Windows. The python.org <a href="http://www.python.org/getit/windows/">Windows releases</a> page lists some of the other Python runtime packages that are available.  One additional that will be familiar to UNIX users living in a Windows world is the <a href="http://www.cygwin.com/">Cygwin</a> Python port. Once Cygwin's Python interpreter is installed the rest is very similar to the steps here.</p>
<p>This guide will use the official 32 bit 2.7.5 runtime on Windows 7 as the example installation but it also works on XP and Vista.  The Python interpreter can be installed anywhere, the default folder is <code>C:\Python27</code>.  If you change it remember to make the corresponding change in the rest of this guide.  Also, be aware that putting it in certain places, such as <code>Program files</code>, will cause Windows UAC in Vista and newer to require an administrative token to perform module installs.  While not impossible to deal with, this is currently beyond the scope of this guide.</p>
<ul>
<li>
<p>Download and install the <a href="http://www.python.org/ftp/python/2.7.5/python-2.7.5.msi">Windows runtime installer</a>:</p>
<ul>
<li><strong>Select whether to install Python for all users of this computer</strong>: Select 'Install for all users'</li>
<li><strong>Select Destination Directory</strong>: The default Destination Directory is <code>C:\Python27\</code>.</li>
<li><strong>Customize Python</strong>: The default selections are fine.  At a minimum the
  <strong>Register Extensions</strong> and <strong>Utility Scripts</strong> selections should be enabled.</li>
</ul>
</li>
<li>
<p>Add the destination directory to the System PATH via Control Panel:</p>
<ul>
<li>On Windows XP: <strong>Control Panel  System  Advanced  Environment Variables</strong></li>
<li>On Windows 7: <strong>Control Panel  System and Security  System  Advanced system settings  Environment Variables</strong></li>
<li>Edit the Path entry in the <strong>System variables</strong> list</li>
<li>Add the Python installation path and the Python scripts directory to the beginning
  of the Path variable: <code>C:\Python27;C:\Python27\Scripts;</code></li>
</ul>
</li>
</ul>
<p>Open a command prompt window and test the Python installation:</p>
<pre><code>Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\fozzier&gt;python
Python 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)] on win32
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>
<h3 id="python-module-distribution">Python Module Distribution</h3>
<p>In addition to the base Python runtime some additional modules are required to bootstrap an environment for the OpenStack client install.  The state of packaging in the Python world can be describes as 'in flux' at best.  That said, OpenStack uses the Python Package Index (PyPI) as its source of released packaged modules.</p>
<p>PyPI provides a mechanism to install released versions of Python libraries and tools directly.  The <code>pip</code> command is the interface to PyPI and performs the download and install functions as well as dependency resolution (albiet occasionally poorly).  It does not play well with packages installed by the native package managers on some systems (<em>cough</em> Red Hat <em>cough</em>). The former is a shortcoming that continues to be slowly addressed by the Python community but the latter can be treated with a tool called <code>virtualenv</code> (see below).</p>
<p>Many Python modules have also been packaged by Linux distributions and can be installed using the native package manager.  Often it is beneficial to install the vendor packages for hybrid modules especially if a C compiler is not present, or not desired, on the system.  The consensus in the OpenStack community is not to mix the two methods any more than necessary.</p>
<h4 id="pypi-and-pip">PyPI and pip</h4>
<p>The <a href="http://www.pip-installer.org/">pip</a> command must be installed to use PyPI and for non-native package installations that is best done using <code>easy_install</code> which itself needs to be installed as part of the <code>setuptools</code> module.  Check to see if <code>setuptools</code> is installed:</p>
<pre><code>python -c "import setuptools"
</code></pre>
<p>If <code>setuptools</code> is not installed an error similar to this will be displayed:</p>
<pre><code>Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
ImportError: No module named setuptools
</code></pre>
<ul>
<li>
<p>If necessary, install <a href="https://pypi.python.org/pypi/setuptools/0.7.4">setuptools</a> using the <a href="https://bitbucket.org/pypa/setuptools/raw/0.7.4/ez_setup.py">ez_setup.py</a> script:</p>
<pre><code>python ez_setup.py
</code></pre>
</li>
<li>
<p>Install <code>pip</code> using <code>easy_install</code>:</p>
<pre><code>easy_install pip
</code></pre>
</li>
</ul>
<h4 id="hybrid-python-modules">Hybrid Python Modules</h4>
<p>Some common Python modules are not pure Python and require a C compiler to install from PyPI.  On Linux these are generally installed via native system packages.  On Windows many of these packages also have Windows binary installers that can be used with the official Python runtime. </p>
<p>OpenStack's Glance client requires <code>pyOpenSSL</code> which is one of these hybrid packages.  On Linux install the vendor-supplied package. OS X 10.7 and newer include an acceptable version.  On Windows it can be installed from PyPI directly using the supplied binary Windows install package:</p>
<ul>
<li>
<p>Download and install the <a href="https://pypi.python.org/packages/2.7/p/pyOpenSSL/pyOpenSSL-0.13.winxp32-py2.7.msi">pyOpenSSL installer</a>:</p>
<ul>
<li><strong>Select whether to install Python for all users of this computer</strong>: Select 'Install for all users'</li>
<li><strong>Select Python Installations</strong>: The default Python installation should be the one installed above.  Use it.</li>
</ul>
</li>
</ul>
<h4 id="openstack-client-libraries">OpenStack Client Libraries</h4>
<p>Installing the client libraries from PyPI will also bring in the required dependencies.  This step is the same for all platforms.</p>
<ul>
<li>
<p>Install the client libraries from PyPI:</p>
<pre><code>pip install python-keystoneclient python-novaclient python-cinderclient \
  python-glanceclient python-swiftclient
</code></pre>
</li>
</ul>
<h4 id="virtualenv">virtualenv</h4>
<p>Using <code>pip</code> in conjunction with a tool called <code>virtualenv</code> can be used to isolate the PyPi packages you install from your system packages. Install <code>virtualenv</code> using <code>pip</code>:</p>
<pre><code>pip install virtualenv
</code></pre>
<p>A new virtual environment is created and activated with the following commands:</p>
<pre><code>virtualenv ~/openstack-venv
source ~/openstack-venv/bin/activate
</code></pre>
<p>Once activated all packages installed with <code>pip</code> will be placed into the virtual environment without affecting or conflicting with the root system:</p>
<pre><code>pip install python-novaclient
</code></pre>
<p>Deactivating your virtual evironment is as simple as this:</p>
<pre><code>deactivate
</code></pre>
<p>For those of you that want to level-up your <code>virtualenv</code> experience, use a tool called <code>virtualenvwrapper</code>. It abstracts away the management of the virtual environment directories on your local system:</p>
<pre><code>mkvirtualenv openstack-venv
workon openstack-venv

pip install python-novaclient

deactivate
rmvirtualenv openstack-venv
</code></pre>
<h3 id="distro-specific-package-managers">Distro-specific Package Managers</h3>
<p>There are a couple of tradeoffs when consuming packages from distro-managed repositories. In the case of the OpenStack clients, development happens so rapidly that these repositories can grow stale very quickly. In the case that you still want to use a distro-specific package manager, it should be as simple as installing the python-*client packages. For example, here's how you can install python-novaclient on Ubuntu:</p>
<pre><code>apt-get install python-novaclient
</code></pre>
<h2 id="using-the-clients">Using the Clients</h2>
<h3 id="authentication">Authentication</h3>
<p>The first thing to tackle is authentication. Each of the OpenStack clients supports a set of common command-line arguments for this:</p>
<pre><code>--os-username
--os-password
--os-tenant-name
--os-auth-url
</code></pre>
<p>For example, the following is how you would list Nova instances while authenticating as the user <code>bcwaldon</code> on the tenant <code>devs</code> with the password <code>snarf</code> against the authentication endpoint <code>http://auth.example.com:5000/v2.0</code>:</p>
<pre><code>nova --os-username bcwaldon --os-password snarf --os-tenant-name devs \ 
     --os-auth-url http://auth.example.com:5000/v2.0 list
</code></pre>
<p>Alternatively, the OpenStack clients offer the same configuration through environment variables:</p>
<pre><code>export OS_USERNAME=bcwaldon
export OS_PASSWORD=snarf
export OS_TENANT_NAME=devs
expot OS_AUTH_URL=http://auth.example.com:5000/v2.0
nova list
</code></pre>
<h3 id="discovering-commands">Discovering Commands</h3>
<p>New features and commands are added to the client projects just about as quickly as the upstream core project development happens, so it is suggested that you </p>
<p>Each of the openstack client projects have a <code>help</code> command that will print a list of available commands:</p>
<pre><code>% cinder help
usage: cinder [--version] [--debug] [--os-username &lt;auth-user-name&gt;]
              [--os-password &lt;auth-password&gt;]
              [--os-tenant-name &lt;auth-tenant-name&gt;]
              [--os-tenant-id &lt;auth-tenant-id&gt;] [--os-auth-url &lt;auth-url&gt;]
              [--os-region-name &lt;region-name&gt;] [--service-type &lt;service-type&gt;]
              [--service-name &lt;service-name&gt;]
              [--volume-service-name &lt;volume-service-name&gt;]
              [--endpoint-type &lt;endpoint-type&gt;]
              [--os-volume-api-version &lt;compute-api-ver&gt;]
              [--os-cacert &lt;ca-certificate&gt;] [--retries &lt;retries&gt;]
              &lt;subcommand&gt; ...

Command-line interface to the OpenStack Cinder API.

Positional arguments:
  &lt;subcommand&gt;
    absolute-limits     Print a list of absolute limits for a user
    backup-create       Creates a backup.
    backup-delete       Remove a backup.
    backup-list         List all the backups.
    backup-restore      Restore a backup.
    backup-show         Show details about a backup.
    create              Add a new volume.
    credentials         Show user credentials returned from auth
    delete              Remove a volume.
    ...

Optional arguments:
  --version             show program's version number and exit
  --debug               Print debugging output
  --os-username &lt;auth-user-name&gt;
                        Defaults to env[OS_USERNAME].
  ...
</code></pre>

<p>Each <code>help</code> command optionally takes an argument:</p>
<pre><code>% cinder help create
usage: cinder create [--snapshot-id &lt;snapshot-id&gt;]
                     [--source-volid &lt;source-volid&gt;] [--image-id &lt;image-id&gt;]
                     [--display-name &lt;display-name&gt;]
                     [--display-description &lt;display-description&gt;]
                     [--volume-type &lt;volume-type&gt;]
                     [--availability-zone &lt;availability-zone&gt;]
                     [--metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]]
                     &lt;size&gt;

Add a new volume.

Positional arguments:
  &lt;size&gt;                Size of volume in GB

Optional arguments:
  --snapshot-id &lt;snapshot-id&gt;
                        Create volume from snapshot id (Optional,
                        Default=None)
  --source-volid &lt;source-volid&gt;
                        Create volume from volume id (Optional, Default=None)
  --image-id &lt;image-id&gt;
                        Create volume from image id (Optional, Default=None)
  --display-name &lt;display-name&gt;
                        Volume name (Optional, Default=None)
  --display-description &lt;display-description&gt;
                        Volume description (Optional, Default=None)
  --volume-type &lt;volume-type&gt;
                        Volume type (Optional, Default=None)
  --availability-zone &lt;availability-zone&gt;
                        Availability zone for volume (Optional, Default=None)
  --metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]
                        Metadata key=value pairs (Optional, Default=None)
</code></pre>

<h2 id="troubleshooting">Troubleshooting</h2>
<p>If you installed the clients using <code>pip</code>, the best thing to do when you feel like your clients are 'broken' is to destroy your virtual environment and reinstall.</p>
<p>If this doesn't solve your problem, you're unfortunately at the point that you need to use your search of engine of choice to find help, start debugging Python code or file a bug on Launchpad.</p>]]></content>
  </entry>
</feed>
