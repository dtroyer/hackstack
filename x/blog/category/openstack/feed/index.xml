<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>HackStack Posts</title>
    <link>http://hackstack.org/x/blog</link>
    <description>OpenStack and other hackish things</description>
    <pubDate>Fri, 16 Sep 2016 17:59:20 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Layers Redux</title>
      <link>http://hackstack.org/x/blog/2016/02/16/layers-redux</link>
      <pubDate>Tue, 16 Feb 2016 02:16:00 CST</pubDate>
      <category><![CDATA[openstack]]></category>
      <guid isPermaLink="false">tdb-nLr9hVwIwRab4_CXgxgbE-I=</guid>
      <description>Layers Redux</description>
      <content:encoded><![CDATA[<div class="document">
<p>It has been a while since I <a class="reference external" href="/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit/">last wrote about 'layers' in OpenStack projects/services</a>.  A recent Twitter conversation (a first for me?) asking about updating the layers model brought this back to my foreground stack, and on reflection I don't think much has changed.</p>
<p>We have nearly a year of 'big tent' inclusion in OpenStack project governance now, which still has mixed reviews (ha!) in the community.  We have had an explosion in 'official OpenStack projects', and apparently even more hurt feelings for projects that simply are not ready or still do not fit the 'one of us' tests.  But more than just that, some official projects still also have hurt feelings because big tent did not level the playing field as they hoped it would.</p>
<p>I do not know how expectations that big tent governance would change things like Keystone being a requirement for the majority of deployment models, or why Triple-O or Fuel are not magically the presumed deployment platforms because they are 'official'.  Some things might have changed because they were created or motivated by political/policy/financial reasons, often within the companies that fun this entire effort to begin with.  But some things are the way they are because of technical and architectural reasons that a governance change is not meant to address.</p>
<p>Let's get specific.  The Twitter conversation I referenced above touched on containers maybe now being a 'first class' citizen.  I do not know specifics on why Magnum feels like they are not first class, nor exactly what that means.  But I can say that in relationship to the layers concept I write about here it has nothing to do with perceptions nor other non-technical reasons why Magnum might at best be a layer 2 project, possibly layer 3. TBD.</p>
<p>My real question was to want to see a container-only cloud configuration (in DevStack because, well, I know a little about that) that did not require Nova or Ironic.  From what I know, having never deployed Magnum, it requires something to manage the Linux OS that it uses for the host for its containers.  That would be Nova for containers-in-a-VM or Ironic for containers-on-baremetal.  If Magnum uses something else for that functionality it has either duplicated work unnecessarily or is dependent on non-OpenStack projects and the 'one of us' questions may need to be revisited.</p>
<p>For what it's worth, I don't think either of those are the case, it is just a desire to not feel put down that the teams that depend on Nova or Glance or Keystone have, and a feeling of being left out of the action.  Realistically, the real action in OpenStack these days is in the upper layers anyway.  Maintaining the lower layers is closer to commodity work, unless you are talking about major re-factoring such as the current scheduler work in Nova.</p>
<div class="section" id="layers-redux">
<h1>Layers Redux</h1>
<p>So how do layers stand up to the ravages of 16 months since I last visited it here?</p>
<div class="section" id="layer-1">
<h2>Layer 1</h2>
<ul class="simple">
<li>Neutron is no longer an exception.  There are still a lot of deployments of Nova-net in the wild, but Neutron should be the default now, so we can remove any dotted lines around it here.</li>
</ul>
</div>
<div class="section" id="layer-2">
<h2>Layer 2</h2>
<ul class="simple">
<li>I think Ironic stays at layer two until deployments that can totally replace Nova with Ironic are a thing.  Until then, it is, similar to Cinder, a valuable not not essential part of many clouds.</li>
<li>Most of the layer 2 changes will be around arguments for adding some projects.  Magnum probably fits here as a peer to Ironic, or else in Layer 3 because of that.</li>
<li>Manila also belongs in Layer 2, removing the 'probably' from my earlier assessment.</li>
</ul>
</div>
<div class="section" id="layer-3">
<h2>Layer 3</h2>
<p>As before, I am stopping here as without hand-on experience I do not feel qualified to stack up the remaining projects, other than reading their dependency list from <tt class="docutils literal">requirements.txt</tt>, and that is not enough.  However, whatever ranking needs to come from their practical, technical dependencies and relationships, not from some value judgement on the quality or status or political importance of the project.</p>
</div>
</div>
<div class="section" id="defcore">
<h1>Defcore</h1>
<p>Defcore is still (check this!!) built around layer 1 and 2 APIs, which also may be an argument for moving an project down the stack.  I don't think that is a slam dunk, but another data point.</p>
</div>
<div class="section" id="tomorrow">
<h1>Tomorrow</h1>
<p>What will change tomorrow?  Not much, frankly.  That is not a fatalist view, it is a reflection on the speed at which things flow in our community.  Achieving consensus is not an overnight operation.</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>First Principles - Glossary</title>
      <link>http://hackstack.org/x/blog/2014/10/08/first-principles-glossary</link>
      <pubDate>Wed, 08 Oct 2014 10:08:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <guid isPermaLink="false">cqT5if_QX0ab2rvyvdQbI0yrot8=</guid>
      <description>First Principles - Glossary</description>
      <content:encoded><![CDATA[<div class="document" id="openstack-terms-and-you">
<h1 class="title">OpenStack Terms and You</h1>
<p>I think we need to start from the beginning to know a bit about where we really are...and to do that we need to speak the same language.  This doesn't mean we must agree on the actual terms used, but that their definition and evidence of need are spelled out and can inform the following steps and discussion.</p>
<p>In some cases, the bikeshed will simply be in primer with placeholders until we converge an appropriate terms.</p>
<p>The Technical Committee <a class="reference external" href="https://wiki.openstack.org/wiki/Governance/Foundation/TechnicalCommittee">Charter</a> refers to some terms that should be clarified early in this process.</p>
<ul class="simple">
<li><strong>Official Programs</strong> - The current unit of organization, called a <strong>program</strong>, logically groups people and code for purposes of achieving the OpenStack project mission.  The current programs are generally teams of people woring on a common project or set of related projects.  These teams elect a Program Team Lead (PTL) who has management responsibilities to the program.  <a class="reference external" href="https://review.openstack.org/#/c/125783/">A proposal</a> to change this terminology to <strong>project team</strong> is in part intended to reinforce the idea that these are groups of people first and code repositories second.  The proposal is part of a series to re-structure the organization of OpenStack projects and by itself is mostly for clarity.  It does not appear to be controversial based on the comments so far so I will adopt it here.</li>
</ul>
<ul class="simple">
<li><strong>Program Leads</strong> - We've all grown to love the term <strong>PTL</strong> and the change from program to team can be used to re-redefine PTL as <strong>project team lead</strong> as it once was back in the day.</li>
<li><strong>Oversight</strong> - The TC charter includes <em>&quot;[the TC] still has oversight over team decisions, especially when they affect other programs or go contrary to general OpenStack project goals&quot;</em>.  There is concern about the balance between project rights and responsibilities changing.  There may also be concerns over what that balance is today.</li>
</ul>
<p>I'm looking for the baked-in connection between governance organization and deliverables.  Today we have a 1:1 between official teams (programs) and their participation in the &quot;integrated release&quot;.  Many of the proposals floating about disconnect those.</p>
<ul>
<li><p class="first"><strong>Coordinated Release Cycle</strong> - The time period roughly starting at or just before the semi-annual Design Summit and ending approximately six months later, again just before the next Design Summit.</p>
</li>
<li><p class="first"><strong>Release Artifacts</strong> - The output of the process formerly known as <em>integrated release</em>.  These are the major output of the <em>coordinated release cycle</em> and typically what distributions would consume in building their periodic releases.</p>
</li>
<li><p class="first"><strong>Project Groups</strong> - Project groups are simply groups of related projects and/or project teams that are dependent on each other and it is often useful to consider the projects together for certain matters.  For example <strong>Group C</strong> includes Nova, Glance, Cinder and Neutron.  Other groups might include <strong>Group I</strong> (Keystone and plugins), <strong>Group O</strong> (Swift), <strong>Group W</strong> (<a class="reference external" href="http://en.wikipedia.org/wiki/Alice's_Restaurant">litterbugs</a>), <strong>Group H</strong> (Ironic and other hypervisor drivers), <strong>Group D</strong> (Trove), <strong>Group M</strong> (Zaqar), and so on.  Many groups may only have one member if there are not multiple projects with a strongly set of similar or mutual dependencies.</p>
<p>One use of project groups is in simplifying the relationship diagram between projects; the diagram between projects within the group can focus on that subset and the diagram between groups can minimize the effect of the intra-group relationships.  These diagrams should be useful in informing testing policy and requirements.</p>
<p>These groups map into my <a class="reference external" href="/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit/">layers description</a> with layer one comprised of Groups I and most of C and layer 2 comprised of Groups O, H and the remainder of C (Cinder).  Remaining groups of official OpenStack projects make up layer 3.</p>
</li>
<li><p class="first"><strong>Integrated Release</strong> - The first step is to change the term <em>integrated release</em> to something better defined and less overloaded.  This is highly contentious as you might expect.  I had intended on using <strong>nucleus</strong> to describe this chewy middle of OpenStack because it has all sorts of metaphorical nooks and crannies to be mined, but then I saw <strong>nuclear release</strong> in writing and decided to just use the rather archaic term <strong>bindle</strong> here instead, as in &quot;the OpenStack Bindle contains the basic cloud necessities&quot;.</p>
<p>The key part of the existing definition, <em>&quot;The OpenStack bindle is a subset of the official OpenStack projects whose member teams submit to stricter policies and oversight&quot;</em>, divides the official OpenStack projects into at least two tiers.  dhellmann describes it like this:</p>
<pre class="literal-block">
size(ecosystem) &gt; size(official-projects) &gt; size(bindle)
</pre>
<p>Inclusion in the integrated release has become the high-value attribute for many corporations judging project worthiness for contribution and/or use.  Removing this attribute is one of the major driving forces for re-considering the organizational structure in general and in renaming and clearly defining the <em>bindle</em>.</p>
<p>The makeup of the bindle defines what release artifacts are published as a result of the release process.  These projects are held to a higher bar in regards to testing, documentation in involvement of team members in horizontal OpenStack activities.</p>
</li>
</ul>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>A Funny Thing Happened On The Way To The Summit</title>
      <link>http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit</link>
      <pubDate>Fri, 03 Oct 2014 10:03:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <guid isPermaLink="false">pYh2olEpo6M1N5yA_pVxnKK2s4Q=</guid>
      <description>A Funny Thing Happened On The Way To The Summit</description>
      <content:encoded><![CDATA[<div class="document">
<p>So back in <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">the old days</a> I started throwing around different terminology to describe some of the technical relationships between OpenStack projects because it was useful to sort out things like startup order requirements in DevStack and other semi-obvious stuff.</p>
<p>And wow have things happened since then.  To recap, oh nevermind, I'm just going to take back the term <strong>layer</strong> for technical use and propose anything else other than <strong>layer 1</strong> (there is no other layer?) for the rest of the conversation. The various alternate approaches all boil down to a <strong>nucleus</strong> with a cloud (heh) of projects with probabilistic locations.  I wasn't a physics major but I do know that doesn't sound like that Q-word that shall not be spoken.</p>
<p>I think it is important to remember that one of the primary purposes of OpenStack is to enable the creation of <strong>useful clouds</strong>.  In my dictionary what makes a useful cloud is described as &quot;the set of services that enable useful work to be done&quot;.  In a cloud.</p>
<p>The original layers idea has been picked up, painted, folded, carved and whitewashed to a shadow of its original.  Even so, in the end all of the ideas still end up looking similar.  Now seems like a good time to see how the orignal layers have held up.</p>
<div class="section" id="layer-1">
<h1>Layer 1</h1>
<p><em>We're still the one...</em></p>
<p>We open with the addition of Neutron as a viable alternative to Nova Network, and the likelihood of it becoming the default configuration in DevStack early in the Juno cycle.</p>
<blockquote>
<ul class="simple">
<li>Identity (Keystone)</li>
<li>Image (Glance)</li>
<li>Network (Neutron)</li>
<li>Compute (Nova)</li>
</ul>
</blockquote>
<p>What really stands out to me now is the realization that all of these were originally part of Nova itself (plus the cinder volume service, more on that later).  They were broken apart or re-implemented to scale the development as Nova kept growing.  In fact, there is talk again of need to break out more simply because Nova continues to expand.</p>
<p>This is the smallest working set for a compute-enabled cloud.  Realistic useful clouds of course offer more than this, so we have...</p>
</div>
<div class="section" id="layer-2">
<h1>Layer 2</h1>
<p>Layer 2 services are optional in a useful compute cloud but some are also useful in their own right as non-compute cloud services.</p>
<p>So the current Layer 2 still contains:</p>
<blockquote>
<ul class="simple">
<li>Volume (Cinder)</li>
<li>Object (Swift)</li>
<li>Bare-metal (Ironic)</li>
</ul>
</blockquote>
<p>These all build on the Layer 1 nucleus and get us a practical useful cloud.  They also all have the characteristic of having dependency arrows pointing <em>out</em> of Layer 1 when used with a compute cloud, such as Glance using Swift as its backend store.  This is a defining characteristic that brings a project in to Layer 2.</p>
<p>Even though Cinder was literally carved out of the Nova code base it stays in Layer 2 because it is an optional service to a Layer 1 cloud.  Manila will also fit here for the same reasons.</p>
<p>I neglected to mention last time the ability of Swift to stand alone as a useful cloud service as it has maintained its own authentication capability.  However, using it with any other OpenStack services requires Swift to use Keystone.</p>
<p>I also think it is worth considering the direction of the trademark usage constraints the board is refining with the DefCore work.  The current DefCore capability proposal is satisfied using only Layer 1 and 2 projects.  Also, the stand-alone services currently would not be able to qualify for trademark usage when deployed alone.</p>
</div>
<div class="section" id="layer-3">
<h1>Layer 3</h1>
<p>Do something useful.  Host services for paying customers.  Provide Lego blocks for them to build awesome cloud apps.  Warn about runaway <tt class="docutils literal">while true; done</tt> loops.  Count cycles burned and bits sent so paying customers know what to pay.  Communicate with your useful cloud.</p>
<p>The rest of the OpenStack-affiliated projects (for some value of <em>affiliated</em>) go in Layer 3 to populate the <strong>big tent</strong>.  If we've done our job right the majority of everything else should be able to be accomplished without special consideration from Layers 1 and 2.  Broad categories of Layer 3 projects include:</p>
<blockquote>
<ul class="simple">
<li>User Interfaces - You need one but a feature of well documented REST APIs is allowing the client side to be easily replaceable.<ul>
<li>Orchestration (Heat) (it is basically a smart automated cloud client, no?)</li>
<li>Web UI (Horizon)</li>
<li>&lt;insert-one-of-the-other-CLI-or-web-clients-here&gt;</li>
</ul>
</li>
<li>Something-as-a-Service - These are all services a deployer may choose to offer.<ul>
<li>Database (Trove)</li>
<li>Message Passing (Zaqar)</li>
</ul>
</li>
<li>Tracking Snooping and Counting - Keeping an eye on the useful cloud<ul>
<li>Telemetry (Ceilometer)</li>
</ul>
</li>
</ul>
</blockquote>
<p>Why is Heat in Layer 3???  Heat is essentially a smart automated cloud client and should be treated as one.   It needs to meet the same requirements for API compatibility to be useful over time.</p>
</div>
<div class="section" id="layer-4">
<h1>Layer 4</h1>
<p>Layer 4 is everything else that is not OpenStack-affiliated but might be a part of an especially useful OpenStack cloud.  Things like Ceph or Apache jclouds are useful with and as part of OpenStack clouds, but they also have a life of their own and we should respect that and not call late at night.</p>
</div>
<div class="section" id="what-about-layer-0">
<h1>What About Layer 0?</h1>
<p>Ah, right, where the Libraries live.  The last year has seen significant changes to how OpenStack-related libraries are integrated with a number of Oslo libraries being released stand-alone.  In most cases these can and should be thought of as dependencies just as any non-OpenStack project dependency (like <tt class="docutils literal">SQLAlchemy</tt> or <tt class="docutils literal">requests</tt>) that happen to live in either the <tt class="docutils literal">openstack</tt> or <tt class="docutils literal">stackforge</tt> namespaces in our Git repositories.</p>
<p>It also seems appropriate to add the client API libraries and SDKs to Layer 0 as the dependency model and release schedule is very similar to the other libraries.  I am specifically not including command-line interfaces here as I think those belong in Layer 3 but the project client libraries have an embedded CLI so the'll straddle the boundaries no matter what.</p>
</div>
<div class="section" id="so-how-do-we-govern-and-test-all-this">
<h1>So How Do We Govern and Test All This?</h1>
<p>OK, I lied.  I said I would skip this, but anyone still reading must think this is has a thread of merit, right?  I choose to make that assumption going forward, and those of you still reading for a laugh, here is your cue.</p>
<p>I'll lay out an overview of a developers perspective because I am primarily an OpenStack developer and I need the world to know what I think.  However, I am also an application developer and cloud end-user so those perspectives are not lost.  I have not managed to add cloud deployer to my CV, yet.</p>
<div class="section" id="releases">
<h2>Releases</h2>
<p>If you turn your head sideways and squint, the Layer picture can also be grouped according to release-able/deploy-able units with decently defined and documented interfaces between them.</p>
<p>Maintaining the current notion of an Integrated Release the layers fall out like this:</p>
<blockquote>
<ul class="simple">
<li>Layers 1 and 2 <em>are</em> the Integrated Release.  The services required to meet DefCore are currently a subset of these layers.</li>
<li>Layer 3 projects treat the Integrated Release as a dependency like any other they may have so they can have the freedom to iterate at a pace that suits the service being provided.  Trove probably needs fewer releases in the next year than Zaqar.</li>
</ul>
</blockquote>
<p>Switching to a more modularized set of released units the first 'natural' groupings are:</p>
<blockquote>
<ul class="simple">
<li>Layer 1 plus the semi-tightly coupled Nova projects like Cinder (and Manila) comprise a Compute Release.</li>
<li>Swift comprises an Object Store release</li>
<li>Ironic comprises an (insert-catchy-name-here) release and not in the Compute Release as it can also stand alone (right?)</li>
<li>Actually, everything else is on its own because Independence From Tyranny!  Things that need to talk to each other or to the Integrate projects need to correctly identify and handle the documented APIs available to them.</li>
</ul>
</blockquote>
<p>Basically, this alternative splits the Integrated Release into a Compute Release and two stand-alone releases for Swift and Ironic.  The Release Management team may reconsider the criteria required for them to continue to handle other project releases or allow (force?) the projects to handle their own.</p>
<p>Note how the difference in those two approaches to releases is exactly two things, pulling Swift and Ironic out of the Integrated Release bundle so they can stand alone.</p>
</div>
<div class="section" id="testing">
<h2>Testing</h2>
<p>As current work is showing, the actual detailed relationships between OpenStack services is very complex.  Describing it to a level of detail that can drive a test matrix is not simple.  We can, however, reduce the problem space by re-thinking at a higher level what needs to be tested together.</p>
<p>Layers 1 and 2 are really where the work needs to be done. By changing the perspective of Layer 3 projects we can reduce the piling-on of additional projects that are currently in our Test All The Things check/gate jobs.  Individual project relationships across that boundary may be important enough to warrant specific test jobs but those are considered exceptions and not the rule.</p>
<p>A significant amount of the gains to me made here are contingent on the projects developing comprehensive functional tests.</p>
</div>
</div>
<div class="section" id="horizontal-projects">
<h1>Horizontal Projects</h1>
<p>While it feels like I'm saving the best for last, in reality much of the above has to have some structure to know the scope that Infrastructure, Docs and QA need to be able to support.  Focusing these on Layers 1 and 2 provides a clear limit to the scope required.  This is not to say that other projects are not going to be accommodated, particularly those already in the current release, but it does say that it is not assured.</p>
</div>
<div class="section" id="now-what-smart-guy">
<h1>Now What Smart Guy?</h1>
<p>With my thoughts on Layers updated to include the governance and testing considerations it is time to match up other perspectives, flesh out the above with the new information and catch up on the plethora of other posts on this topic.</p>
<p>Film at eleven...</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenStack Low Level API</title>
      <link>http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api</link>
      <pubDate>Mon, 15 Sep 2014 09:15:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[api]]></category>
      <category><![CDATA[client]]></category>
      <guid isPermaLink="false">eIZng7kEM5KyvZSHnaO9Z1LG1s8=</guid>
      <description>OpenStack Low Level API</description>
      <content:encoded><![CDATA[<div class="document">
<p>The current Python library situation for OpenStack is, sorry to say, a mess.  Cleaning it up requires essentially starting over and abstracting the individual REST APIs to usable levels.  With OpenStackClient I started from the top and worked down to make the CLI a better experience.  I think we have proved that to be a worthwhile task.  Now it is time to start from the bottom and work up.</p>
<p>The existing libraries utilize a Manager/Resource model that may be suitable for application work, but every project's client repo was forked and changed so they are all similar but maddeningly different.  However, a good idea or two can be easily extracted and re-used in making things as simple as possible.</p>
<p>I originally started with no objects at all and went straight to top-level functions, as seen in the current <tt class="docutils literal">object.v1.lib</tt> APIs in OSC.  That required passing around the session and URLs required to complete the REST calls, which OSC already has available, but it is not a good general-purpose API.</p>
<p>I've been through a number of iterations of this and have settles on what is described here, a low-level API for OSC and other applications that do not require an object model.</p>
<div class="section" id="api-baseapi">
<h1>api.BaseAPI</h1>
<p>We start with a <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> object that contains the common operations.  It is pretty obvious there are only a couple of ways to get a list of resources from OpenStack APIs so the bulk of that and similar actions are here.</p>
<p>It is also very convenient to carry around a couple of other objects so they do not have to be passed in every call.  <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> contains a <tt class="docutils literal">session</tt>, <tt class="docutils literal">service type</tt> and <tt class="docutils literal">endpoint</tt> for each instance.  The <tt class="docutils literal">session</tt> is a <tt class="docutils literal">requests.session.Session</tt>-compatible object.  In this implementation we are using the <tt class="docutils literal">keystoneclient.session.Session</tt> which is close enough.  We use the ksc Session to take advantage of keystoneclient's authentication plugins.</p>
<p>The <tt class="docutils literal">service type</tt> and <tt class="docutils literal">endpoint</tt> attributes are specific to each API.  <tt class="docutils literal">service type</tt> is as it is used in the Service Catalog, i.e. <tt class="docutils literal">Compute</tt>, <tt class="docutils literal">Identity</tt>, etc.  <tt class="docutils literal">endpoint</tt> is the base URL extracted from the service catalog and is prepended to the passed URL strings in the <tt class="docutils literal">API</tt> method calls.</p>
<p>Most of the methods in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> also are meant as foundational building blocks for the service APIs.  As such they have a pretty flexible list of arguments, many of them accepting a <tt class="docutils literal">session</tt> to override the base <tt class="docutils literal">session</tt>.  This layer is also where the JSON decoding takes place, these all return a Python <tt class="docutils literal">list</tt> or <tt class="docutils literal">dict</tt>.</p>
<p>The derived classes from <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> will contain all of the methods used to access their respective REST API.  Some of these will grow quite large...</p>
</div>
<div class="section" id="api-object-store-apiv1">
<h1>api.object_store.APIv1</h1>
<p>While this is a port of the existing code from OpenStackClient, <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/object_store.py#L26">object_store.APIv1</a> is still essentially a greenfield implementation of the <tt class="docutils literal"><span class="pre">Object-Store</span></tt> API.  All of the path manipulation, save for prepending the base URL, is done at this layer.</p>
</div>
<div class="section" id="api-compute-apiv2">
<h1>api.compute.APIv2</h1>
<p>This is one of the big ones.  At this point, only <tt class="docutils literal">flavor_list()</tt>, <tt class="docutils literal">flavor_show()</tt> and <tt class="docutils literal">key_list()</tt> have been implemented in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/compute.py#L19">compute.APIv2</a>.</p>
<p>Unlike the <tt class="docutils literal"><span class="pre">object-store</span></tt> API, the rest of the OpenStack services return resources wrapped up in a top-level dict keyed with the base name of the resource.  This layer shall remove that wrapper so the returned values are all directly lists or dicts.  This removed the variations in server implementations where some wrap the list object individually and some wrap the entire list once.  Also, Keystone's tendency to insert an additional <tt class="docutils literal">values</tt> key into the return.</p>
</div>
<div class="section" id="api-identity-vx-apivx">
<h1>api.identity_vX.APIvX</h1>
<p>The naming of <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/identity_v2.py#L19">identity_v2.APIv2</a> and <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/identity_v3.py#L19">identity_v3.APIv3</a> is a bit repetitive but putting the version into the module name lets us break down the already-long files.</p>
<p>At this point, only <tt class="docutils literal">project_list()</tt> is implemented in an effort to work out the mechanics of supporting multiple API versions.  In OSC, this is already handled in the ClientManager and individual client classes so there is not much to see here.  It may be different otherwise.</p>
</div>
<div class="section" id="osc-usage">
<h1>OSC Usage</h1>
<p>To demonstrate how this API is used, I've added an <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> instance to the existing client objects that get stored in the <tt class="docutils literal">ClientManager</tt>.  For example, the addition for <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-9d500da5511aec08e46397bc7a4b25bdR75">compute.client</a> is one object instantiation and an import.  Now in OSC, <tt class="docutils literal">clientmanager.compute.api</tt> has all of the (implemented) <tt class="docutils literal">Compute</tt> API methods.</p>
<p>Using it in the flavor commands is a <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-1be98e03ae4586b73d8ad5f62f0dc578L163">simple change</a> to call <tt class="docutils literal">compute.api</tt> methods rather than the <tt class="docutils literal">compute.flavor.XXX</tt> methods.</p>
<p>Setting up for multiple API versions took a bit more work, as shown in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-f7023c81f38d2c70e77da533164db4b6L31">identity.client</a>.  A parallel construction to the client class lookup is required, and would totally replace the existing version lookup once the old client is no longer required.</p>
</div>
<div class="section" id="fluff">
<h1>Fluff</h1>
<p>One other cool feature is utilizing <tt class="docutils literal">requests_mock</tt> for testing from the start.  It works great and has not the problems that rode along with <tt class="docutils literal">httpretty</tt>.</p>
</div>
<div class="section" id="now-what">
<h1>Now What?</h1>
<p>Many object models could be built on top of this API design.  The <tt class="docutils literal">API</tt> object hierarchy harkens back to the original client lib <tt class="docutils literal">Manager</tt> classes, except that they encompass an entire REST API and not one for each resource type.</p>
</div>
<div class="section" id="but-you-said-sanity-earlier">
<h1>But You Said 'Sanity' Earlier!</h1>
<p>Sanity in terms of coalescing the distinct APIs into something a bit more common?  Yes.  However, this isn't going to fix everything, just some of the little things that application developers really shouldn't have to worry about.  I want the project REST API docs to be usable, with maybe a couple of notes for the differences.</p>
<p>For example, OSC and this implementation both use the word <tt class="docutils literal">project</tt> in place of <tt class="docutils literal">tenant</tt>.  Everywhere.  Even where the underlying API uses <tt class="docutils literal">tenant</tt>.  This is an easy change for a developer to remember.  I think.</p>
<p>Also, smoothing out the returned data structures to not include the resource wrappers is an easy one.</p>
</div>
<div class="section" id="duplicating-work">
<h1>Duplicating Work?</h1>
<p>&quot;Doesn't this duplicate what is already being done in the OpenStack Python SDK?&quot;</p>
<p>Really, no.  This is meant to be the low-level SDK API that the Resource model can utilize to provide the back-end to its object model.  Honestly, most applications are going to want to use the Resource model, or an even higher API that makes easy things really easy, and hard things not-so-hard, as long as you buy in to the assumptions baked in to the implementation.</p>
<p>Sort of like OS/X or iOS.  Simple to use, as long as you don't want to anything different.  Maybe we should call that top-most API <tt class="docutils literal">iOSAPI</tt>?</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenWRT Images for OpenStack</title>
      <link>http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack</link>
      <pubDate>Sun, 17 Aug 2014 08:17:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[openwrt]]></category>
      <guid isPermaLink="false">eYdqmkKAVncy1u3mOePk4UrR_wQ=</guid>
      <description>OpenWRT Images for OpenStack</description>
      <content:encoded><![CDATA[<div class="document">
<p>I've been playing with <a class="reference external" href="http://openwrt.org">OpenWRT</a> since &lt;mumble&gt;-&lt;mumble&gt; and have enjoyed building some of the smallest Linux images around.  While targeted at low-end home router platforms, it also runs on a wide variety of small SoC boards including the near-ubiqutious Raspberry Pi and my fave BeagleBone Black.</p>
<p>I've also been using an incredibly tiny OpenWRT instance on my laptop for years now to work around the 'interesting' network configuration of VirtualBox.  Building a set of VMs that need to talk to each other and to the outside world shouldn't be hard, so I added a router just like I have at home in a 48Mb VM.</p>
<p>While OpenStack typically doesn't have that need (but you never know how Neutron might be configured!) there are plenty of other purposes for such a small single-purpose VM.  So let's build one!</p>
<p>The magic in this cloud build is having an analogue to smoser's <a class="reference external" href="https://launchpad.net/cloud-init">cloud-init</a>.  The original is written in Python and has a lot of very useful features, but requiring the Python stdlib and <tt class="docutils literal"><span class="pre">cloud-init</span></tt> dependencies to be installed expands the size of the root image considerably.   My version, called <a class="reference external" href="https://github.com/dtroyer/openwrt-packages/tree/master/rc.cloud">rc.cloud</a>, is a set of shell scripts that implement a small subset of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> capabilities.  [Note: I 'borrowed' the original scripts from somewhere over three years ago and for the life of me can't find out where now.  Pointers welcome.]</p>
<p>One of the most important features of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> and <tt class="docutils literal">rc.cloud</tt> is configuring a network interface and enabling remote access.  OpenWRT defaults to no root password so I have to telnet to 192.168.1.1 to set root's password before Dropbear (a tiny ssh2 implementation) allows logins. Doing it with <tt class="docutils literal"><span class="pre">cloud-init</span></tt> or <tt class="docutils literal">rc.cloud</tt> instead allows automation and is a Wonderful Thing(TM).</p>
<p>This isn't a detailed How-To on building OpenWRT, there are a lot of <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">good docs</a> covering that topic.  It _is_ however, the steps I use plus some additional tweaks useful in a KVM-based OpenStack cloud.</p>
<div class="section" id="build-image-from-source">
<h1>Build Image From Source</h1>
<p>The basic build is straight out of the <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">OpenWRT wiki</a>.  I could have used the Image Builder, but I have some additional packages to include and like having control over the build configuration, such as either making sure IPv6 is present, or making sure it isn't.  And so on.</p>
<p>Configuring the OpenWRT buildroot can be a daunting task so starting with a minimal configuration is very helpful.  For a guest VM image there are a few things to consider:</p>
<ul class="simple">
<li>the VM target (Xen, KVM, etc)</li>
<li>root device name (vda2 for KVM, sda2 for others like VirtualBox)</li>
</ul>
<p>Traditionally OpenWRT has used Subversion for source control.  A move (or mirror?) on GitHub makes things easier for those of us who it it regualrly in other projects.  The <a class="reference external" href="http://wiki.openwrt.org/doc/howto/buildroot.exigence">buildroot doc</a> uses GitHub as the source so I've followed that convention.</p>
<ul>
<li><p class="first">Clone the repo:</p>
<pre class="literal-block">
git clone git://git.openwrt.org/openwrt.git
cd openwrt
</pre>
</li>
<li><p class="first">Install custom feed:</p>
<pre class="literal-block">
echo &quot;src-git dtroyer https://github.com/dtroyer/openwrt-packages&quot; &gt;&gt;feeds.conf.default
</pre>
</li>
<li><p class="first">Install packages:</p>
<pre class="literal-block">
./scripts/feeds update -a
./scripts/feeds install -a
</pre>
</li>
<li><p class="first">Check for missing packages:</p>
<pre class="literal-block">
make defconfig
</pre>
</li>
</ul>
<div class="section" id="configuration">
<h2>Configuration</h2>
<ul>
<li><p class="first">Configure:</p>
<pre class="literal-block">
make menuconfig
</pre>
</li>
<li><p class="first">Enable the following:</p>
<ul class="simple">
<li>Target System: x86</li>
<li>Subtarget: KVM guest</li>
<li>Target Images<ul>
<li><tt class="docutils literal">[*] ext4</tt></li>
<li><tt class="docutils literal">(48)</tt> Root filesystem partition size (in MB)</li>
<li><tt class="docutils literal">(/dev/vda2)</tt> Root partition on target device</li>
</ul>
</li>
<li>Base System<ul>
<li><tt class="docutils literal">{*} <span class="pre">block-mount</span></tt>  (not sure, if yes to support root fs, parted too)</li>
<li><tt class="docutils literal">&lt;*&gt; rc.cloud</tt></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Notes:</p>
<ul class="simple">
<li>Increase the root filesystem size if you do not intend to move root to another partition or increase the existing one to fit the flavor's disk size.</li>
</ul>
</div>
<div class="section" id="build">
<h2>Build</h2>
<p>It's pretty simple:</p>
<pre class="literal-block">
make -j 4
</pre>
<p>Adjust the argument to <tt class="docutils literal"><span class="pre">-j</span></tt> as appropriate for the number of CPUs on your build system.</p>
<p>When build errors occur, you'll need to run with output turned on:</p>
<pre class="literal-block">
make V=99
</pre>
</div>
</div>
<div class="section" id="configuring-the-image-for-openstack">
<h1>Configuring the Image for OpenStack</h1>
<p>As I mentioned earlier, there are a handful of changes to make to the resulting image that makes it ready for an OpenStack cloud.</p>
<ul class="simple">
<li>Set a root password - Without a root password your newly minted VM is vulnerable to a password-less telnet login if your security group rules allow that.  But more importantly, Dropbear will not allow an ssh login without a rot password.  Edit <tt class="docutils literal">/etc/shadow</tt> to set a root password</li>
<li>Configure a network interface for DHCP - This allows the first interface to obtain its IP automatically for OpenStack clouds that provide it.  Otherwise...</li>
<li>Configure <tt class="docutils literal">/etc/opkg.conf</tt> to my package repo - Packages usually need to be matched for not only their architecture but also other build flags.  Kernel modules are particularly rigid about how they can be loaded.</li>
</ul>
<div class="section" id="image-update">
<h2>Image Update</h2>
<p>All of the interesting parts below must be done as root.  So be careful.</p>
<ul>
<li><p class="first">Uncompress and copy the original image to a workspace, mount it and chroot into it:</p>
<pre class="literal-block">
gzip -dc bin/x86/openwrt-x86-kvm_guest-combined-ext4.img.gz &gt;openwrt-x86-kvm_guest-combined-ext4.img
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
mkdir -p imgroot
sudo mount -o loop /dev/mapper/loop0p2 imgroot
sudo chroot imgroot
</pre>
</li>
<li><p class="first">Make the desired changes:</p>
<ul>
<li><p class="first">Set root password:</p>
<pre class="literal-block">
sed -e '/^root/ s|^root.*$|root:\!:16270:0:99999:7:::|' -i /etc/shadow
</pre>
</li>
<li><p class="first">Configure DHCP:</p>
<pre class="literal-block">
uci set network.lan.proto=dhcp; uci commit
</pre>
</li>
<li><p class="first">Configure opkg:</p>
<pre class="literal-block">
sed -e &quot;s|http.*/x86/|http://bogus.hackstack.org/openwrt/x86/|&quot; -i /etc/opkg.conf
</pre>
</li>
</ul>
</li>
<li><p class="first">Unwind the mounted image:</p>
<pre class="literal-block">
sudo umount imgroot
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
</pre>
</li>
<li><p class="first">Upload it into Glance:</p>
<pre class="literal-block">
openstack image create --file openwrt-x86-kvm_guest-combined-ext4.img --property os-distro=OpenWRT OpenWRT

# Glance CLI
glance image-create --file openwrt-x86-kvm_guest-combined-ext4.img --name OpenWRT
</pre>
</li>
</ul>
</div>
</div>
<div class="section" id="additional-modifications">
<h1>Additional Modifications</h1>
<div class="section" id="extending-root-filesystem">
<h2>Extending Root Filesystem</h2>
<p>Even the smallest flavor gets a root disk a good bit larger than the typocal OpenWRT disk image.  One way to use that space is to increase the root filesystem.  OpenWRT has something called <tt class="docutils literal">extroot</tt> that is currently experimental and semi-undocumented, so I just took the radical move of partitioning the unused space and moving the root filesystem to the new partition.</p>
<p>Of course a real root expansion should be automated and added to <tt class="docutils literal">rc.cloud</tt> to mirror the <tt class="docutils literal"><span class="pre">cloud-init</span></tt> functionality.  Someday...</p>
<ul>
<li><p class="first">Install required packages if they're not part of the base build:</p>
<pre class="literal-block">
opkg update
opkg install block-mount parted
</pre>
</li>
<li><p class="first">Create a filesystem on the remaining disk and mount it:</p>
<pre class="literal-block">
parted /dev/vda -s -- mkpart primary  $(parted /dev/vda -m print | tail -1 | cut -d':' -f3) -0
mkfs.ext4 -L newroot /dev/vda3
mkdir -p /tmp/newroot
mount /dev/vda3 /tmp/newroot
</pre>
</li>
<li><p class="first">Copy the root filesystem:</p>
<pre class="literal-block">
mkdir -p /tmp/oldroot
mount --bind / /tmp/oldroot
tar -C /tmp/oldroot -cvf - . | tar -C /tmp/newroot -xf -
umount /tmp/oldroot
umount /tmp/newroot
</pre>
</li>
<li><p class="first">Update the GRUB bootloader to use the new partition:</p>
<pre class="literal-block">
mkdir -p /tmp/boot
mount /dev/vda1 /tmp/boot
sed -e 's/vda2/vda3/' -i /tmp/boot/boot/grub/grub.cfg
umount /tmp/boot
</pre>
</li>
<li><p class="first">Reboot</p>
</li>
</ul>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>More Notes on Windows Images</title>
      <link>http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images</link>
      <pubDate>Sun, 13 Jul 2014 07:13:00 CDT</pubDate>
      <category><![CDATA[windows]]></category>
      <category><![CDATA[openstack]]></category>
      <guid isPermaLink="false">_MyylKSqiFGJ3m4qsDhVRwgOUEs=</guid>
      <description>More Notes on Windows Images</description>
      <content:encoded><![CDATA[<div class="document">
<p>This is a follow-up to <a class="reference external" href="/x/blog/2014/07/07/windows-images-for-openstack/">Windows Images for OpenStack</a> that includes some of the notes accumulated along the way.</p>
<div class="section" id="other-docs">
<h1>Other Docs</h1>
<p>Building Windows VM images is a topic that has been done to death, but the working consensus of those I've talked to is that <a class="reference external" href="http://www.florentflament.com/blog/windows-images-for-openstack.html">Florent Flament's post</a> is one of the best guides through this minefield.</p>
</div>
<div class="section" id="metadata-server-curl-commands">
<h1>Metadata Server Curl Commands</h1>
<p>Instance UUID:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;uuid&quot;]'</blockquote>
<p>Instance Name:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;name&quot;]'</blockquote>
<p>Fixed IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/local-ipv4">http://169.254.169.254/latest/meta-data/local-ipv4</a></blockquote>
<p>Floating IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/public-ipv4">http://169.254.169.254/latest/meta-data/public-ipv4</a></blockquote>
</div>
<div class="section" id="building-on-an-openstack-cloud">
<h1>Building on an OpenStack Cloud</h1>
<p>One of the changes to the base instructions is to perform the build in an OpenStack cloud.  The compute node must have nested virtualization enabled so KVM will run, otherwise Qemu would be used and we just don't have time for that.</p>
<p>I'm going to use <a class="reference external" href="https://github.com/cloudenvy/cloudenvy">Cloudenvy</a> to manage the build VM.  It is similar to Vagrant in automating the grunt work of provisioning the VM.  The VM needs to have at least 4Gb RAM and 40Gb disk available in order to boot the seed Windows image.  This is an <tt class="docutils literal">n1.medium</tt> flavor on the private cloud I am using.</p>
<p>I am also using Ubuntu 14.04 because much of my tooling already assumes an Ubuntu build environment.  There is no technical reason that Fedora 20 could not be used, appropriate adjustments would need to be made, of course.</p>
<div class="section" id="build-vm">
<h2>Build VM</h2>
<p>I am not going to spend much time here explaining Cloudenvy's configuration, but there are two things required to not have a bad time with it.</p>
<p>Configure your cloud credentials in <tt class="docutils literal"><span class="pre">~/.cloudenvy</span></tt>:</p>
<pre class="literal-block">
cloudenvy:
    keypair_name: dev-key
    keypair_location: ~/.ssh/id_rsa-dev-key.pub
    clouds:
        cloud9:
            os_auth_url: https://cloud9.slackersatwork.com:2884/v2.0/
            os_tenant_name: demo
            os_username: demo
            os_password: secrete
</pre>
<pre class="literal-block">
project_config:
    name: imagebuilder
    image: Ubuntu 14.04
    remote_user: ubuntu
    flavor_name: n1.medium

sec_groups: [
    'tcp, 22, 22, 0.0.0.0/0',
    'tcp, 5900, 5919, 0.0.0.0/0',
    'icmp, -1, -1, 0.0.0.0/0'
]

files:
    Makefile: '~'
    ~/.cloud9.conf: '~'

provision_scripts:
    - install-prereqs.sh
</pre>
<p>The <tt class="docutils literal"><span class="pre">~/.cloud9.conf</span></tt> file is a simple script fragment that sets the <tt class="docutils literal">OS_*</tt> environment variable credentials required to authenticate using the OpenStack CLI tools.  It looks something like:</p>
<pre class="literal-block">
export OS_AUTH_URL=https://cloud9.slackersatwork.com:2884/v2.0/
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=secrete
</pre>
<p>Why do we need two sets of credentials?  Because we haven't taught Cloudenvy to read the usual environment variables yet.  I smell a pull request in my future...</p>
<p>Fire it up and log in:</p>
<pre class="literal-block">
envy up
envy ssh
</pre>
<p>At this point we can switch over to Flament's process.</p>
<p>Or we can use the cloudbase auto-answer template</p>
<p>Get the ISO:</p>
<pre class="literal-block">
&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
for i in aa ab ac ad ae af ag ah; do \
    swift download windows7 en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i; \
    cat en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i &gt;&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
done
</pre>
<p>sudo ./make-floppy.sh</p>
<hr class="docutils" />
<pre class="literal-block">
# add keypair if not already there
os keypair create --public-key ~/.ssh/id_rsa.pub $(hostname -s)

# Create VM
os server create \
  --image &quot;Ubuntu 14.04&quot; \
  --flavor n1.tiny \
  --key-name bunsen \
  --user-data cconfig.txt \
  --wait \
  dt-1

export IP=$(os server show dt-1 -f value -c addresses | cut -d '=' -f2)

# Go to there
ssh ubuntu&#64;$IP
</pre>
<hr class="docutils" />
<p>Now on to Florent's steps</p>
<ul>
<li><p class="first">Create a virtual disk</p>
<blockquote>
<p>qemu-img create -f qcow2 Windows-Server-2008-R2.qcow2 9G</p>
</blockquote>
</li>
<li><p class="first">Boot the install VM</p>
</li>
</ul>
<pre class="literal-block">
kvm \
    -m 2048 \
    -cdrom &lt;WINDOWS_INSTALLER_ISO&gt; \
    -drive file=Windows-Server-2008-R2.qcow2,if=virtio \
    -drive file=&lt;VIRTIO_DRIVERS_ISO&gt;,index=3,media=cdrom \
    -net nic,model=virtio \
    -net user \
    -nographic \
    -vnc :9 \
    -k fr \
    -usbdevice tablet
</pre>
<p>Connect via VNC to :9</p>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Windows Images for OpenStack</title>
      <link>http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack</link>
      <pubDate>Mon, 07 Jul 2014 07:07:00 CDT</pubDate>
      <category><![CDATA[windows]]></category>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[virtualbox]]></category>
      <guid isPermaLink="false">NEdHQUMnNDPOA2ZMbkUe63N2oT0=</guid>
      <description>Windows Images for OpenStack</description>
      <content:encoded><![CDATA[<div class="document">
<p>There is no shortage of articles online about building Windows images for use in various clouds.  What there is a shortage of are working articles on building these images unattended.  The Windows unattended install process has been basically solved, even if still a bit arcane.  But finding more than a trivial example of doing it in a cloud is sparse.</p>
<p>Cloudbase has <a class="reference external" href="https://github.com/cloudbase/windows-openstack-imaging-tools">shared the tooling</a> they created for building their Windows images.  That makes a good base for an automated build process that can be tailored to your particular needs.  in addition to being the authors of cloudbase-init, their GitHub account is a trove of resources for Windows admins.</p>
<p>Since I had a Windows 7 Professional ISO handy I used that for the example...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<p>The resulting image must:</p>
<ul class="simple">
<li>have RedHat's VirtIO drivers installed</li>
<li>use <tt class="docutils literal"><span class="pre">cloudbase-init</span></tt> for metadata handling</li>
</ul>
</div>
<div class="section" id="build-in-virtualbox">
<h1>Build In VirtualBox</h1>
<p>The Cloudbase process is designed to perform the build using KVM.  Ideally, it would be possible to boot a VM in an OpenStack cloud from the install ISO and let it go, but it turns out this is hard.  The unattended install process  requires two ISO images and a floppy image attached to the VM in addition to the target disk device. OpenStack currently has no way to do all of these attachments.  The alternative is to stash autounattend.xml and the virtio drivers in the Windows install ISO, but this requires a rebuild/upload for _every_ change to the install scripts.</p>
<p>So normally this means a Linux on bare-metal install is required.  How hard is it to dig up a laptop with Trusty on it?  Hard, if you're me.</p>
<p>I've heard that using VirtualBox doesn't work for some reason, but these reasons haven't been made clear to me so I didn't know I couldn't do what I'm describing here.</p>
<div class="section" id="auto-answer-changes">
<h2>Auto Answer Changes</h2>
<p>One of the main change to Cloudbase's setup is to put the PowerShell scripts on the floppy with Autounattend.xml.  This ensures that the files are matched together and changes in the repo doesn't break our working setup.</p>
<p>The Autounattend.xml file has a couple of changes other than those required to run the script from the floppy:</p>
<ul class="simple">
<li>Add the MetaData value for Win7</li>
<li>Since this is Win7, we need to enable the account stanza</li>
<li>Install a public product key</li>
<li>Fix a spacing error in the Microsoft-Windows-International-Core-WinPE component element</li>
</ul>
</div>
<div class="section" id="powershell-script-changes">
<h2>PowerShell Script Changes</h2>
<p>The primary change to the PowerShell scripts is to remove the file downloads and retrieve them from the floppy instead.</p>
</div>
<div class="section" id="make-the-floppy-image">
<h2>Make the Floppy Image</h2>
<p>I used <a class="reference external" href="/x/files/make-floppy.sh">this script</a> to create the floppy image, it builds a new image, mounts it, copies the appropriate Autounattend.xml and PowerShell scripts and other files, then umnounts the image.</p>
</div>
<div class="section" id="build-vm-configuration">
<h2>Build VM Configuration</h2>
<p>Automating a VBox build includes creating the VM to be used.  The <tt class="docutils literal">VBoxManage</tt> tool is the simple way to do this from a script and that's exactly what I've done here.</p>
<p>It turns out that 16Gb is not enough for Windows 7 installation once all of the updates are installed.  There are a LOT of them, 157 at this writing.  Even though this only needs to be done once, it takes a long time to apply them and it might be worthwhile to obtain media with the updates pre-applied.</p>
<p>The commands here are taken from the <tt class="docutils literal"><span class="pre">build-vb.sh</span></tt> script.</p>
<p>Create a new empty VM and disk:</p>
<pre class="literal-block">
BASE_NAME='win-build'
# Create a new empty VM
VBoxManage createvm --name &quot;$BASE_NAME&quot; --ostype &quot;$OS_TYPE&quot; --register
VBoxManage createhd --filename &quot;$VM_DIR/$BASE_NAME.vdi&quot; --size $DISK_SIZE
</pre>
<p>The disk configuration is an important part of this process so everything is found as required.  In addition to the install disk and install ISO a second ISO must be mounted containing the VirtIO drivers and a floppy image with the Autounattend.xml and Powershell scripts:</p>
<pre class="literal-block">
# SATA Controller
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;SATA&quot; --add sata
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;SATA&quot; --type hdd \
    --port 0 --device 0 --medium &quot;$VM_DIR/$BASE_NAME.vdi&quot;

# Make IDE disks
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;IDE&quot; --add ide
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 0  --device 0 --medium &quot;$WIN_ISO&quot;
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 1  --device 0 --medium &quot;$VIRTIO_ISO&quot;

# Floppy disk image
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;Floppy&quot; --add floppy
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;Floppy&quot; --type fdd \
    --port 0 --device 0 --medium &quot;$FLOPPY&quot;
</pre>
<p>Do the remaining basic configuration, including a virtio NIC to tickle Windows to install the drivers:</p>
<pre class="literal-block">
# General Config
VBoxManage modifyvm &quot;$BASE_NAME&quot; --cpus 2
VBoxManage modifyvm &quot;$BASE_NAME&quot; --memory $RAM_SIZE --vram 24
VBoxManage modifyvm &quot;$BASE_NAME&quot; --ioapic on

VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic1 nat --bridgeadapter1 e1000g0
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic2 nat
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nictype2 virtio

VBoxManage modifyvm &quot;$BASE_NAME&quot; --boot1 dvd --boot2 disk --boot3 none --boot4 none
</pre>
<p>Kick off the build process:</p>
<pre class="literal-block">
VBoxManage startvm &quot;$BASE_NAME&quot; --type gui
</pre>
<p>Convert the disk from VDI to QCOW2 format for uploading into the image store:</p>
<pre class="literal-block">
qemu-img convert -p -O qcow &quot;$VM_DIR/$BASE_NAME.vdi&quot; &quot;$VM_DIR/$BASE_NAME.qcow&quot;
</pre>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenStack Icehouse Developer Summit</title>
      <link>http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit</link>
      <pubDate>Sun, 10 Nov 2013 11:10:00 CST</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[devstack]]></category>
      <guid isPermaLink="false">3SuDmuCE-gxe1BOWhZ-SaroxFcc=</guid>
      <description>OpenStack Icehouse Developer Summit</description>
      <content:encoded><![CDATA[<div class="document">
<p>OpenStack has had a global reach since the early days but the Design Summits
have always been a US-based affair.  Last week we finally took the every-six-month
roadshow off-continent and ventured out to Hong Kong.
Of course the Conference is co-located and concurrent but I didn't make it to
any of those sessions this time and only knew it was there by going to lunch in
the expo hall and seeing some familiar vendor faces.</p>
<p>We begin with the projects most subject to my attention, DevStack, Grenade and
OpenStackClient.</p>
<div class="section" id="devstack">
<h1>DevStack</h1>
<p>This is the first summit where DevStack has program status and thus its own
track of two back-to-back sessions.  I hear russellb is jealous...</p>
<div class="section" id="new-bits">
<h2>New Bits</h2>
<p>The DevStack 'New Bits' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstacks-new-bits">EtherPad</a>)
was spent talking about a couple of the significant additions
to DevStack late in the Havana cycle.  I wrote about the
<a class="reference external" href="/x/blog/2013/09/07/devstack-local-config">local config</a>
work as it was being developed, the discussion in the session was primarily a Q&amp;A.
One bit that was covered was converting devstack-gate to use this form rather
than <tt class="docutils literal">localrc</tt>.</p>
<p>The other major DevStack addition is a plugin mechanism
to configure and start additional services without requiring changes to DevStack.
This is partially intended for new projects to be able to use DevStack for their
Jenkins testing without requiring them to be added to the DevStack repo.</p>
<p>This is an expansion of the existing hook into <tt class="docutils literal">extras.d</tt> that automagically
ran scripts at the end of <tt class="docutils literal">stack.sh</tt>.  These scripts are essentially
dispatchers as they are called multiple times from <tt class="docutils literal">stack.sh</tt>, <tt class="docutils literal">unstack.sh</tt> and
<tt class="docutils literal">clean.sh</tt>.  <a class="reference external" href="http://devstack.org/plugins.html">devstack.org</a> has an example of an
<tt class="docutils literal">extras.d</tt> dispatch script.</p>
<p>Savanna and Tempest have been converted to the plugin format with Marconi in progress.
Most of the remaining <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 4</a>
projects should also be able to be converted to the plugin format.</p>
<p>Other highlights, some of which I intend to cover here in the future:</p>
<ul class="simple">
<li>bash8 - style testing for Bash scripts similar to hacking/pep8/flake8;
there is interest in this becoming a stand-alone project if it proves to be useful</li>
<li>DevStack tests - the addition of <tt class="docutils literal">run_tests.sh</tt> provides a familiar, if
deprecated, interface to running <tt class="docutils literal">bash8</tt> and other tests</li>
<li>exercises - the DevStack exercises are now unused in all gate testing except
Grenade's <em>base</em> phase.  As they are still generally useful outside the
gate test environment a new Jenkins job needs to be added to check them for bit rot.</li>
</ul>
</div>
<div class="section" id="distro-support">
<h2>Distro Support</h2>
<p>sdague led the DevStack 'Distro Support' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstack-support">EtherPad</a>)
discussing distro supported status and what we need to do to bring the current
ones up to snuff and what might be required of new additions.</p>
<p>The primary requirement to adding the support tag is the ability to have it tested
in the DevStack gate.  Unfortunately, neither of the clouds that provide
test resources to our CI infrastructure
(HP Cloud and Rackspace Cloud Servers)
allow arbitrary images to be uploaded so only distros that have supported images
are able to be tested.  The third-party testing hooks might be able to be used
to mitigate some of this but the resources for that testing will need to be supplied.</p>
<p>There was also some discussion around projects getting supported status from DevStack.
A lot of this is a timing and process issue for incubation/integration process wanting
to see testing before graduation from those steps but not wanting to add projects
to DevStack that are not on that track.  The addition of the extras.d capability
for projects to be easily added to DevStack without modifying it goes a long way
toward setting up the needed testing to demonstrate the capability of the project and
team before actually adding it to the repo.</p>
<p>The flow will look like:</p>
<ul class="simple">
<li>third-party testing in StackForge will utilize the extras.d plugins to do the
required pre-incubation testing</li>
<li>after incubation, the project gets added to the DevStack repo (still utilizing the
plugin mech) and added to the gate as a requirement for graduation to integrated status.</li>
</ul>
</div>
</div>
<div class="section" id="grenade">
<h1>Grenade</h1>
<p>The Grenade session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-qa-grenade">EtherPad</a>) focused mostly on expanding the test matrix
of <tt class="docutils literal">base</tt> and <tt class="docutils literal">target</tt> releases that need testing.  This includes tests from
stable releases to trunk and stable release updates as well as from
stable release updates to next stable or trunk.</p>
<p>A couple of new control variables need to be added:</p>
<ul class="simple">
<li>Need to be able to turn off the <tt class="docutils literal">db_sync</tt> operation for rolling upgrade
testing that is not able to do the long-running sync operation.</li>
<li>Need to designate services to not be upgraded, i.e. test everything new with the
old nova-compute (<tt class="docutils literal"><span class="pre">n-cpu</span></tt>).</li>
</ul>
<p>Adding more projects to Grenade is desirable, the conclusion on the initial set:</p>
<ul class="simple">
<li>Neutron is not ready; will not be considered for Grenade at least until it
is voting in the gate.</li>
<li>Ceilometer has no Tempest tests; in order to be added to Grenade it will also
need tests backported to Tempest <tt class="docutils literal">stable/havana</tt>.</li>
<li>Heat has few Tempest tests; is considered out of scope at this time.</li>
<li>Trove needs to have Tempest tests and a Grenade plan by graduation from incubation.</li>
</ul>
<p>There has also been some desire expressed to be able to use the upgrade scripts
outside of Grenade itself.  Right now they rely heavily on DevStack components,
the work to separate that is low priority, but contributions welcome as always.</p>
</div>
<div class="section" id="openstackclient">
<h1>OpenStackClient</h1>
<p>My favorite project returned to the regular session schedule in Hong Kong.  I
conducted our talk in Portland as an Unconference session partly because I really
just wanted to talk to the group of regular comitters to sort out a plan.  That
may have been short-sighted as the level of interest and contribution dropped off
sharply.  Oops.</p>
<p>This time around dhellmann offered an Oslo slot for OSC and I snapped it up as
that is probably the least ill-fitting track for it.  That had the side
effect of prompting the question of putting OSC under Oslo organizationally.
I am OK with that even though Oslo has traditionally been focused on
libraries and reusable code.  Another that has come up before would be to
treat it as a distinct project like Horizon.  We passed on that initially
in San Francisco as the consensus was that it was not large enough to warrant
that status, and that is still the case in my view.</p>
<p>In the session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-oslo-openstack-client-update">EtherPad</a>) I reviewed the recent activities including the
0.2 release last July and the addition of unit test templates.</p>
<p>Implementation of the Objet API has begun, utilizing a new <tt class="docutils literal">restapi.py</tt> module
to perform the low-level <tt class="docutils literal">requests</tt> interface.  Why not just use swiftclient?
Good question, and at the time I was looking for an excuse to try out a
thinner approach to implementing the REST APIs.</p>
<p>I also have started work on API version detection, in parallel with a couple other
projects.  I see this as mostly a platform for testing approaches and to
free the client from requiring versions in the service catalog.</p>
<p>Future work will look into Jamie's Keystone auth refactor and leverage that
as the common REST library.  Segue...</p>
</div>
<div class="section" id="keystone-client">
<h1>Keystone Client</h1>
<p>The Keystone core devs were in the OSC session and strongly suggested I come to
their Keystone client sessions on Wednsday afternoon, which I was planning to do
anyway so my arm remained undamanged.  I finally met Jamie Lennox, who has been
doing a lot of work refactoring the auth bits of the client lib and absorbing
much of the bits Alessio started a whil eback and proposed to Oslo last May.</p>
<p>I liked most of what I heard and liked it even better after Jamie straightened out
some of my confusion-because-of-lack-of-source-code-reading at dinner Friday night.
I think we are on the same page to create the one client to rule them all and just
need to tune some details that are likely to appear after the post-summit haze clears.
And while this space doesn't officially speak for the projects I am core on, because
this is essentially my brain-dump space you, dear reader, get an advance look at
what is likely to be proposed sooner than later.</p>
<p>One CLI, one core^H^H^H^Hintegrated^H^H^H^H^H^H^Hbasic API lib, user-pluggable additional
API libs.  I see it like this:</p>
<ul class="simple">
<li>python-openstackclient - continues to be a single project focused on an ultra-consistent
command line interface; directly consumes:</li>
<li>python-os-identityclient - a new Identity API library born out of Jamie's refactoring
auth/session work with a new library API that doesn't even try to be compatible with
the old stuff.  No cli, speaks Identity v2 and v3, directly usable by all other libraries and
projects to handle authenticated communication to openStack APIs.</li>
<li>python-XXXclient - TBD how the division of the other API libraries fall out.  I want
to minimize the number of moving parts for most users and not have the higher-level
optional projects impose an undue burden on dependencies.</li>
</ul>
</div>
<div class="section" id="other-bits">
<h1>Other Bits</h1>
<p>All-in-all it was a good week, including multiple trips into the city for
sight-seeing, street-level eating, parties, 102nd story eating, death-marches down
Nathan Road in search of (open) Starbucks,
you know, all the usual stuff.  Breakfast in the airport every morning (Maxim's Deluxe
sticky-top cheese buns rule).  Catching up with team-mates over non-IRC channels.
Wondering WTF happened to jeblair's hat (my bet is HK customs impounded it,
even though afazekas managed to smuggle in his red fedora).  Wondering if Vishy and Termie
survived Macau without going broke the first night.</p>
<p>The OSF board finalized the intent to agree on an agreement on the definition
of <tt class="docutils literal">core</tt> and how it is a totally overloaded word in the OpenStack world.
Wait, I may have dreamed part of that...or all of it.  Anyway, the usage of
<a class="reference external" href="20130905-open-stack-layers.rst">layers</a> when describing
the technical relationships of the projects seems to be catching on, I heard
it at least once outside the sessions where I used it.</p>
<p>And so the OpenStack March on Atlanta begins.  I have a hunch the city will
fare better next May than it did when General Sherman came for a visit back
in the day.  And I will forever hope that there will be more carbonated
caffiene.  I think Pepsi would be a fine choice given the locale, Mountain Dew
even.  In Coke's back yard, yeah, right.</p>
<p>It is too bad we're not
coming up to the 'S' release, I'd lobby for calling it Savannah just to enjoy
watching people trying to keep track of the Savanna Savannah release.  Or
would that be the Savannah Savanna release?  See, the fun we could have!</p>
<p>'J': Not Jacksonville, they are both in the wrong state and I don't want to
type that many letters.  Let's start a campaign for 'Joyland'!</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Cloud Image Updates</title>
      <link>http://hackstack.org/x/blog/2013/10/21/cloud-image-updates</link>
      <pubDate>Mon, 21 Oct 2013 10:21:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[fedora]]></category>
      <category><![CDATA[ubuntu]]></category>
      <guid isPermaLink="false">mZYbQCavtoqgCL-_uWb2raiP9mw=</guid>
      <description>Cloud Image Updates</description>
      <content:encoded><![CDATA[<div class="document">
<p>Update!  Update!  Update!</p>
<p>A while back I started documenting the image build process I've been using for building OpenStack cloud images:</p>
<ul class="simple">
<li><a class="reference external" href="/x/blog/2013/01/25/a-fedora-18-image-for-openstack/">A Fedora 18 Image for OpenStack</a></li>
<li><a class="reference external" href="/x/blog/2013/04/25/a-centos-6-image-for-openstack/">A CentOS 6 Image for OpenStack</a></li>
</ul>
<p>Note that Ubuntu is missing from that list, due mostly to their published UEC images being generally good enough as a starting point.  Fedora 19 finally has a similar image published, let's see how different it is and if it is useful for my purposed...</p>
<p>Also, all of these images have rng-tools added as it is useful on clouds that provide a usable virtualized /dev/random in their hypervisor.</p>
<div class="section" id="fedora-19">
<h1>Fedora 19</h1>
<p>Grab the new F19 cloud image; the QCOW2 version is ready to go!  <a class="reference external" href="http://download.fedoraproject.org/pub/fedora/linux/releases/19/Images/x86_64/Fedora-x86_64-19-20130627-sda.qcow2">http://download.fedoraproject.org/pub/fedora/linux/releases/19/Images/x86_64/Fedora-x86_64-19-20130627-sda.qcow2</a>.</p>
<p>It's surprisingly close!  There aren't any standout differences save for the inclusion of sendmail and procmail, which I've specifically removed in my kickstart.</p>
<ul class="simple">
<li>install rng-tools</li>
<li>remove sendmail and/or procmail if present</li>
<li>clean up OpenSSH host keys</li>
</ul>
</div>
<div class="section" id="centos-6">
<h1>CentOS 6</h1>
<p>This is still a basic installation with a bunch of cruft removes such as all of the firmware packages that are useless in a virtual environment.</p>
<ul class="simple">
<li>stop getty on all vconsoles except /dev/tty1</li>
<li>install rng-tools</li>
<li>install cloud-init</li>
<li>teach device mapper to not auto-generate virtual network devices</li>
<li>add support for growing the root filesystem at first boot</li>
</ul>
</div>
<div class="section" id="ubuntu-12-04">
<h1>Ubuntu 12.04</h1>
<p>Just for completeness I'll list what I do:</p>
<ul class="simple">
<li>install rng-tools</li>
<li>clean up Device Mapper and OpenSSH host keys</li>
</ul>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>DevStack Local Config</title>
      <link>http://hackstack.org/x/blog/2013/09/07/devstack-local-config</link>
      <pubDate>Sat, 07 Sep 2013 09:17:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[devstack]]></category>
      <guid isPermaLink="false">XPq24BFbobrSwXVdJ6QBSd1zKF8=</guid>
      <description>DevStack Local Config</description>
      <content:encoded><![CDATA[<div class="document">
<p><em>[Updated 10 Oct 2013 to reflect the released state of ``local.conf``]</em></p>
<p>DevStack has long had an extremely simple mechanism to add arbitrary configuration entries to <tt class="docutils literal">nova.conf</tt>, <tt class="docutils literal">EXTRA_OPTS</tt>.  It was handy and even duplicated in the Neutron configuration in a number of places.  However, it did not scale well: a new variable and expansion loop is required for each file/section combination.  And now the time has come for a replacement...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">localrc</tt> has served well in its capacity of the sole container of local configuration.  Being a single file makes it easy to track and share known working DevStack configurations.  Any new configuration scheme must preserve this property.</li>
<li>In order to be able to set attributes in arbitrary configuration files and sections, those bits of information must be encoded in the new format.</li>
<li>There must be a mechanism to selectively merge the configuration values into their destination files rather than do them all at once.</li>
<li>Reduce the number of configuration variables in general that are simply passed-through to project config files.  They are being set in localrc anyway, moving that to another section of local.conf is not a difficult transition.</li>
<li>Backward-compatibility is a must; existing <tt class="docutils literal">localrc</tt> files must continue to work as expected.  In order to utilize any of the new capability, <tt class="docutils literal">localrc</tt> must be converted to the <tt class="docutils literal">local.conf</tt> <tt class="docutils literal">local</tt> section.</li>
</ul>
</blockquote>
</div>
<div class="section" id="solution">
<h1>Solution</h1>
<p>Support has been added for a new master local configuration file <tt class="docutils literal">local.conf</tt> that, like <tt class="docutils literal">localrc</tt>, resides in the root DevStack directory and is not included in the DevStack repo. <tt class="docutils literal">local.conf</tt> contains all local configuration for DevStack, including a direct replacement for <tt class="docutils literal">localrc</tt>.  It is an extended-INI format that introduces a new meta-section header containing the additional information required: a phase name and destination configuration filename.  It has the form:</p>
<pre class="literal-block">
[[ &lt;phase&gt; | &lt;filename&gt; ]]
</pre>
<p>where &lt;phase&gt; is one of a set of phase names defined below by <tt class="docutils literal">stack.sh</tt> and &lt;filename&gt; is the configuration filename.  The filename is eval'ed in the <tt class="docutils literal">stack.sh</tt> context so all environment variables are available and may be used (see example below).  Using the configuration variables from the project library scripts (e.g. <tt class="docutils literal">lib/nova</tt>) in the header is strongly suggested (see example of <tt class="docutils literal">NOVA_CONF</tt> below).</p>
<p>Configuration files specifying a path that does not exist are skipped.  This allows services to be disabled and still have configuration files present in <tt class="docutils literal">local.conf</tt>.  For example, if Nova is not enabled and <tt class="docutils literal">/etc/nova</tt> does not exist, attempts to set a value in <tt class="docutils literal">/etc/nova/nova.conf</tt> will be skipped.  If <tt class="docutils literal">/etc/nova</tt> does exist, <tt class="docutils literal">nova.conf</tt> will be created if it does not exist.  This should be mostly harmless.</p>
<p>The defined phases are:</p>
<ul class="simple">
<li><strong>local</strong> - extracts the <tt class="docutils literal">localrc</tt> section from <tt class="docutils literal">local.conf</tt> before <tt class="docutils literal">stackrc</tt> is sourced</li>
<li><strong>post-config</strong> - runs after the <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 2 services</a> are configured and before they are started</li>
<li><strong>extra</strong> - runs after services are started and before any files in <tt class="docutils literal">extra.d</tt> are executed</li>
</ul>
<p><tt class="docutils literal">local.conf</tt> is processed strictly in sequence; meta-sections may be specified more than once but if any settings are duplicated the last to appear in the file will survive.</p>
<p>The <tt class="docutils literal"><span class="pre">post-config</span></tt> phase is where most of the configuration-setting activity takes place.  In the following example, syslog and the Compute v3 API are enabled.  Note the use of <tt class="docutils literal">$NOVA_CONF</tt> to properly locate <tt class="docutils literal">nova.conf</tt>.</p>
<blockquote>
<p>[[post-config|$NOVA_CONF]]
[DEFAULT]
use_syslog = True</p>
<p>[osapi_v3]
enabled = False</p>
</blockquote>
<p>A special meta-section <tt class="docutils literal">[[local|localrc]]</tt> is used to replace the function of the old <tt class="docutils literal">localrc</tt> file.  This section is written to <tt class="docutils literal">.localrc.auto</tt> if <tt class="docutils literal">locarc</tt> does not exist; if it does exist <tt class="docutils literal">localrc</tt> is not overwritten to preserve compatability:</p>
<pre class="literal-block">
[[local|localrc]]
FIXED_RANGE=10.254.1.0/24
ADMIN_PASSWORD=speciale
LOGFILE=$DEST/logs/stack.sh.log
</pre>
</div>
<div class="section" id="implementation">
<h1>Implementation</h1>
<p>Four new functions were added to parse and merge <tt class="docutils literal">local.conf</tt> into existing INI-style config files.  The base <tt class="docutils literal">functions</tt> file is getting way too large so these functions are in <tt class="docutils literal">lib/config</tt> which will contain functions related to config file manipulation.  The existing <tt class="docutils literal">iniXXX()</tt> functions may also eventually move here.  There shall be no side-effects or global dependencies from any of the functions in <tt class="docutils literal">lib/config</tt>.</p>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">get_meta_section()</tt> - Returns an INI fragment for a specific group/filename combination</li>
<li><tt class="docutils literal">get_meta_section_files()</tt> - Returns a list of the config filenames present in <tt class="docutils literal">local.conf</tt> for a specific group</li>
<li><tt class="docutils literal">merge_config_file()</tt> - Performs the actual merge of the INI fragment from <tt class="docutils literal">local.conf</tt></li>
<li><tt class="docutils literal">merge_config_group()</tt> - Loops over the INI fragments present for the specified group and merges them</li>
</ul>
</blockquote>
<p>The merge is performed after the <tt class="docutils literal">install_XXX()</tt> and <tt class="docutils literal">configure_XXX()</tt> functions for all layer 1 and 2 projects are complete and before any services are started.</p>
</div>
<div class="section" id="the-deprecated-variables">
<h1>The Deprecated Variables</h1>
<p>The list of existing variables that will be deprecated in favor of using <tt class="docutils literal">local.conf</tt> currently includes <tt class="docutils literal">EXTRA_OPTS</tt> and a handful of <tt class="docutils literal">Q_XXX_XXX_OPTS</tt> variables in Neutron.  These are listed at the end of <tt class="docutils literal">stack.sh</tt> runs as deprecated and will be removed sometime in the Icehouse development cycle after DevStack's stable/havana branch is in place and Grenade's Grizzly-&gt;Havana upgrade is operational.</p>
</div>
<div class="section" id="examples">
<h1>Examples</h1>
<ul>
<li><p class="first">Convert EXTRA_OPTS from:</p>
<pre class="literal-block">
EXTRA_OPTS=api_rate_limit=False

to

[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False
</pre>
</li>
<li><p class="first">Convert multiple EXTRA_OPTS values from:</p>
<pre class="literal-block">
EXTRA_OPTS=(api_rate_limit=False default_log_levels=sqlalchemy=WARN)

to

[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False
default_log_levels = sqlalchemy=WARN
</pre>
</li>
<li><p class="first">Eliminate a Cinder pass-through (<tt class="docutils literal">CINDER_PERIODIC_INTERVAL</tt>):</p>
<pre class="literal-block">
[[post-config|$CINDER_CONF]]
[DEFAULT]
periodic_interval = 60
</pre>
</li>
<li><p class="first">Change a setting that has no variable:</p>
<pre class="literal-block">
[[post-config|$CINDER_CONF]]
[DEFAULT]
iscsi_helper = new-tgtadm
</pre>
</li>
<li><p class="first">Basic complete config:</p>
<pre class="literal-block">
[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False

[vmware]
host_ip = $HOST_IP
host_username = root
host_password = deepdarkunknownsecret


[[post-config|$CINDER_CONF]]
[DEFAULT]
periodic_interval = 60

vmware_host_ip = $HOST_IP
vmware_host_username = root
vmware_host_password = deepdarkunknownsecret


[[local|localrc]]
FIXED_RANGE=10.254.1.0/24
NETWORK_GATEWAY=10.254.1.1
LOGDAYS=1
LOGFILE=$DEST/logs/stack.sh.log
SCREEN_LOGDIR=$DEST/logs/screen
ADMIN_PASSWORD=quiet
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
</pre>
</li>
</ul>
</div>
</div>
]]></content:encoded>
    </item>
  </channel>
</rss>
