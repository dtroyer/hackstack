<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>HackStack Posts</title>
    <link>http://hackstack.org/x/blog</link>
    <description>OpenStack and other hackish things</description>
    <pubDate>Thu, 21 Aug 2014 19:04:10 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>OpenWRT Images for OpenStack</title>
      <link>http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack</link>
      <pubDate>Sun, 17 Aug 2014 08:17:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[openwrt]]></category>
      <guid isPermaLink="false">eYdqmkKAVncy1u3mOePk4UrR_wQ=</guid>
      <description>OpenWRT Images for OpenStack</description>
      <content:encoded><![CDATA[<div class="document">
<p>I've been playing with <a class="reference external" href="http://openwrt.org">OpenWRT</a> since &lt;mumble&gt;-&lt;mumble&gt; and have enjoyed building some of the smallest Linux images around.  While targeted at low-end home router platforms, it also runs on a wide variety of small SoC boards including the near-ubiqutious Raspberry Pi and my fave BeagleBone Black.</p>
<p>I've also been using an incredibly tiny OpenWRT instance on my laptop for years now to work around the 'interesting' network configuration of VirtualBox.  Building a set of VMs that need to talk to each other and to the outside world shouldn't be hard, so I added a router just like I have at home in a 48Mb VM.</p>
<p>While OpenStack typically doesn't have that need (but you never know how Neutron might be configured!) there are plenty of other purposes for such a small single-purpose VM.  So let's build one!</p>
<p>The magic in this cloud build is having an analogue to smoser's <a class="reference external" href="https://launchpad.net/cloud-init">cloud-init</a>.  The original is written in Python and has a lot of very useful features, but requiring the Python stdlib and <tt class="docutils literal"><span class="pre">cloud-init</span></tt> dependencies to be installed expands the size of the root image considerably.   My version, called <a class="reference external" href="https://github.com/dtroyer/openwrt-packages/tree/master/rc.cloud">rc.cloud</a>, is a set of shell scripts that implement a small subset of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> capabilities.  [Note: I 'borrowed' the original scripts from somewhere over three years ago and for the life of me can't find out where now.  Pointers welcome.]</p>
<p>One of the most important features of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> and <tt class="docutils literal">rc.cloud</tt> is configuring a network interface and enabling remote access.  OpenWRT defaults to no root password so I have to telnet to 192.168.1.1 to set root's password before Dropbear (a tiny ssh2 implementation) allows logins. Doing it with <tt class="docutils literal"><span class="pre">cloud-init</span></tt> or <tt class="docutils literal">rc.cloud</tt> instead allows automation and is a Wonderful Thing(TM).</p>
<p>This isn't a detailed How-To on building OpenWRT, there are a lot of <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">good docs</a> covering that topic.  It _is_ however, the steps I use plus some additional tweaks useful in a KVM-based OpenStack cloud.</p>
<div class="section" id="build-image-from-source">
<h1>Build Image From Source</h1>
<p>The basic build is straight out of the <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">OpenWRT wiki</a>.  I could have used the Image Builder, but I have some additional packages to include and like having control over the build configuration, such as either making sure IPv6 is present, or making sure it isn't.  And so on.</p>
<p>Configuring the OpenWRT buildroot can be a daunting task so starting with a minimal configuration is very helpful.  For a guest VM image there are a few things to consider:</p>
<ul class="simple">
<li>the VM target (Xen, KVM, etc)</li>
<li>root device name (vda2 for KVM, sda2 for others like VirtualBox)</li>
</ul>
<p>Traditionally OpenWRT has used Subversion for source control.  A move (or mirror?) on GitHub makes things easier for those of us who it it regualrly in other projects.  The <a class="reference external" href="http://wiki.openwrt.org/doc/howto/buildroot.exigence">buildroot doc</a> uses GitHub as the source so I've followed that convention.</p>
<ul>
<li><p class="first">Clone the repo:</p>
<pre class="literal-block">
git clone git://git.openwrt.org/openwrt.git
cd openwrt
</pre>
</li>
<li><p class="first">Install custom feed:</p>
<pre class="literal-block">
echo &quot;src-git dtroyer https://github.com/dtroyer/openwrt-packages&quot; &gt;&gt;feeds.conf.default
</pre>
</li>
<li><p class="first">Install packages:</p>
<pre class="literal-block">
./scripts/feeds update -a
./scripts/feeds install -a
</pre>
</li>
<li><p class="first">Check for missing packages:</p>
<pre class="literal-block">
make defconfig
</pre>
</li>
</ul>
<div class="section" id="configuration">
<h2>Configuration</h2>
<ul>
<li><p class="first">Configure:</p>
<pre class="literal-block">
make menuconfig
</pre>
</li>
<li><p class="first">Enable the following:</p>
<ul class="simple">
<li>Target System: x86</li>
<li>Subtarget: KVM guest</li>
<li>Target Images<ul>
<li><tt class="docutils literal">[*] ext4</tt></li>
<li><tt class="docutils literal">(48)</tt> Root filesystem partition size (in MB)</li>
<li><tt class="docutils literal">(/dev/vda2)</tt> Root partition on target device</li>
</ul>
</li>
<li>Base System<ul>
<li><tt class="docutils literal">{*} <span class="pre">block-mount</span></tt>  (not sure, if yes to support root fs, parted too)</li>
<li><tt class="docutils literal">&lt;*&gt; rc.cloud</tt></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Notes:</p>
<ul class="simple">
<li>Increase the root filesystem size if you do not intend to move root to another partition or increase the existing one to fit the flavor's disk size.</li>
</ul>
</div>
<div class="section" id="build">
<h2>Build</h2>
<p>It's pretty simple:</p>
<pre class="literal-block">
make -j 4
</pre>
<p>Adjust the argument to <tt class="docutils literal"><span class="pre">-j</span></tt> as appropriate for the number of CPUs on your build system.</p>
<p>When build errors occur, you'll need to run with output turned on:</p>
<pre class="literal-block">
make V=99
</pre>
</div>
</div>
<div class="section" id="configuring-the-image-for-openstack">
<h1>Configuring the Image for OpenStack</h1>
<p>As I mentioned earlier, there are a handful of changes to make to the resulting image that makes it ready for an OpenStack cloud.</p>
<ul class="simple">
<li>Set a root password - Without a root password your newly minted VM is vulnerable to a password-less telnet login if your security group rules allow that.  But more importantly, Dropbear will not allow an ssh login without a rot password.  Edit <tt class="docutils literal">/etc/shadow</tt> to set a root password</li>
<li>Configure a network interface for DHCP - This allows the first interface to obtain its IP automatically for OpenStack clouds that provide it.  Otherwise...</li>
<li>Configure <tt class="docutils literal">/etc/opkg.conf</tt> to my package repo - Packages usually need to be matched for not only their architecture but also other build flags.  Kernel modules are particularly rigid about how they can be loaded.</li>
</ul>
<div class="section" id="image-update">
<h2>Image Update</h2>
<p>All of the interesting parts below must be done as root.  So be careful.</p>
<ul>
<li><p class="first">Uncompress and copy the original image to a workspace, mount it and chroot into it:</p>
<pre class="literal-block">
gzip -dc bin/x86/openwrt-x86-kvm_guest-combined-ext4.img.gz &gt;openwrt-x86-kvm_guest-combined-ext4.img
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
mkdir -p imgroot
sudo mount -o loop /dev/mapper/loop0p2 imgroot
sudo chroot imgroot
</pre>
</li>
<li><p class="first">Make the desired changes:</p>
<ul>
<li><p class="first">Set root password:</p>
<pre class="literal-block">
sed -e '/^root/ s|^root.*$|root:\!:16270:0:99999:7:::|' -i /etc/shadow
</pre>
</li>
<li><p class="first">Configure DHCP:</p>
<pre class="literal-block">
uci set network.lan.proto=dhcp; uci commit
</pre>
</li>
<li><p class="first">Configure opkg:</p>
<pre class="literal-block">
sed -e &quot;s|http.*/x86/|http://bogus.hackstack.org/openwrt/x86/|&quot; -i /etc/opkg.conf
</pre>
</li>
</ul>
</li>
<li><p class="first">Unwind the mounted image:</p>
<pre class="literal-block">
sudo umount imgroot
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
</pre>
</li>
<li><p class="first">Upload it into Glance:</p>
<pre class="literal-block">
openstack image create --file openwrt-x86-kvm_guest-combined-ext4.img --property os-distro=OpenWRT OpenWRT

# Glance CLI
glance image-create --file openwrt-x86-kvm_guest-combined-ext4.img --name OpenWRT
</pre>
</li>
</ul>
</div>
</div>
<div class="section" id="additional-modifications">
<h1>Additional Modifications</h1>
<div class="section" id="extending-root-filesystem">
<h2>Extending Root Filesystem</h2>
<p>Even the smallest flavor gets a root disk a good bit larger than the typocal OpenWRT disk image.  One way to use that space is to increase the root filesystem.  OpenWRT has something called <tt class="docutils literal">extroot</tt> that is currently experimental and semi-undocumented, so I just took the radical move of partitioning the unused space and moving the root filesystem to the new partition.</p>
<p>Of course a real root expansion should be automated and added to <tt class="docutils literal">rc.cloud</tt> to mirror the <tt class="docutils literal"><span class="pre">cloud-init</span></tt> functionality.  Someday...</p>
<ul>
<li><p class="first">Install required packages if they're not part of the base build:</p>
<pre class="literal-block">
opkg update
opkg install block-mount parted
</pre>
</li>
<li><p class="first">Create a filesystem on the remaining disk and mount it:</p>
<pre class="literal-block">
parted /dev/vda -s -- mkpart primary  $(parted /dev/vda -m print | tail -1 | cut -d':' -f3) -0
mkfs.ext4 -L newroot /dev/vda3
mkdir -p /tmp/newroot
mount /dev/vda3 /tmp/newroot
</pre>
</li>
<li><p class="first">Copy the root filesystem:</p>
<pre class="literal-block">
mkdir -p /tmp/oldroot
mount --bind / /tmp/oldroot
tar -C /tmp/oldroot -cvf - . | tar -C /tmp/newroot -xf -
umount /tmp/oldroot
umount /tmp/newroot
</pre>
</li>
<li><p class="first">Update the GRUB bootloader to use the new partition:</p>
<pre class="literal-block">
mkdir -p /tmp/boot
mount /dev/vda1 /tmp/boot
sed -e 's/vda2/vda3/' -i /tmp/boot/boot/grub/grub.cfg
umount /tmp/boot
</pre>
</li>
<li><p class="first">Reboot</p>
</li>
</ul>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>More Notes on Windows Images</title>
      <link>http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images</link>
      <pubDate>Sun, 13 Jul 2014 07:13:00 CDT</pubDate>
      <category><![CDATA[windows]]></category>
      <category><![CDATA[openstack]]></category>
      <guid isPermaLink="false">_MyylKSqiFGJ3m4qsDhVRwgOUEs=</guid>
      <description>More Notes on Windows Images</description>
      <content:encoded><![CDATA[<div class="document">
<p>This is a follow-up to <a class="reference external" href="/x/blog/2014/07/07/windows-images-for-openstack/">Windows Images for OpenStack</a> that includes some of the notes accumulated along the way.</p>
<div class="section" id="other-docs">
<h1>Other Docs</h1>
<p>Building Windows VM images is a topic that has been done to death, but the working consensus of those I've talked to is that <a class="reference external" href="http://www.florentflament.com/blog/windows-images-for-openstack.html">Florent Flament's post</a> is one of the best guides through this minefield.</p>
</div>
<div class="section" id="metadata-server-curl-commands">
<h1>Metadata Server Curl Commands</h1>
<p>Instance UUID:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;uuid&quot;]'</blockquote>
<p>Instance Name:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;name&quot;]'</blockquote>
<p>Fixed IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/local-ipv4">http://169.254.169.254/latest/meta-data/local-ipv4</a></blockquote>
<p>Floating IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/public-ipv4">http://169.254.169.254/latest/meta-data/public-ipv4</a></blockquote>
</div>
<div class="section" id="building-on-an-openstack-cloud">
<h1>Building on an OpenStack Cloud</h1>
<p>One of the changes to the base instructions is to perform the build in an OpenStack cloud.  The compute node must have nested virtualization enabled so KVM will run, otherwise Qemu would be used and we just don't have time for that.</p>
<p>I'm going to use <a class="reference external" href="https://github.com/cloudenvy/cloudenvy">Cloudenvy</a> to manage the build VM.  It is similar to Vagrant in automating the grunt work of provisioning the VM.  The VM needs to have at least 4Gb RAM and 40Gb disk available in order to boot the seed Windows image.  This is an <tt class="docutils literal">n1.medium</tt> flavor on the private cloud I am using.</p>
<p>I am also using Ubuntu 14.04 because much of my tooling already assumes an Ubuntu build environment.  There is no technical reason that Fedora 20 could not be used, appropriate adjustments would need to be made, of course.</p>
<div class="section" id="build-vm">
<h2>Build VM</h2>
<p>I am not going to spend much time here explaining Cloudenvy's configuration, but there are two things required to not have a bad time with it.</p>
<p>Configure your cloud credentials in <tt class="docutils literal"><span class="pre">~/.cloudenvy</span></tt>:</p>
<pre class="literal-block">
cloudenvy:
    keypair_name: dev-key
    keypair_location: ~/.ssh/id_rsa-dev-key.pub
    clouds:
        cloud9:
            os_auth_url: https://cloud9.slackersatwork.com:2884/v2.0/
            os_tenant_name: demo
            os_username: demo
            os_password: secrete
</pre>
<pre class="literal-block">
project_config:
    name: imagebuilder
    image: Ubuntu 14.04
    remote_user: ubuntu
    flavor_name: n1.medium

sec_groups: [
    'tcp, 22, 22, 0.0.0.0/0',
    'tcp, 5900, 5919, 0.0.0.0/0',
    'icmp, -1, -1, 0.0.0.0/0'
]

files:
    Makefile: '~'
    ~/.cloud9.conf: '~'

provision_scripts:
    - install-prereqs.sh
</pre>
<p>The <tt class="docutils literal"><span class="pre">~/.cloud9.conf</span></tt> file is a simple script fragment that sets the <tt class="docutils literal">OS_*</tt> environment variable credentials required to authenticate using the OpenStack CLI tools.  It looks something like:</p>
<pre class="literal-block">
export OS_AUTH_URL=https://cloud9.slackersatwork.com:2884/v2.0/
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=secrete
</pre>
<p>Why do we need two sets of credentials?  Because we haven't taught Cloudenvy to read the usual environment variables yet.  I smell a pull request in my future...</p>
<p>Fire it up and log in:</p>
<pre class="literal-block">
envy up
envy ssh
</pre>
<p>At this point we can switch over to Flament's process.</p>
<p>Or we can use the cloudbase auto-answer template</p>
<p>Get the ISO:</p>
<pre class="literal-block">
&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
for i in aa ab ac ad ae af ag ah; do \
    swift download windows7 en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i; \
    cat en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i &gt;&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
done
</pre>
<p>sudo ./make-floppy.sh</p>
<hr class="docutils" />
<pre class="literal-block">
# add keypair if not already there
os keypair create --public-key ~/.ssh/id_rsa.pub $(hostname -s)

# Create VM
os server create \
  --image &quot;Ubuntu 14.04&quot; \
  --flavor n1.tiny \
  --key-name bunsen \
  --user-data cconfig.txt \
  --wait \
  dt-1

export IP=$(os server show dt-1 -f value -c addresses | cut -d '=' -f2)

# Go to there
ssh ubuntu&#64;$IP
</pre>
<hr class="docutils" />
<p>Now on to Florent's steps</p>
<ul>
<li><p class="first">Create a virtual disk</p>
<blockquote>
<p>qemu-img create -f qcow2 Windows-Server-2008-R2.qcow2 9G</p>
</blockquote>
</li>
<li><p class="first">Boot the install VM</p>
</li>
</ul>
<pre class="literal-block">
kvm \
    -m 2048 \
    -cdrom &lt;WINDOWS_INSTALLER_ISO&gt; \
    -drive file=Windows-Server-2008-R2.qcow2,if=virtio \
    -drive file=&lt;VIRTIO_DRIVERS_ISO&gt;,index=3,media=cdrom \
    -net nic,model=virtio \
    -net user \
    -nographic \
    -vnc :9 \
    -k fr \
    -usbdevice tablet
</pre>
<p>Connect via VNC to :9</p>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Windows Images for OpenStack</title>
      <link>http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack</link>
      <pubDate>Mon, 07 Jul 2014 07:07:00 CDT</pubDate>
      <category><![CDATA[windows]]></category>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[virtualbox]]></category>
      <guid isPermaLink="false">NEdHQUMnNDPOA2ZMbkUe63N2oT0=</guid>
      <description>Windows Images for OpenStack</description>
      <content:encoded><![CDATA[<div class="document">
<p>There is no shortage of articles online about building Windows images for use in various clouds.  What there is a shortage of are working articles on building these images unattended.  The Windows unattended install process has been basically solved, even if still a bit arcane.  But finding more than a trivial example of doing it in a cloud is sparse.</p>
<p>Cloudbase has <a class="reference external" href="https://github.com/cloudbase/windows-openstack-imaging-tools">shared the tooling</a> they created for building their Windows images.  That makes a good base for an automated build process that can be tailored to your particular needs.  in addition to being the authors of cloudbase-init, their GitHub account is a trove of resources for Windows admins.</p>
<p>Since I had a Windows 7 Professional ISO handy I used that for the example...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<p>The resulting image must:</p>
<ul class="simple">
<li>have RedHat's VirtIO drivers installed</li>
<li>use <tt class="docutils literal"><span class="pre">cloudbase-init</span></tt> for metadata handling</li>
</ul>
</div>
<div class="section" id="build-in-virtualbox">
<h1>Build In VirtualBox</h1>
<p>The Cloudbase process is designed to perform the build using KVM.  Ideally, it would be possible to boot a VM in an OpenStack cloud from the install ISO and let it go, but it turns out this is hard.  The unattended install process  requires two ISO images and a floppy image attached to the VM in addition to the target disk device. OpenStack currently has no way to do all of these attachments.  The alternative is to stash autounattend.xml and the virtio drivers in the Windows install ISO, but this requires a rebuild/upload for _every_ change to the install scripts.</p>
<p>So normally this means a Linux on bare-metal install is required.  How hard is it to dig up a laptop with Trusty on it?  Hard, if you're me.</p>
<p>I've heard that using VirtualBox doesn't work for some reason, but these reasons haven't been made clear to me so I didn't know I couldn't do what I'm describing here.</p>
<div class="section" id="auto-answer-changes">
<h2>Auto Answer Changes</h2>
<p>One of the main change to Cloudbase's setup is to put the PowerShell scripts on the floppy with Autounattend.xml.  This ensures that the files are matched together and changes in the repo doesn't break our working setup.</p>
<p>The Autounattend.xml file has a couple of changes other than those required to run the script from the floppy:</p>
<ul class="simple">
<li>Add the MetaData value for Win7</li>
<li>Since this is Win7, we need to enable the account stanza</li>
<li>Install a public product key</li>
<li>Fix a spacing error in the Microsoft-Windows-International-Core-WinPE component element</li>
</ul>
</div>
<div class="section" id="powershell-script-changes">
<h2>PowerShell Script Changes</h2>
<p>The primary change to the PowerShell scripts is to remove the file downloads and retrieve them from the floppy instead.</p>
</div>
<div class="section" id="make-the-floppy-image">
<h2>Make the Floppy Image</h2>
<p>I used <a class="reference external" href="/x/files/make-floppy.sh">this script</a> to create the floppy image, it builds a new image, mounts it, copies the appropriate Autounattend.xml and PowerShell scripts and other files, then umnounts the image.</p>
</div>
<div class="section" id="build-vm-configuration">
<h2>Build VM Configuration</h2>
<p>Automating a VBox build includes creating the VM to be used.  The <tt class="docutils literal">VBoxManage</tt> tool is the simple way to do this from a script and that's exactly what I've done here.</p>
<p>It turns out that 16Gb is not enough for Windows 7 installation once all of the updates are installed.  There are a LOT of them, 157 at this writing.  Even though this only needs to be done once, it takes a long time to apply them and it might be worthwhile to obtain media with the updates pre-applied.</p>
<p>The commands here are taken from the <tt class="docutils literal"><span class="pre">build-vb.sh</span></tt> script.</p>
<p>Create a new empty VM and disk:</p>
<pre class="literal-block">
BASE_NAME='win-build'
# Create a new empty VM
VBoxManage createvm --name &quot;$BASE_NAME&quot; --ostype &quot;$OS_TYPE&quot; --register
VBoxManage createhd --filename &quot;$VM_DIR/$BASE_NAME.vdi&quot; --size $DISK_SIZE
</pre>
<p>The disk configuration is an important part of this process so everything is found as required.  In addition to the install disk and install ISO a second ISO must be mounted containing the VirtIO drivers and a floppy image with the Autounattend.xml and Powershell scripts:</p>
<pre class="literal-block">
# SATA Controller
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;SATA&quot; --add sata
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;SATA&quot; --type hdd \
    --port 0 --device 0 --medium &quot;$VM_DIR/$BASE_NAME.vdi&quot;

# Make IDE disks
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;IDE&quot; --add ide
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 0  --device 0 --medium &quot;$WIN_ISO&quot;
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 1  --device 0 --medium &quot;$VIRTIO_ISO&quot;

# Floppy disk image
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;Floppy&quot; --add floppy
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;Floppy&quot; --type fdd \
    --port 0 --device 0 --medium &quot;$FLOPPY&quot;
</pre>
<p>Do the remaining basic configuration, including a virtio NIC to tickle Windows to install the drivers:</p>
<pre class="literal-block">
# General Config
VBoxManage modifyvm &quot;$BASE_NAME&quot; --cpus 2
VBoxManage modifyvm &quot;$BASE_NAME&quot; --memory $RAM_SIZE --vram 24
VBoxManage modifyvm &quot;$BASE_NAME&quot; --ioapic on

VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic1 nat --bridgeadapter1 e1000g0
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic2 nat
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nictype2 virtio

VBoxManage modifyvm &quot;$BASE_NAME&quot; --boot1 dvd --boot2 disk --boot3 none --boot4 none
</pre>
<p>Kick off the build process:</p>
<pre class="literal-block">
VBoxManage startvm &quot;$BASE_NAME&quot; --type gui
</pre>
<p>Convert the disk from VDI to QCOW2 format for uploading into the image store:</p>
<pre class="literal-block">
qemu-img convert -p -O qcow &quot;$VM_DIR/$BASE_NAME.vdi&quot; &quot;$VM_DIR/$BASE_NAME.qcow&quot;
</pre>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Client Layers</title>
      <link>http://hackstack.org/x/blog/2014/02/11/client-layers</link>
      <pubDate>Tue, 11 Feb 2014 02:11:00 CST</pubDate>
      <category><![CDATA[openstackclient sdk]]></category>
      <guid isPermaLink="false">Pw-YTmh6zBnZVMMNm4kmIzUYZ78=</guid>
      <description>Client Layers</description>
      <content:encoded><![CDATA[<div class="document">
<p>[Work In Progress, you have been warned]</p>
<p>So clients are like pie.  Creamy, gooey, butterscotch cream pie with meringue.  Known at the in-laws house as Baby Bear Pie for reasons unknown-to-me.  Meringue is yummy but not much by itself.  Pie crust does its job most of the time without being noticed, unless it is sub-par.  It's the cream filling that gets all of the attention.  Butterscotch, lemon or chocolate, that's where the glory is.</p>
<p>REST clients are like pie, what with their multiple layers of communication handlers, data marshallers and output formatters and all.  So lets talk client crust.  It is the least sexy of the layers, going about its job, semi-appreciated when it is right, scorned when it is bad and otherwise taken for granted.  In OpenStack we have a large number of client projects that all talk to REST APIs.  Without going too far into the history, most of these are forks of forks and all have been independently enhanced and updated and none of them have more than a familial resemblance to each other.</p>
<p>Alessio Ababilov tried to fix this, actually making a working common REST layer for the (at the time) 5 or so client libraries.  This was proposed to oslo-incubator and has gained some traction of late.  Some things that did get merged early on were the change to use the Requests package to replace httplib2, but that did nothing to unify the low-level internal API.</p>
<p>Rather than try to fix the legacy clients the right solution here is to define some requirements and build a solid common foundation that API libraries can build on.  Rather than call it crust, let's use the even-less-sexy 'transport layer' name, totally mis-appropriated from the OSI stack.</p>
<p>Independent of the Oslo apiclient work, Jamie Lennox rebuilt the transport layer of keystoneclient as part of a refactor of the authentication bits into pluggable classes.  This happens to be excitingly close to what I had been prototyping in OpenStackClient and was possibly the biggest highlight of the week in Hong Kong.</p>
<p>So lets see if we can't turn that into a generic SDK-style transport layer for our clients.  On top of that we will layer the basic OpenStack API version discovery, authentication and service catalog.</p>
<div class="section" id="layer-1-transport-layer">
<h1>Layer 1: Transport Layer</h1>
<p>The Transport Layer includes the basic components that implement the REST API client and essentially is a wrapper around the Python Requests package with some additional header handling and logging built in.</p>
<div class="section" id="design-notes">
<h2>Design Notes</h2>
<p>The rationale for some of the differences from the 'traditional' client structure:</p>
<ul class="simple">
<li>There is only 1 client (HTTPClient) instance.  This takes the role similar to OpenStackClient's ClientManager class.  It handles the authentication for all APIs one time and contains the instances of the specific API client objects, which now are little more than containers for their Manager class instances.</li>
</ul>
</div>
<div class="section" id="namespace">
<h2>Namespace</h2>
<p>Everything lives under the top-level <tt class="docutils literal">openstack</tt> namespace</p>
<ul class="simple">
<li><tt class="docutils literal">openstack.restapi</tt> - The layer 1 transport and base classes (session, exceptions) and  base layer 2 classes (discovery, base clients, service catalog)</li>
<li><tt class="docutils literal">openstack.restapi.auth</tt> - The base authentication classes</li>
<li><tt class="docutils literal">openstack.restapi.identity</tt>  - API-specific classes required for layer 2 operation (identity client)</li>
<li><tt class="docutils literal">openstack.client.identity</tt> - The layer 2 classes for the Identity API (<tt class="docutils literal">.v2_0</tt>, <tt class="docutils literal">.v3</tt>)</li>
<li><tt class="docutils literal"><span class="pre">openstack.client.&lt;api&gt;</span></tt> - Other layer 2 API classes</li>
</ul>
</div>
<div class="section" id="openstack-restapi-session-session">
<h2>openstack.restapi.session.Session</h2>
<p>Session is the lowest layer, basically a wrapper that adds the following to requests.Session:</p>
<ul class="simple">
<li>create a new requests.Session instance if one is not supplied (using requests.Session implies the TLS control lies here and is one reason for passing in an existing Session)</li>
<li>populate the X-Auth-Token header from an auth object contained by the Session that implements a get_token() method</li>
<li>populate headers as required: User-Agent, Forwarded, Content-Type</li>
<li>change requests' redirect handling to be more appropriate for an API</li>
<li>include wrappers for the REST methods: head(), get(), post(), put(), delete(), patch()</li>
<li>debug logging of request/response data</li>
</ul>
</div>
<div class="section" id="openstack-restapi-baseclient-client">
<h2>openstack.restapi.baseclient.Client</h2>
<p>The base Client class defines the methods that reflect into the Session.</p>
<ul class="simple">
<li>create a new Session instance if one is not supplied</li>
<li>contains a ServiceCatalog instance (applications requiring multiple identity contexts at a time should use multiple Client instances)</li>
<li>performs the API version discovery (see ApiVersion class below)</li>
<li>define the cache interface for client-side caching of API data</li>
<li>include wrappers for the REST methods: head(), get(), post(), put(), delete(), patch()</li>
</ul>
</div>
<div class="section" id="openstack-restapi-base-baseauthplugin">
<h2>openstack.restapi.base.BaseAuthPlugin</h2>
<p>The abstract auth plugin class</p>
<blockquote>
<ul class="simple">
<li>handles the specifics of authenticating a user and providing a token to Session when requested via get_token()</li>
</ul>
</blockquote>
</div>
<div class="section" id="openstack-restapi-httpclient-httpclient-baseclient-client-base-baseauthplugin">
<h2>openstack.restapi.httpclient.HTTPClient(baseclient.Client, base.BaseAuthPlugin)</h2>
<p>HTTPClient is the primary interface used by the project API layers (gooey-creamy!).</p>
<ul class="simple">
<li>creates a ServiceCatalog from the token received from Identity</li>
<li>uses <tt class="docutils literal">keyring</tt> to cache tokens</li>
<li>authenticate() calls get_raw_token_from_identity_service()</li>
</ul>
</div>
<div class="section" id="openstack-restapi-access-accessinfo">
<h2>openstack.restapi.access.AccessInfo</h2>
<p>Base class for auth plugins</p>
<ul class="simple">
<li>defines the basic auth interface</li>
<li>AccessInfoV2</li>
<li>AccessInfoV3</li>
</ul>
</div>
</div>
<div class="section" id="layer-2-discovery">
<h1>Layer 2: Discovery</h1>
<p>Discovery rides just above the transport layer and is the logic used to determine the best API version available between those support by the server and the client.</p>
<div class="section" id="openstack-restapi-api-discovery-apiversion">
<h2>openstack.restapi.api_discovery.ApiVersion</h2>
<p>A resource class for API versions used by BaseVersion</p>
<ul class="simple">
<li>normalizes version information</li>
</ul>
</div>
<div class="section" id="openstack-restapi-api-discovery-baseversion">
<h2>openstack.restapi.api_discovery.BaseVersion</h2>
<p>The root class for API version discovery.</p>
<ul class="simple">
<li>queries API server for supported version information</li>
<li>normalizes both server and client versions</li>
<li>select the appropriate version from those availalble (if possible)</li>
</ul>
</div>
<div class="section" id="openstack-restapi-identity-client-identityversion-api-discovery-baseversion">
<h2>openstack.restapi.identity.client.IdentityVersion(api_discovery.BaseVersion)</h2>
<p>A Version discovery class that handles the peculiarities of Keystone</p>
<ul class="simple">
<li>optionally removes 'v2.0' from the auth_url to do proper discovery on old-style deployment configurations</li>
<li>normalizes the returned dict to remove the <tt class="docutils literal">values</tt> key</li>
</ul>
</div>
</div>
<div class="section" id="layer-2-authentication">
<h1>Layer 2: Authentication</h1>
</div>
<div class="section" id="layer-2-service-catalog">
<h1>Layer 2: Service Catalog</h1>
</div>
<div class="section" id="examples">
<h1>Examples</h1>
<div class="section" id="create-a-session-with-private-ca-certificates">
<h2>Create A Session With Private CA Certificates</h2>
<pre class="literal-block">
session = api_session.Session(
    verify=ca_certificate_file,
    user_agent=USER_AGENT,
)
</pre>
</div>
<div class="section" id="add-a-base-client">
<h2>Add A Base Client</h2>
<pre class="literal-block">
client = httpclient.HTTPClient(
    session=session,
    auth_url=&quot;https://localhost:5000&quot;,
    project_name=&quot;sez-me-street&quot;,
    username=&quot;bert&quot;,
    password=&quot;pidgeon&quot;,
)
</pre>
</div>
<div class="section" id="identity-version-discovery">
<h2>Identity Version Discovery</h2>
<pre class="literal-block">
# Supported Identity client classes
API_VERSIONS = {
    '2.0': 'keystoneclient.v2_0.client.Client',
    '3': 'keystoneclient.v3.client.Client',
}

ver = identity.client.IdentityVersion(
    clients=API_VERSIONS.keys(),
    auth_url=&quot;https://localhost:5000&quot;,
)
print &quot;client class: %s=%s&quot; % (ver.client_version.id, API_VERSIONS[ver.server_version.id])
</pre>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenStackClient Plugins</title>
      <link>http://hackstack.org/x/blog/2014/01/14/openstackclient-plugins</link>
      <pubDate>Tue, 14 Jan 2014 01:14:00 CST</pubDate>
      <category><![CDATA[openstackclient]]></category>
      <guid isPermaLink="false">w5QmePmIOSDsbwWXApSH5dR1Eu0=</guid>
      <description>OpenStackClient Plugins</description>
      <content:encoded><![CDATA[<div class="document">
<p>OpenStackClient (OSC) has been in my project queue for almost two years now.  It was Feb 2012 that I stayed up all night mucking about with something called DrStack with the goal of combining the then four OpenStack CLI binaries into a single command set.</p>
<p>OSC is the second major realization of that goal having a greatly improved internal command architecture courtesy of dhellmann's Cliff framework.  It also somehow got an informal blessing without becoming an official project, a status that it still carries.  We have a roadmap of where to go with that but that is a topic for another day.</p>
<p>Today we talk plugins!</p>
<p>Yes, I know, that is an overused term in OpenStack, where everything seems to be a plugin or an extension or optional in some way.  But I don't have anything better at the moment so plugin it is.</p>
<div class="section" id="openstackclient-plugins">
<h1>OpenStackClient Plugins</h1>
<p>OSC development has been a series of quiet periods interspersed with bouts of furious activity.  Fast-forward to last month (Dec 2013) and the introduction of a viable command plugin system for OSC first released in version 0.3.0.  The OSC side is documented in the <a class="reference external" href="http://docs.openstack.org/developer/python-openstackclient/plugins.html">OpenStackClient Developer Documentation</a>.</p>
<div class="section" id="goals">
<h2>Goals</h2>
<p>The previous OSC plugin mechanism was too naive and did not allow for adding client objects to the ClientManager. We needed to:</p>
<ul class="simple">
<li>define an initialization to add global options for API versions and whatnot (parser phase)</li>
<li>define an initialization function(s) to select an API version add an appropriate client to the ClientManager (client phase)</li>
</ul>
</div>
<div class="section" id="implementation">
<h2>Implementation</h2>
<p>As an exercise to validate the completeness of the plugin mechanism, the Compute, Image and Volume API commands were converted to initialize via the plugin mechanism.  The only difference from an external plugin is that they are included in the OSC repo.</p>
<p>The new plugin mechanism builds on the use of <a class="reference external" href="https://pypi.python.org/pypi/cliffâ€Ž">Cliff</a> to dynamically load the command classes and modifies the existing OSC <tt class="docutils literal">ClientManager</tt> to define the client objects to be instantiated at run-time.  This allows additional clients to insert themselves into the <tt class="docutils literal">ClientManager</tt>.</p>
<p>OSC looks for plugins that register their client class under <tt class="docutils literal">openstack.cli.extension</tt>.  The key is the API name and must be unique for all plugins, the value is the module name that contains the public initialization functions.</p>
<p>The initialization module is typically names <tt class="docutils literal"><span class="pre">&lt;project-name&gt;.client</span></tt>, although there is no technical requirement to follow this convention.  It was adopted as that was already the name of the modules used by the built-in API classes.</p>
<p>The initialization module must implement a set of constants that are used to identify the plugin and two functions that instantiate the actual API client class and define any global options required.</p>
</div>
</div>
<div class="section" id="python-oscplugin">
<h1>python-oscplugin</h1>
<p>Since most actual OSC plugins are not going to be part of the repo, we created a sample plugin in a stand-alone project to demonstrate the bits required.  <a class="reference external" href="https://github.com/dtroyer/python-oscplugin">python-oscplugin</a> began life as a <a class="reference external" href="https://github.com/openstack-dev/cookiecutter">cookiecutter</a> project (worth using to bootstrap a project in the OpenStack-way) and expanded to become a simple demonstration of an OSC command plugin.</p>
<p>So let's walk through the sample plugin to see how this works...</p>
<div class="section" id="plugin-initialization">
<h2>Plugin Initialization</h2>
<p>It all starts with the initialization module, here named <tt class="docutils literal">oscplugin.plugin</tt>, defining the rest of the identifiers used to locate plugin bits.  Naming this module is flexible, it just needs to be specified in the <tt class="docutils literal">openstack.cli.extension</tt> entry point group.</p>
<ul class="simple">
<li><tt class="docutils literal">API_NAME</tt> - A short string describing the API or command set name.  It is used in the entry point group name and is the key name in the <tt class="docutils literal">openstack.cli.extension</tt> group to identify the plugin.  Must be a valid Python identifier string.</li>
<li><tt class="docutils literal">API_VERSION_OPTION</tt> - An optional name of the API version attribute used in the global command-line options to specify an API version.  It will be used in <tt class="docutils literal">build_option_parser()</tt> if setting an API version is required.  Must be a valid Python identifier string.</li>
<li><tt class="docutils literal">API_VERSIONS</tt> - A dict mapping version strings to client class names.</li>
</ul>
<p>Two functions are required that perform the initialization work for the plugin.</p>
<ul class="simple">
<li><tt class="docutils literal">build_option_parser()</tt> - The top-level parser object is passed in and available to add plugin-specific options, usually an API version selector.</li>
<li><tt class="docutils literal">make_client()</tt> - Instantiate the actual client object taking in to consideration any version that may be specified.  The mapping of version to client class is handled here.  Also, any authentication or service selection the specific client requires is passed in here.</li>
</ul>
</div>
<div class="section" id="client-api">
<h2>Client API</h2>
<p><tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> contains its own equivalent to a client API object.  In this case it is just a placeholder as the <tt class="docutils literal">plugin</tt> commands do not have an external client library.  For most API clients this is the actual client class, such as <tt class="docutils literal">glanceclient.v2.client.Client</tt> for the Image v2 API.</p>
<p>There are cases where the API client class is insufficient for some reason and adaptations are required.  The Image v1 client is a good example.  The ImageManager class in <tt class="docutils literal">glanceclient</tt> does not have a <tt class="docutils literal">find()</tt> method so we implemented one in <tt class="docutils literal">openstackclient.image.client.Client_v1</tt> that uses <tt class="docutils literal">openstackclient.image.client.ImageManager_v1</tt> with added <tt class="docutils literal">find()</tt> and <tt class="docutils literal">findall()</tt> methods.</p>
</div>
<div class="section" id="commands">
<h2>Commands</h2>
<p>The commands implemented in <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> are in <tt class="docutils literal">oscplugin.v1.plugin</tt> and follow the basic pattern used by the other OSC command classes.  Again they are mostly placeholders here.</p>
</div>
<div class="section" id="tests">
<h2>Tests</h2>
<p>The structure of the tests also follows that of the existing OSC API commands.  They use <tt class="docutils literal">mock</tt> and fakes to perform unit tests on the command classes.</p>
</div>
<div class="section" id="project-stuff">
<h2>Project Stuff</h2>
<p>Ad <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> was created using <a class="reference external" href="https://github.com/openstack-dev/cookiecutter">cookiecutter</a> it includes the usual OpenStack features such as <tt class="docutils literal">pbr</tt> and friends.  The specific bits pertaining to an OSC plugin:</p>
<ul>
<li><p class="first"><tt class="docutils literal">setup.cfg</tt> - All of the plugin-specific content is in the <tt class="docutils literal">[entry_points]</tt> section:</p>
<pre class="literal-block">
[entry_points]
openstack.cli.extension =
    oscplugin = oscplugin.plugin

openstack.oscplugin.v1 =
    plugin_list = oscplugin.v1.plugin:ListPlugin
    plugin_show = oscplugin.v1.plugin:ShowPlugin
</pre>
</li>
</ul>
<p>Note that OSC defines the group name as <tt class="docutils literal"><span class="pre">openstack.&lt;api-name&gt;.v&lt;version&gt;</span></tt>
so the version should not contain the leading 'v' character.</p>
<ul class="simple">
<li><tt class="docutils literal">requirements.txt</tt> - We've added  <tt class="docutils literal">openstackclient</tt> as <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> is useless without it.  <tt class="docutils literal">keystoneclient</tt> is here too, while <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> does not require it, most OpenStack API clients will.  <tt class="docutils literal">cliff</tt> is also needed here.</li>
<li><tt class="docutils literal"><span class="pre">test-requirements.txt</span></tt> - <tt class="docutils literal">mock</tt> is required for testing.</li>
</ul>
</div>
<div class="section" id="a-note-about-versions">
<h2>A Note About Versions</h2>
<p>Internally OSC uses the convention <tt class="docutils literal">vXXX</tt> for version identifiers, where <tt class="docutils literal">XXX</tt> is a valid Python identifier in its own right (i.e., uses '_' rather than '.' internally).  OSC adds the leading 'v' so versions expressed in constant declarations should not include it.</p>
</div>
</div>
<div class="section" id="eot">
<h1>EOT</h1>
<p>The plugin structure should allow any base install of OSC to be extended simply by installing the desired client package.  Af of right now there are no other clients that implement the plugin, but that will be changing soon.  Film at eleven...</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenStack Icehouse Developer Summit</title>
      <link>http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit</link>
      <pubDate>Sun, 10 Nov 2013 11:10:00 CST</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[devstack]]></category>
      <guid isPermaLink="false">3SuDmuCE-gxe1BOWhZ-SaroxFcc=</guid>
      <description>OpenStack Icehouse Developer Summit</description>
      <content:encoded><![CDATA[<div class="document">
<p>OpenStack has had a global reach since the early days but the Design Summits
have always been a US-based affair.  Last week we finally took the every-six-month
roadshow off-continent and ventured out to Hong Kong.
Of course the Conference is co-located and concurrent but I didn't make it to
any of those sessions this time and only knew it was there by going to lunch in
the expo hall and seeing some familiar vendor faces.</p>
<p>We begin with the projects most subject to my attention, DevStack, Grenade and
OpenStackClient.</p>
<div class="section" id="devstack">
<h1>DevStack</h1>
<p>This is the first summit where DevStack has program status and thus its own
track of two back-to-back sessions.  I hear russellb is jealous...</p>
<div class="section" id="new-bits">
<h2>New Bits</h2>
<p>The DevStack 'New Bits' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstacks-new-bits">EtherPad</a>)
was spent talking about a couple of the significant additions
to DevStack late in the Havana cycle.  I wrote about the
<a class="reference external" href="/x/blog/2013/09/07/devstack-local-config">local config</a>
work as it was being developed, the discussion in the session was primarily a Q&amp;A.
One bit that was covered was converting devstack-gate to use this form rather
than <tt class="docutils literal">localrc</tt>.</p>
<p>The other major DevStack addition is a plugin mechanism
to configure and start additional services without requiring changes to DevStack.
This is partially intended for new projects to be able to use DevStack for their
Jenkins testing without requiring them to be added to the DevStack repo.</p>
<p>This is an expansion of the existing hook into <tt class="docutils literal">extras.d</tt> that automagically
ran scripts at the end of <tt class="docutils literal">stack.sh</tt>.  These scripts are essentially
dispatchers as they are called multiple times from <tt class="docutils literal">stack.sh</tt>, <tt class="docutils literal">unstack.sh</tt> and
<tt class="docutils literal">clean.sh</tt>.  <a class="reference external" href="http://devstack.org/plugins.html">devstack.org</a> has an example of an
<tt class="docutils literal">extras.d</tt> dispatch script.</p>
<p>Savanna and Tempest have been converted to the plugin format with Marconi in progress.
Most of the remaining <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 4</a>
projects should also be able to be converted to the plugin format.</p>
<p>Other highlights, some of which I intend to cover here in the future:</p>
<ul class="simple">
<li>bash8 - style testing for Bash scripts similar to hacking/pep8/flake8;
there is interest in this becoming a stand-alone project if it proves to be useful</li>
<li>DevStack tests - the addition of <tt class="docutils literal">run_tests.sh</tt> provides a familiar, if
deprecated, interface to running <tt class="docutils literal">bash8</tt> and other tests</li>
<li>exercises - the DevStack exercises are now unused in all gate testing except
Grenade's <em>base</em> phase.  As they are still generally useful outside the
gate test environment a new Jenkins job needs to be added to check them for bit rot.</li>
</ul>
</div>
<div class="section" id="distro-support">
<h2>Distro Support</h2>
<p>sdague led the DevStack 'Distro Support' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstack-support">EtherPad</a>)
discussing distro supported status and what we need to do to bring the current
ones up to snuff and what might be required of new additions.</p>
<p>The primary requirement to adding the support tag is the ability to have it tested
in the DevStack gate.  Unfortunately, neither of the clouds that provide
test resources to our CI infrastructure
(HP Cloud and Rackspace Cloud Servers)
allow arbitrary images to be uploaded so only distros that have supported images
are able to be tested.  The third-party testing hooks might be able to be used
to mitigate some of this but the resources for that testing will need to be supplied.</p>
<p>There was also some discussion around projects getting supported status from DevStack.
A lot of this is a timing and process issue for incubation/integration process wanting
to see testing before graduation from those steps but not wanting to add projects
to DevStack that are not on that track.  The addition of the extras.d capability
for projects to be easily added to DevStack without modifying it goes a long way
toward setting up the needed testing to demonstrate the capability of the project and
team before actually adding it to the repo.</p>
<p>The flow will look like:</p>
<ul class="simple">
<li>third-party testing in StackForge will utilize the extras.d plugins to do the
required pre-incubation testing</li>
<li>after incubation, the project gets added to the DevStack repo (still utilizing the
plugin mech) and added to the gate as a requirement for graduation to integrated status.</li>
</ul>
</div>
</div>
<div class="section" id="grenade">
<h1>Grenade</h1>
<p>The Grenade session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-qa-grenade">EtherPad</a>) focused mostly on expanding the test matrix
of <tt class="docutils literal">base</tt> and <tt class="docutils literal">target</tt> releases that need testing.  This includes tests from
stable releases to trunk and stable release updates as well as from
stable release updates to next stable or trunk.</p>
<p>A couple of new control variables need to be added:</p>
<ul class="simple">
<li>Need to be able to turn off the <tt class="docutils literal">db_sync</tt> operation for rolling upgrade
testing that is not able to do the long-running sync operation.</li>
<li>Need to designate services to not be upgraded, i.e. test everything new with the
old nova-compute (<tt class="docutils literal"><span class="pre">n-cpu</span></tt>).</li>
</ul>
<p>Adding more projects to Grenade is desirable, the conclusion on the initial set:</p>
<ul class="simple">
<li>Neutron is not ready; will not be considered for Grenade at least until it
is voting in the gate.</li>
<li>Ceilometer has no Tempest tests; in order to be added to Grenade it will also
need tests backported to Tempest <tt class="docutils literal">stable/havana</tt>.</li>
<li>Heat has few Tempest tests; is considered out of scope at this time.</li>
<li>Trove needs to have Tempest tests and a Grenade plan by graduation from incubation.</li>
</ul>
<p>There has also been some desire expressed to be able to use the upgrade scripts
outside of Grenade itself.  Right now they rely heavily on DevStack components,
the work to separate that is low priority, but contributions welcome as always.</p>
</div>
<div class="section" id="openstackclient">
<h1>OpenStackClient</h1>
<p>My favorite project returned to the regular session schedule in Hong Kong.  I
conducted our talk in Portland as an Unconference session partly because I really
just wanted to talk to the group of regular comitters to sort out a plan.  That
may have been short-sighted as the level of interest and contribution dropped off
sharply.  Oops.</p>
<p>This time around dhellmann offered an Oslo slot for OSC and I snapped it up as
that is probably the least ill-fitting track for it.  That had the side
effect of prompting the question of putting OSC under Oslo organizationally.
I am OK with that even though Oslo has traditionally been focused on
libraries and reusable code.  Another that has come up before would be to
treat it as a distinct project like Horizon.  We passed on that initially
in San Francisco as the consensus was that it was not large enough to warrant
that status, and that is still the case in my view.</p>
<p>In the session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-oslo-openstack-client-update">EtherPad</a>) I reviewed the recent activities including the
0.2 release last July and the addition of unit test templates.</p>
<p>Implementation of the Objet API has begun, utilizing a new <tt class="docutils literal">restapi.py</tt> module
to perform the low-level <tt class="docutils literal">requests</tt> interface.  Why not just use swiftclient?
Good question, and at the time I was looking for an excuse to try out a
thinner approach to implementing the REST APIs.</p>
<p>I also have started work on API version detection, in parallel with a couple other
projects.  I see this as mostly a platform for testing approaches and to
free the client from requiring versions in the service catalog.</p>
<p>Future work will look into Jamie's Keystone auth refactor and leverage that
as the common REST library.  Segue...</p>
</div>
<div class="section" id="keystone-client">
<h1>Keystone Client</h1>
<p>The Keystone core devs were in the OSC session and strongly suggested I come to
their Keystone client sessions on Wednsday afternoon, which I was planning to do
anyway so my arm remained undamanged.  I finally met Jamie Lennox, who has been
doing a lot of work refactoring the auth bits of the client lib and absorbing
much of the bits Alessio started a whil eback and proposed to Oslo last May.</p>
<p>I liked most of what I heard and liked it even better after Jamie straightened out
some of my confusion-because-of-lack-of-source-code-reading at dinner Friday night.
I think we are on the same page to create the one client to rule them all and just
need to tune some details that are likely to appear after the post-summit haze clears.
And while this space doesn't officially speak for the projects I am core on, because
this is essentially my brain-dump space you, dear reader, get an advance look at
what is likely to be proposed sooner than later.</p>
<p>One CLI, one core^H^H^H^Hintegrated^H^H^H^H^H^H^Hbasic API lib, user-pluggable additional
API libs.  I see it like this:</p>
<ul class="simple">
<li>python-openstackclient - continues to be a single project focused on an ultra-consistent
command line interface; directly consumes:</li>
<li>python-os-identityclient - a new Identity API library born out of Jamie's refactoring
auth/session work with a new library API that doesn't even try to be compatible with
the old stuff.  No cli, speaks Identity v2 and v3, directly usable by all other libraries and
projects to handle authenticated communication to openStack APIs.</li>
<li>python-XXXclient - TBD how the division of the other API libraries fall out.  I want
to minimize the number of moving parts for most users and not have the higher-level
optional projects impose an undue burden on dependencies.</li>
</ul>
</div>
<div class="section" id="other-bits">
<h1>Other Bits</h1>
<p>All-in-all it was a good week, including multiple trips into the city for
sight-seeing, street-level eating, parties, 102nd story eating, death-marches down
Nathan Road in search of (open) Starbucks,
you know, all the usual stuff.  Breakfast in the airport every morning (Maxim's Deluxe
sticky-top cheese buns rule).  Catching up with team-mates over non-IRC channels.
Wondering WTF happened to jeblair's hat (my bet is HK customs impounded it,
even though afazekas managed to smuggle in his red fedora).  Wondering if Vishy and Termie
survived Macau without going broke the first night.</p>
<p>The OSF board finalized the intent to agree on an agreement on the definition
of <tt class="docutils literal">core</tt> and how it is a totally overloaded word in the OpenStack world.
Wait, I may have dreamed part of that...or all of it.  Anyway, the usage of
<a class="reference external" href="20130905-open-stack-layers.rst">layers</a> when describing
the technical relationships of the projects seems to be catching on, I heard
it at least once outside the sessions where I used it.</p>
<p>And so the OpenStack March on Atlanta begins.  I have a hunch the city will
fare better next May than it did when General Sherman came for a visit back
in the day.  And I will forever hope that there will be more carbonated
caffiene.  I think Pepsi would be a fine choice given the locale, Mountain Dew
even.  In Coke's back yard, yeah, right.</p>
<p>It is too bad we're not
coming up to the 'S' release, I'd lobby for calling it Savannah just to enjoy
watching people trying to keep track of the Savanna Savannah release.  Or
would that be the Savannah Savanna release?  See, the fun we could have!</p>
<p>'J': Not Jacksonville, they are both in the wrong state and I don't want to
type that many letters.  Let's start a campaign for 'Joyland'!</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Cloud Image Updates</title>
      <link>http://hackstack.org/x/blog/2013/10/21/cloud-image-updates</link>
      <pubDate>Mon, 21 Oct 2013 10:21:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[fedora]]></category>
      <category><![CDATA[ubuntu]]></category>
      <guid isPermaLink="false">mZYbQCavtoqgCL-_uWb2raiP9mw=</guid>
      <description>Cloud Image Updates</description>
      <content:encoded><![CDATA[<div class="document">
<p>Update!  Update!  Update!</p>
<p>A while back I started documenting the image build process I've been using for building OpenStack cloud images:</p>
<ul class="simple">
<li><a class="reference external" href="/x/blog/2013/01/25/a-fedora-18-image-for-openstack/">A Fedora 18 Image for OpenStack</a></li>
<li><a class="reference external" href="/x/blog/2013/04/25/a-centos-6-image-for-openstack/">A CentOS 6 Image for OpenStack</a></li>
</ul>
<p>Note that Ubuntu is missing from that list, due mostly to their published UEC images being generally good enough as a starting point.  Fedora 19 finally has a similar image published, let's see how different it is and if it is useful for my purposed...</p>
<p>Also, all of these images have rng-tools added as it is useful on clouds that provide a usable virtualized /dev/random in their hypervisor.</p>
<div class="section" id="fedora-19">
<h1>Fedora 19</h1>
<p>Grab the new F19 cloud image; the QCOW2 version is ready to go!  <a class="reference external" href="http://download.fedoraproject.org/pub/fedora/linux/releases/19/Images/x86_64/Fedora-x86_64-19-20130627-sda.qcow2">http://download.fedoraproject.org/pub/fedora/linux/releases/19/Images/x86_64/Fedora-x86_64-19-20130627-sda.qcow2</a>.</p>
<p>It's surprisingly close!  There aren't any standout differences save for the inclusion of sendmail and procmail, which I've specifically removed in my kickstart.</p>
<ul class="simple">
<li>install rng-tools</li>
<li>remove sendmail and/or procmail if present</li>
<li>clean up OpenSSH host keys</li>
</ul>
</div>
<div class="section" id="centos-6">
<h1>CentOS 6</h1>
<p>This is still a basic installation with a bunch of cruft removes such as all of the firmware packages that are useless in a virtual environment.</p>
<ul class="simple">
<li>stop getty on all vconsoles except /dev/tty1</li>
<li>install rng-tools</li>
<li>install cloud-init</li>
<li>teach device mapper to not auto-generate virtual network devices</li>
<li>add support for growing the root filesystem at first boot</li>
</ul>
</div>
<div class="section" id="ubuntu-12-04">
<h1>Ubuntu 12.04</h1>
<p>Just for completeness I'll list what I do:</p>
<ul class="simple">
<li>install rng-tools</li>
<li>clean up Device Mapper and OpenSSH host keys</li>
</ul>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>DevStack Local Config</title>
      <link>http://hackstack.org/x/blog/2013/09/07/devstack-local-config</link>
      <pubDate>Sat, 07 Sep 2013 09:17:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[devstack]]></category>
      <guid isPermaLink="false">XPq24BFbobrSwXVdJ6QBSd1zKF8=</guid>
      <description>DevStack Local Config</description>
      <content:encoded><![CDATA[<div class="document">
<p><em>[Updated 10 Oct 2013 to reflect the released state of ``local.conf``]</em></p>
<p>DevStack has long had an extremely simple mechanism to add arbitrary configuration entries to <tt class="docutils literal">nova.conf</tt>, <tt class="docutils literal">EXTRA_OPTS</tt>.  It was handy and even duplicated in the Neutron configuration in a number of places.  However, it did not scale well: a new variable and expansion loop is required for each file/section combination.  And now the time has come for a replacement...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">localrc</tt> has served well in its capacity of the sole container of local configuration.  Being a single file makes it easy to track and share known working DevStack configurations.  Any new configuration scheme must preserve this property.</li>
<li>In order to be able to set attributes in arbitrary configuration files and sections, those bits of information must be encoded in the new format.</li>
<li>There must be a mechanism to selectively merge the configuration values into their destination files rather than do them all at once.</li>
<li>Reduce the number of configuration variables in general that are simply passed-through to project config files.  They are being set in localrc anyway, moving that to another section of local.conf is not a difficult transition.</li>
<li>Backward-compatibility is a must; existing <tt class="docutils literal">localrc</tt> files must continue to work as expected.  In order to utilize any of the new capability, <tt class="docutils literal">localrc</tt> must be converted to the <tt class="docutils literal">local.conf</tt> <tt class="docutils literal">local</tt> section.</li>
</ul>
</blockquote>
</div>
<div class="section" id="solution">
<h1>Solution</h1>
<p>Support has been added for a new master local configuration file <tt class="docutils literal">local.conf</tt> that, like <tt class="docutils literal">localrc</tt>, resides in the root DevStack directory and is not included in the DevStack repo. <tt class="docutils literal">local.conf</tt> contains all local configuration for DevStack, including a direct replacement for <tt class="docutils literal">localrc</tt>.  It is an extended-INI format that introduces a new meta-section header containing the additional information required: a phase name and destination configuration filename.  It has the form:</p>
<pre class="literal-block">
[[ &lt;phase&gt; | &lt;filename&gt; ]]
</pre>
<p>where &lt;phase&gt; is one of a set of phase names defined below by <tt class="docutils literal">stack.sh</tt> and &lt;filename&gt; is the configuration filename.  The filename is eval'ed in the <tt class="docutils literal">stack.sh</tt> context so all environment variables are available and may be used (see example below).  Using the configuration variables from the project library scripts (e.g. <tt class="docutils literal">lib/nova</tt>) in the header is strongly suggested (see example of <tt class="docutils literal">NOVA_CONF</tt> below).</p>
<p>Configuration files specifying a path that does not exist are skipped.  This allows services to be disabled and still have configuration files present in <tt class="docutils literal">local.conf</tt>.  For example, if Nova is not enabled and <tt class="docutils literal">/etc/nova</tt> does not exist, attempts to set a value in <tt class="docutils literal">/etc/nova/nova.conf</tt> will be skipped.  If <tt class="docutils literal">/etc/nova</tt> does exist, <tt class="docutils literal">nova.conf</tt> will be created if it does not exist.  This should be mostly harmless.</p>
<p>The defined phases are:</p>
<ul class="simple">
<li><strong>local</strong> - extracts the <tt class="docutils literal">localrc</tt> section from <tt class="docutils literal">local.conf</tt> before <tt class="docutils literal">stackrc</tt> is sourced</li>
<li><strong>post-config</strong> - runs after the <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 2 services</a> are configured and before they are started</li>
<li><strong>extra</strong> - runs after services are started and before any files in <tt class="docutils literal">extra.d</tt> are executed</li>
</ul>
<p><tt class="docutils literal">local.conf</tt> is processed strictly in sequence; meta-sections may be specified more than once but if any settings are duplicated the last to appear in the file will survive.</p>
<p>The <tt class="docutils literal"><span class="pre">post-config</span></tt> phase is where most of the configuration-setting activity takes place.  In the following example, syslog and the Compute v3 API are enabled.  Note the use of <tt class="docutils literal">$NOVA_CONF</tt> to properly locate <tt class="docutils literal">nova.conf</tt>.</p>
<blockquote>
<p>[[post-config|$NOVA_CONF]]
[DEFAULT]
use_syslog = True</p>
<p>[osapi_v3]
enabled = False</p>
</blockquote>
<p>A special meta-section <tt class="docutils literal">[[local|localrc]]</tt> is used to replace the function of the old <tt class="docutils literal">localrc</tt> file.  This section is written to <tt class="docutils literal">.localrc.auto</tt> if <tt class="docutils literal">locarc</tt> does not exist; if it does exist <tt class="docutils literal">localrc</tt> is not overwritten to preserve compatability:</p>
<pre class="literal-block">
[[local|localrc]]
FIXED_RANGE=10.254.1.0/24
ADMIN_PASSWORD=speciale
LOGFILE=$DEST/logs/stack.sh.log
</pre>
</div>
<div class="section" id="implementation">
<h1>Implementation</h1>
<p>Four new functions were added to parse and merge <tt class="docutils literal">local.conf</tt> into existing INI-style config files.  The base <tt class="docutils literal">functions</tt> file is getting way too large so these functions are in <tt class="docutils literal">lib/config</tt> which will contain functions related to config file manipulation.  The existing <tt class="docutils literal">iniXXX()</tt> functions may also eventually move here.  There shall be no side-effects or global dependencies from any of the functions in <tt class="docutils literal">lib/config</tt>.</p>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">get_meta_section()</tt> - Returns an INI fragment for a specific group/filename combination</li>
<li><tt class="docutils literal">get_meta_section_files()</tt> - Returns a list of the config filenames present in <tt class="docutils literal">local.conf</tt> for a specific group</li>
<li><tt class="docutils literal">merge_config_file()</tt> - Performs the actual merge of the INI fragment from <tt class="docutils literal">local.conf</tt></li>
<li><tt class="docutils literal">merge_config_group()</tt> - Loops over the INI fragments present for the specified group and merges them</li>
</ul>
</blockquote>
<p>The merge is performed after the <tt class="docutils literal">install_XXX()</tt> and <tt class="docutils literal">configure_XXX()</tt> functions for all layer 1 and 2 projects are complete and before any services are started.</p>
</div>
<div class="section" id="the-deprecated-variables">
<h1>The Deprecated Variables</h1>
<p>The list of existing variables that will be deprecated in favor of using <tt class="docutils literal">local.conf</tt> currently includes <tt class="docutils literal">EXTRA_OPTS</tt> and a handful of <tt class="docutils literal">Q_XXX_XXX_OPTS</tt> variables in Neutron.  These are listed at the end of <tt class="docutils literal">stack.sh</tt> runs as deprecated and will be removed sometime in the Icehouse development cycle after DevStack's stable/havana branch is in place and Grenade's Grizzly-&gt;Havana upgrade is operational.</p>
</div>
<div class="section" id="examples">
<h1>Examples</h1>
<ul>
<li><p class="first">Convert EXTRA_OPTS from:</p>
<pre class="literal-block">
EXTRA_OPTS=api_rate_limit=False

to

[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False
</pre>
</li>
<li><p class="first">Convert multiple EXTRA_OPTS values from:</p>
<pre class="literal-block">
EXTRA_OPTS=(api_rate_limit=False default_log_levels=sqlalchemy=WARN)

to

[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False
default_log_levels = sqlalchemy=WARN
</pre>
</li>
<li><p class="first">Eliminate a Cinder pass-through (<tt class="docutils literal">CINDER_PERIODIC_INTERVAL</tt>):</p>
<pre class="literal-block">
[[post-config|$CINDER_CONF]]
[DEFAULT]
periodic_interval = 60
</pre>
</li>
<li><p class="first">Change a setting that has no variable:</p>
<pre class="literal-block">
[[post-config|$CINDER_CONF]]
[DEFAULT]
iscsi_helper = new-tgtadm
</pre>
</li>
<li><p class="first">Basic complete config:</p>
<pre class="literal-block">
[[post-config|$NOVA_CONF]]
[DEFAULT]
api_rate_limit = False

[vmware]
host_ip = $HOST_IP
host_username = root
host_password = deepdarkunknownsecret


[[post-config|$CINDER_CONF]]
[DEFAULT]
periodic_interval = 60

vmware_host_ip = $HOST_IP
vmware_host_username = root
vmware_host_password = deepdarkunknownsecret


[[local|localrc]]
FIXED_RANGE=10.254.1.0/24
NETWORK_GATEWAY=10.254.1.1
LOGDAYS=1
LOGFILE=$DEST/logs/stack.sh.log
SCREEN_LOGDIR=$DEST/logs/screen
ADMIN_PASSWORD=quiet
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
</pre>
</li>
</ul>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenStack - Seven Layer Dip as a Service</title>
      <link>http://hackstack.org/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service</link>
      <pubDate>Thu, 05 Sep 2013 09:05:00 CDT</pubDate>
      <category><![CDATA[openstack]]></category>
      <category><![CDATA[rant]]></category>
      <guid isPermaLink="false">n_L3VOh8Wa6r8DsnH3_0f0RV7GE=</guid>
      <description>OpenStack - Seven Layer Dip as a Service</description>
      <content:encoded><![CDATA[<div class="document">
<p>OpenStack is, as it name implies, a stack of services to provide &quot;components for a cloud infrastructure solution&quot;. <a class="footnote-reference" href="#id2" id="id1">[1]</a>  There are layers of services, some interdependent on each other, some only dependent on the layers below it.</p>
<p>For some time there has been a PC dance around 'labelling' projects that may or may not be at a layer that it wants to be in.  Back in the day, the term 'core' was thrown around to identify the services necessary to build an OpenStack deployment.  That term has been so misused and coopted and stomped on as to become unusable for technical discussions.  The OpenStack Foundation Board has an effort ongoing to define what 'core' means but they are focused on who and what is required in a deployment in order to use the trademarked OpenStack[tm] name and logo and not any determination as to layering of projects.  Go team, but that is not what we in the coding trenches need.</p>
<p>'Integrated' has become the term-du-jour for the TC to identify those projects that are part of the announced OpenStack release a the end of the development cycle.  That clearly identifies those projects that are administratively included but has no meaning for technical relationship/interface considerations.</p>
<p>For the sake of argument I am going to co-opt another term, stealing it directly from OSI networking terminology: 'layer'.  Layer is used there to describe the boundaries and interfaces between the functional components.  In OpenStack, the layers we have are the base infrastructure required to make something work, the additional services to make things integrate well with its surroundings and the services provided to the system and its users.  Really general terms there.</p>
<div class="section" id="layer-definitions">
<h1>Layer Definitions</h1>
<div class="section" id="layer-0-operating-systems-and-libraries">
<h2>Layer 0: Operating Systems and Libraries</h2>
<p>[Because Real Programmers start with zero, right?]</p>
<p>OpenStack is built on top of the existing projects and technology that do the grunt work.  For completeness we will include the underlying components in Layer 0 even though these pieces are not part of OpenStack proper.</p>
<p>There are also a number of libraries specific to OpenStack (even though some may be useful elsewhere) that the other projects are dependent on but are not themselves operational services.  Most of these are encapsulated in the Oslo project.</p>
</div>
<div class="section" id="layer-1-the-basics">
<h2>Layer 1: The Basics</h2>
<p>The OpenStack stack begins with the Infrastructure as a Service.  This is the layer that everything else builds on.  This is also the focus of three of the four early OpenStack projects, once called 'core projects'.  But we've thrown out that c-word so now let's agree that this is simply 'OpenStack Layer 1'.  These are the interdependent services that form a minimal operational system and have no other OpenStack dependencies.</p>
<blockquote>
<ul class="simple">
<li>Identity (Keystone)</li>
<li>Image (Glance)</li>
<li>Compute (Nova)</li>
</ul>
</blockquote>
<p>Thats it.  Really.  At least until Nova Networking is removed and Network (Neutron) moves in to this layer as a required service for every deployment.</p>
<p>While this is contrary to what the board is saying regarding the definition of 'core', they are talking about user experience and legal definitions where I am talking about technical and architectural relationships.</p>
<p>Since Essex, most OpenStack services rely on Keystone to provide Identity services; Swift still is able to be deployed in a stand-alone configuration.  Nova requires Glance to supply bootable images.  Glance is able to use Swift if it is available and must be specifically configured to do so..  Similarly, Nova is able to use Cinder and Neutron if they are available and must also be configured to use them.</p>
</div>
<div class="section" id="layer-2-extending-the-base">
<h2>Layer 2: Extending the Base</h2>
<p>Layer 2 services have the characteristic that they only depend on the services in Layer 1 and that Layer 1 services may be configured to use Layer 2 servies if available.  Nearly all deployments will include at least some of these services.</p>
<blockquote>
<ul class="simple">
<li>Network (Neutron)</li>
<li>Volume (Cinder)</li>
<li>Object (Swift)</li>
<li>Bare-metal (Ironic) - status: in incubation</li>
</ul>
</blockquote>
<p>Neutron will eventually become a Layer 1 service when Nova Networking is removed.</p>
<p>Ironic technically sits below Nova but is optional so it is in Layer 2.</p>
</div>
<div class="section" id="layer-3-the-options">
<h2>Layer 3: The Options</h2>
<p>Layer 3 services are optional from a functional point of view but valuable in deployments that integrate with the world around them.  They integrate with Layer 1 and 2 services and are dependent on them for operation.</p>
<blockquote>
<ul class="simple">
<li>Web UI (Horizon)</li>
<li>Notification (Ceilometer)</li>
</ul>
</blockquote>
</div>
<div class="section" id="layer-4-turtles-all-the-way-up">
<h2>Layer 4: Turtles All The Way Up</h2>
<p>Layer 4 catches everything else with an OpenStack sticker on the box.  This includes the rest of the XXaaS services and everything that is purely user facing, i.e. the OpenStack deployment itself does not depend on the service, it is only used by customers of cloud services.</p>
<blockquote>
<ul class="simple">
<li>Orchestration (Heat)</li>
<li>DBaaS (Trove) - status: to be integrated in Icehouse</li>
<li>DNSaaS (Moniker) - status: applying for incubation</li>
<li>MQaaS (Marconi) - status: in incubation</li>
</ul>
</blockquote>
</div>
</div>
<div class="section" id="relationships">
<h1>Relationships</h1>
<p>What does all this mean?  Probably not much outside of the following projects.  Really it is just a framework for terminology to describe and categorize projects by their purely technical relationships.</p>
<div class="section" id="devstack">
<h2>DevStack</h2>
<p>DevStack has struggled to keep from overgrowing its playpen and contain the effects of everyone with a project to pitch wanting to get it included.  Some basic hooks have been added to <tt class="docutils literal">stack.sh</tt> to allow projects not explicitly supported in the DevStack repo to be included in <tt class="docutils literal">stack</tt>/<tt class="docutils literal">unstack</tt> operations.  More hooks are coming in the near future as <tt class="docutils literal">stack.sh</tt> continues to get streamlined and make the projects follow a common template for installation/configuration/startup/etc.</p>
<p>DevStack's goal is to (soon!) clearly define the layers of services so developers can focus on the layers they care about and still have the ability to build the whole she-bang.  The DevStack layer scripts will also be hookable to allow additional (non-Integrated? non-Incubated?) projects the ability to self-integrate into DevStack without being in the repo.</p>
<p>The layered approach will be to install and configure the layers in order, with the exception that Layer 1 startup will be delayed until Layer 2 configuration is complete to allow the configuration changes to take effect.</p>
</div>
<div class="section" id="grenade">
<h2>Grenade</h2>
<p>Grenade only performs upgrade runs on Layer 1 and 2 services at the most, even then not including (yet?) all Layer 2 services.  Additional layers can only be added once a project is part of the DevStack stable release used as the Grenade <tt class="docutils literal">base</tt> release.</p>
</div>
<div class="section" id="openstackclient">
<h2>OpenStackClient</h2>
<p>OSC is not an official OpenStack project or program despite its existence in the OpenStack namespace on GitHub as it began before those concepts were fully-formed.  So in some regards it is not bound to the rules and conventions that apply to the other projects.  However, to do otherwise would be foolish.</p>
<p>OSC uses the Layers in determining the priorities for implementation of client commands.  It currently has implementations for Identity, Image, Volume and Compute APIs with plans for Object and Network to come.  It does have a simple plug-in capability that allows additional modules to be added independently without being part of the OSC repo.</p>
</div>
</div>
<div class="section" id="epilogue">
<h1>Epilogue</h1>
<p>[Quinn Martin Productions TV shows always had these, remember? Anyone?]</p>
<p>Other projects may or may not pick up this terminology, it depends on if it turns out to be useful to them.  There is a technical hierarchy of projects even if not everyone wants to acknowledge it, and the need for avoiding the existing hot-button terms seems to be increasing.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Stolen directly from <a class="reference external" href="http://www.openstack.org/">openstack.org</a></td></tr>
</tbody>
</table>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>OpenStack Clients</title>
      <link>http://hackstack.org/x/blog/2013/06/20/openstack-clients</link>
      <pubDate>Thu, 20 Jun 2013 06:20:00 CDT</pubDate>
      <category><![CDATA[windows]]></category>
      <category><![CDATA[openstack]]></category>
      <guid isPermaLink="false">ORQg9UezE3vW3Fmm8VvDLdWq53E=</guid>
      <description>OpenStack Clients</description>
      <content:encoded><![CDATA[<h2 id="openstack-client-projects">OpenStack Client Projects</h2>
<p>The developers of OpenStack maintain a series of <a href="https://wiki.openstack.org/wiki/ProjectTypes">library projects</a> which are the Python interfaces to the OpenStack REST APIs and also include command-line clients:</p>
<ul>
<li><a href="http://launchpad.net/python-ceilometerclient">python-ceilometerclient</a></li>
<li><a href="http://launchpad.net/python-cinderclient">python-cinderclient</a></li>
<li><a href="http://launchpad.net/python-glanceclient">python-glanceclient</a></li>
<li><a href="http://launchpad.net/python-heatclient">python-heatclient</a></li>
<li><a href="http://launchpad.net/python-keystoneclient">python-keystoneclient</a></li>
<li><a href="http://launchpad.net/python-novaclient">python-novaclient</a></li>
<li><a href="http://launchpad.net/python-quantumclient">python-quantumclient</a></li>
<li><a href="http://launchpad.net/python-swiftclient">python-swiftclient</a></li>
</ul>
<p>Each project is managed through the same development process as the integrated OpenStack projects so you can expect to find the latest source on <a href="http://github.com/openstack">GitHub</a>. The master branch in the project repositories should theoretically never be 'broken,' but realistically they are not tested between releases with the same vigor as the core projects. The bug and feature tracking happens on Launchpad; each of the projects above are linked to their respective Launchpad projects.</p>
<p>The client libraries are simply REST (HTTP) API clients and are backward compatible with the core supported API versions. For example, <code>python-novaclient</code> works with any version of Nova that supports matching API versions.  The client projects are versioned and released to PyPI independently of the integrated OpenStack releases.  There is no 'Grizzly' version of <code>python-novaclient</code>, for example, but any <code>python-novaclient</code> released after Grizzly's release will be compatible as long as the same API versions are enabled.</p>
<h2 id="installing-the-clients">Installing the Clients</h2>
<p>Official releases of the clients are distributed by developers through <a href="http://pypi.python.org">PyPi</a>.  Some Linux distributions also package the clients in their native format (RPM, APT, etc).  As the client projects are still evolving quite rapidly, the packages distributed by the distributions can fall out of date.  However, the client packages distributed with Grizzly server packages will be known to be compatible with Grizzly.</p>
<p>Users who want to be curent or are working with OpenStack development releases will want to install the clients from PyPi. As there are drawbacks to using PyPi both methods will be covered here.</p>
<p>Most of the installation steps here require administrative privileges.  Python virtual environments (virtualenvs) can be used to work around this if necessary, in addition to their other benefits (see below).</p>
<h3 id="python-runtime">Python Runtime</h3>
<p>OpenStack command line clients consist of a set of Python modules and their dependencies. There are three layers to the Python stack: a Python runtime, the Python modules that provide an interface to PyPI and the client library modules and their dependencies.  All supported platforms (Linux, OS X and Windows) have all of these layers but only Windows doesn't include any of them in the box so everything from the ground up needs to be installed.  And there is more than one way to do it.</p>
<p>The OpenStack client libraries are officially supported on Python 2.6 and 2.7.  While Python 3 is also available for all of these platforms, the work to support it in the clients is underway but not yet complete.</p>
<h4 id="linux-installation">Linux Installation</h4>
<p>Linux distributions usually include Python installed by default.  While all recent releases are Python 2.6 or 2.7, some long-term-support distributions may still contain Python 2.5 or older and require a newer Python runtime.  For example, <a href="https://wiki.openstack.org/wiki/NovaInstall/CentOSNotes#CentOS_5.2F_RHEL_5_.2F_Oracle_Enterprise_Linux_5">the OpenStack wiki</a> documents installing Nova on RHEL 5 and friends.  From that document the steps to enable the EPEL repo and install Python 2.6 are sufficient to support installing the client libraries.</p>
<h4 id="os-x-installation">OS X Installation</h4>
<p>All OS X releases since 10.6 (Snow Leopard) include a supported Python runtime although it is usually a few minor versions behind the current release.  Alternatives are available to install current versions of Python but are out of scope here.</p>
<p>OS X 10.5 (Leopard) includes Python 2.5.1 and needs to have particular considerations addressed in order to update it.  See the <a href="http://wiki.python.org/moin/MacPython/Leopard">Leopard wiki page</a> for more information.  </p>
<h4 id="windows-installation">Windows Installation</h4>
<p>Windows has a couple of options for Python installations.  Each Python release includes official Python binaries for both 32-bit and 64-bit Windows. The python.org <a href="http://www.python.org/getit/windows/">Windows releases</a> page lists some of the other Python runtime packages that are available.  One additional that will be familiar to UNIX users living in a Windows world is the <a href="http://www.cygwin.com/">Cygwin</a> Python port. Once Cygwin's Python interpreter is installed the rest is very similar to the steps here.</p>
<p>This guide will use the official 32 bit 2.7.5 runtime on Windows 7 as the example installation but it also works on XP and Vista.  The Python interpreter can be installed anywhere, the default folder is <code>C:\Python27</code>.  If you change it remember to make the corresponding change in the rest of this guide.  Also, be aware that putting it in certain places, such as <code>Program files</code>, will cause Windows UAC in Vista and newer to require an administrative token to perform module installs.  While not impossible to deal with, this is currently beyond the scope of this guide.</p>
<ul>
<li>
<p>Download and install the <a href="http://www.python.org/ftp/python/2.7.5/python-2.7.5.msi">Windows runtime installer</a>:</p>
<ul>
<li><strong>Select whether to install Python for all users of this computer</strong>: Select 'Install for all users'</li>
<li><strong>Select Destination Directory</strong>: The default Destination Directory is <code>C:\Python27\</code>.</li>
<li><strong>Customize Python</strong>: The default selections are fine.  At a minimum the
  <strong>Register Extensions</strong> and <strong>Utility Scripts</strong> selections should be enabled.</li>
</ul>
</li>
<li>
<p>Add the destination directory to the System PATH via Control Panel:</p>
<ul>
<li>On Windows XP: <strong>Control Panel â†’ System â†’ Advanced â†’ Environment Variables</strong></li>
<li>On Windows 7: <strong>Control Panel â†’ System and Security â†’ System â†’ Advanced system settings â†’ Environment Variables</strong></li>
<li>Edit the Path entry in the <strong>System variables</strong> list</li>
<li>Add the Python installation path and the Python scripts directory to the beginning
  of the Path variable: <code>C:\Python27;C:\Python27\Scripts;</code></li>
</ul>
</li>
</ul>
<p>Open a command prompt window and test the Python installation:</p>
<pre><code>Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\fozzier&gt;python
Python 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)] on win32
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>
<h3 id="python-module-distribution">Python Module Distribution</h3>
<p>In addition to the base Python runtime some additional modules are required to bootstrap an environment for the OpenStack client install.  The state of packaging in the Python world can be describes as 'in flux' at best.  That said, OpenStack uses the Python Package Index (PyPI) as its source of released packaged modules.</p>
<p>PyPI provides a mechanism to install released versions of Python libraries and tools directly.  The <code>pip</code> command is the interface to PyPI and performs the download and install functions as well as dependency resolution (albiet occasionally poorly).  It does not play well with packages installed by the native package managers on some systems (<em>cough</em> Red Hat <em>cough</em>). The former is a shortcoming that continues to be slowly addressed by the Python community but the latter can be treated with a tool called <code>virtualenv</code> (see below).</p>
<p>Many Python modules have also been packaged by Linux distributions and can be installed using the native package manager.  Often it is beneficial to install the vendor packages for hybrid modules especially if a C compiler is not present, or not desired, on the system.  The consensus in the OpenStack community is not to mix the two methods any more than necessary.</p>
<h4 id="pypi-and-pip">PyPI and pip</h4>
<p>The <a href="http://www.pip-installer.org/">pip</a> command must be installed to use PyPI and for non-native package installations that is best done using <code>easy_install</code> which itself needs to be installed as part of the <code>setuptools</code> module.  Check to see if <code>setuptools</code> is installed:</p>
<pre><code>python -c "import setuptools"
</code></pre>
<p>If <code>setuptools</code> is not installed an error similar to this will be displayed:</p>
<pre><code>Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
ImportError: No module named setuptools
</code></pre>
<ul>
<li>
<p>If necessary, install <a href="https://pypi.python.org/pypi/setuptools/0.7.4">setuptools</a> using the <a href="https://bitbucket.org/pypa/setuptools/raw/0.7.4/ez_setup.py">ez_setup.py</a> script:</p>
<pre><code>python ez_setup.py
</code></pre>
</li>
<li>
<p>Install <code>pip</code> using <code>easy_install</code>:</p>
<pre><code>easy_install pip
</code></pre>
</li>
</ul>
<h4 id="hybrid-python-modules">Hybrid Python Modules</h4>
<p>Some common Python modules are not pure Python and require a C compiler to install from PyPI.  On Linux these are generally installed via native system packages.  On Windows many of these packages also have Windows binary installers that can be used with the official Python runtime. </p>
<p>OpenStack's Glance client requires <code>pyOpenSSL</code> which is one of these hybrid packages.  On Linux install the vendor-supplied package. OS X 10.7 and newer include an acceptable version.  On Windows it can be installed from PyPI directly using the supplied binary Windows install package:</p>
<ul>
<li>
<p>Download and install the <a href="https://pypi.python.org/packages/2.7/p/pyOpenSSL/pyOpenSSL-0.13.winxp32-py2.7.msi">pyOpenSSL installer</a>:</p>
<ul>
<li><strong>Select whether to install Python for all users of this computer</strong>: Select 'Install for all users'</li>
<li><strong>Select Python Installations</strong>: The default Python installation should be the one installed above.  Use it.</li>
</ul>
</li>
</ul>
<h4 id="openstack-client-libraries">OpenStack Client Libraries</h4>
<p>Installing the client libraries from PyPI will also bring in the required dependencies.  This step is the same for all platforms.</p>
<ul>
<li>
<p>Install the client libraries from PyPI:</p>
<pre><code>pip install python-keystoneclient python-novaclient python-cinderclient \
  python-glanceclient python-swiftclient
</code></pre>
</li>
</ul>
<h4 id="virtualenv">virtualenv</h4>
<p>Using <code>pip</code> in conjunction with a tool called <code>virtualenv</code> can be used to isolate the PyPi packages you install from your system packages. Install <code>virtualenv</code> using <code>pip</code>:</p>
<pre><code>pip install virtualenv
</code></pre>
<p>A new virtual environment is created and activated with the following commands:</p>
<pre><code>virtualenv ~/openstack-venv
source ~/openstack-venv/bin/activate
</code></pre>
<p>Once activated all packages installed with <code>pip</code> will be placed into the virtual environment without affecting or conflicting with the root system:</p>
<pre><code>pip install python-novaclient
</code></pre>
<p>Deactivating your virtual evironment is as simple as this:</p>
<pre><code>deactivate
</code></pre>
<p>For those of you that want to level-up your <code>virtualenv</code> experience, use a tool called <code>virtualenvwrapper</code>. It abstracts away the management of the virtual environment directories on your local system:</p>
<pre><code>mkvirtualenv openstack-venv
workon openstack-venv

pip install python-novaclient

deactivate
rmvirtualenv openstack-venv
</code></pre>
<h3 id="distro-specific-package-managers">Distro-specific Package Managers</h3>
<p>There are a couple of tradeoffs when consuming packages from distro-managed repositories. In the case of the OpenStack clients, development happens so rapidly that these repositories can grow stale very quickly. In the case that you still want to use a distro-specific package manager, it should be as simple as installing the python-*client packages. For example, here's how you can install python-novaclient on Ubuntu:</p>
<pre><code>apt-get install python-novaclient
</code></pre>
<h2 id="using-the-clients">Using the Clients</h2>
<h3 id="authentication">Authentication</h3>
<p>The first thing to tackle is authentication. Each of the OpenStack clients supports a set of common command-line arguments for this:</p>
<pre><code>--os-username
--os-password
--os-tenant-name
--os-auth-url
</code></pre>
<p>For example, the following is how you would list Nova instances while authenticating as the user <code>bcwaldon</code> on the tenant <code>devs</code> with the password <code>snarf</code> against the authentication endpoint <code>http://auth.example.com:5000/v2.0</code>:</p>
<pre><code>nova --os-username bcwaldon --os-password snarf --os-tenant-name devs \ 
     --os-auth-url http://auth.example.com:5000/v2.0 list
</code></pre>
<p>Alternatively, the OpenStack clients offer the same configuration through environment variables:</p>
<pre><code>export OS_USERNAME=bcwaldon
export OS_PASSWORD=snarf
export OS_TENANT_NAME=devs
expot OS_AUTH_URL=http://auth.example.com:5000/v2.0
nova list
</code></pre>
<h3 id="discovering-commands">Discovering Commands</h3>
<p>New features and commands are added to the client projects just about as quickly as the upstream core project development happens, so it is suggested that you </p>
<p>Each of the openstack client projects have a <code>help</code> command that will print a list of available commands:</p>
<pre><code>% cinder help
usage: cinder [--version] [--debug] [--os-username &lt;auth-user-name&gt;]
              [--os-password &lt;auth-password&gt;]
              [--os-tenant-name &lt;auth-tenant-name&gt;]
              [--os-tenant-id &lt;auth-tenant-id&gt;] [--os-auth-url &lt;auth-url&gt;]
              [--os-region-name &lt;region-name&gt;] [--service-type &lt;service-type&gt;]
              [--service-name &lt;service-name&gt;]
              [--volume-service-name &lt;volume-service-name&gt;]
              [--endpoint-type &lt;endpoint-type&gt;]
              [--os-volume-api-version &lt;compute-api-ver&gt;]
              [--os-cacert &lt;ca-certificate&gt;] [--retries &lt;retries&gt;]
              &lt;subcommand&gt; ...

Command-line interface to the OpenStack Cinder API.

Positional arguments:
  &lt;subcommand&gt;
    absolute-limits     Print a list of absolute limits for a user
    backup-create       Creates a backup.
    backup-delete       Remove a backup.
    backup-list         List all the backups.
    backup-restore      Restore a backup.
    backup-show         Show details about a backup.
    create              Add a new volume.
    credentials         Show user credentials returned from auth
    delete              Remove a volume.
    ...

Optional arguments:
  --version             show program's version number and exit
  --debug               Print debugging output
  --os-username &lt;auth-user-name&gt;
                        Defaults to env[OS_USERNAME].
  ...
</code></pre>

<p>Each <code>help</code> command optionally takes an argument:</p>
<pre><code>% cinder help create
usage: cinder create [--snapshot-id &lt;snapshot-id&gt;]
                     [--source-volid &lt;source-volid&gt;] [--image-id &lt;image-id&gt;]
                     [--display-name &lt;display-name&gt;]
                     [--display-description &lt;display-description&gt;]
                     [--volume-type &lt;volume-type&gt;]
                     [--availability-zone &lt;availability-zone&gt;]
                     [--metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]]
                     &lt;size&gt;

Add a new volume.

Positional arguments:
  &lt;size&gt;                Size of volume in GB

Optional arguments:
  --snapshot-id &lt;snapshot-id&gt;
                        Create volume from snapshot id (Optional,
                        Default=None)
  --source-volid &lt;source-volid&gt;
                        Create volume from volume id (Optional, Default=None)
  --image-id &lt;image-id&gt;
                        Create volume from image id (Optional, Default=None)
  --display-name &lt;display-name&gt;
                        Volume name (Optional, Default=None)
  --display-description &lt;display-description&gt;
                        Volume description (Optional, Default=None)
  --volume-type &lt;volume-type&gt;
                        Volume type (Optional, Default=None)
  --availability-zone &lt;availability-zone&gt;
                        Availability zone for volume (Optional, Default=None)
  --metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]
                        Metadata key=value pairs (Optional, Default=None)
</code></pre>

<h2 id="troubleshooting">Troubleshooting</h2>
<p>If you installed the clients using <code>pip</code>, the best thing to do when you feel like your clients are 'broken' is to destroy your virtual environment and reinstall.</p>
<p>If this doesn't solve your problem, you're unfortunately at the point that you need to use your search of engine of choice to find help, start debugging Python code or file a bug on Launchpad.</p>]]></content:encoded>
    </item>
  </channel>
</rss>
