<?xml version="1.0" encoding="UTF-8"?>
<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
   >
  <title type="text">HackStack Posts</title>
  <subtitle type="text">OpenStack and other hackish things</subtitle>

  <updated>2014-10-25T01:45:41Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog" />
  <id>http://hackstack.org/x/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://hackstack.org/x/blog/feed/atom/" />
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[How Dost Thy Cloud Know Me, Let Me Count The Ways]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/10/24/how-dost-thy-cloud-know-me-let-me-count-the-ways" />
    <id>http://hackstack.org/x/blog/2014/10/24/how-dost-thy-cloud-know-me-let-me-count-the-ways</id>
    <updated>2014-10-24T10:24:00Z</updated>
    <published>2014-10-24T10:24:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstackclient" />
    <summary type="html"><![CDATA[How Dost Thy Cloud Know Me, Let Me Count The Ways]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/10/24/how-dost-thy-cloud-know-me-let-me-count-the-ways"><![CDATA[<div class="document">
<p>One of the coolest (IMHO) new features <a class="footnote-reference" href="#id2" id="id1">[1]</a> recently added to OpenStackClient is its leveraging of a new-ish feature of Keystone's client library, authentication plugins.  As that name implies, this allows for Keystone client to be able to use an extendable set of authentication backends for validating users.  At press time (keypress time for the pedantic) the freshly released <a class="reference external" href="https://pypi.python.org/pypi/python-keystoneclient">python-keystoneclient</a> 0.11.2 includes the traditional password method, a new variant on the token method and a recent addition supporting SAML2.</p>
<p>Happily, the <a class="reference external" href="http://git.openstack.org/cgit/openstack/python-openstackclient">master branch</a> of OpenStackClient has learned how to take advantage of these plugins, plus any additional ones written for Keystone client.  This creates a new problem because the authentication type can not always be inferred and needs to be supplied by the user.  And thus we arrive at the <em>Topic of the Day</em>.</p>
<div class="section" id="but-first-preview-time">
<h1>But First, Preview Time</h1>
<p>There is one additional yet-to-come feature that I can't resist mentioning now that it has been <a class="reference external" href="https://review.openstack.org/129795">proposed for review</a>. It leverages mordred's <a class="reference external" href="http://git.openstack.org/cgit/stackforge/os-client-config">os-client-config</a> module to read configuration information from a file by name.  In plain language, rather than set up a handful of environment variables or command-line options, all of the authentication and other configuration for OSC can be stashed in a YAML file and called by name:</p>
<pre class="literal-block">
openstack --os-cloud devstack-1 image list --long
</pre>
<p>This is also step one in simplifying dealing with multiple clouds:</p>
<pre class="literal-block">
for cloud in devstack-1 hpcloud-az2 rax-ord; do
    openstack --os-cloud image list --long
</pre>
<p>It is a small thing, but small things often make us happy.  It figures in to the following authentication discussion that I don't want to update again in a month.  So until <tt class="docutils literal"><span class="pre">os-cloud</span></tt> support merges, ignore references to YAML, <tt class="docutils literal">CloudConfig</tt>, etc. below.</p>
<!-- mordred -->
<!-- os-cloud https://review.openstack.org/129795 -->
</div>
<div class="section" id="sources-of-truth">
<h1>Sources of Truth</h1>
<p>OpenStackClient has three sources of configuration information (in decreasing priority order):</p>
<ul class="simple">
<li>command line options</li>
<li>environment</li>
<li><tt class="docutils literal">CloudConfig</tt> (<tt class="docutils literal"><span class="pre">~/.config/openstack/clouds.yaml</span></tt> file)</li>
</ul>
<p>Once all of the sources have been processed and a single configuration object assembled, the fun can begin.  If an authentication type is not provided, the authentication options are examined to determine if one of the default types can be used. If no match is found an error is reported and a period of stillness is declared.  Rather, the program exits.</p>
<p>Note that the authentication call to the Identity service has not yet
occurred.  It is deferred until the last possible moment in order to
reduce the number of unnecessary queries to the server, such as when further
processing detects an invalid command.</p>
</div>
<div class="section" id="keystone-authentication-plugins">
<h1>Keystone Authentication Plugins</h1>
<p>The Keystone client library implements the base set of plugins.  Additional
plugins may be available from the Keystone project or other sources.
See the <a class="reference external" href="http://docs.openstack.org/developer/python-keystoneclient/authentication-plugins.html">Keystone client documentation</a> for more information.</p>
<p>There are at least three authentication types that are always available:</p>
<ul>
<li><p class="first"><strong>Password</strong>: A username and password, plus optional project and/or domain,
are used to identify the user.  This is the most common type and the
default any time a username is supplied.  An authentication URL for the
Identity service is also required.  [Required: <tt class="docutils literal"><span class="pre">--os-auth-url</span></tt>, <tt class="docutils literal"><span class="pre">--os-project-name</span></tt>, <tt class="docutils literal"><span class="pre">--os-username</span></tt>; Optional: <tt class="docutils literal"><span class="pre">--os-password</span></tt>]</p>
</li>
<li><p class="first"><strong>Token</strong>: This is slightly different from the usual token authentication
(described below as token/endpoint) in that a token and an authentication
URL are supplied and the plugin retrieves a new (scoped?) token.
[Required: <tt class="docutils literal"><span class="pre">--os-auth-url</span></tt>, <tt class="docutils literal"><span class="pre">--os-token</span></tt>]</p>
</li>
<li><p class="first"><strong>Token/Endpoint</strong>: This is the original token authentication (known as 'token
flow' in the early CLI documentation in the OpenStack wiki).  It requires
a token and a direct endpoint that is used in the API call.  The difference
from the new Token type is this token is used as-is, no call is made
to the Identity service from the client.  This type is most often used to
bootstrap a Keystone server where the token is the <tt class="docutils literal">admin_token</tt> configured
in <tt class="docutils literal">keystone.conf</tt>.  It will also work with other services and a regular
scoped token such as one obtained from a <tt class="docutils literal">token issue</tt> command.  [Required: <tt class="docutils literal"><span class="pre">--os-url</span></tt>, <tt class="docutils literal"><span class="pre">--os-token</span></tt>]</p>
<p><em>[Note that the Token/Endpoint plugin is currently supplied by OSC itself and is not available for other clients using the Keystone client lib.  It shall move to its Proper Home in Good Time.]</em></p>
</li>
<li><p class="first"><strong>Others</strong>: There are SAML and other (Kerberos?) plugins under development
that are also supported.  To use them they must be selected by supplying
the <tt class="docutils literal"><span class="pre">--os-auth-type</span></tt> options.</p>
</li>
</ul>
</div>
<div class="section" id="how-it-s-made">
<h1>How It's Made</h1>
<p><em>[Who doesn't love</em> <a class="reference external" href="http://www.sciencechannel.com/tv-shows/how-its-made">that show</a>? <em>]</em></p>
<p>The authentication process flows from OSC's <tt class="docutils literal">OpenStackShell</tt> to the New-and-Improved <tt class="docutils literal">ClientManager</tt>.</p>
<ul class="simple">
<li>But first, on import <tt class="docutils literal">api.auth</tt>:<ul>
<li>obtains the list of installed Keystone authentication
plugins from the <tt class="docutils literal">keystoneclient.auth.plugin</tt> entry point.</li>
<li>builds a list of authentication options from the plugins.</li>
</ul>
</li>
<li><tt class="docutils literal">OpenStackShell</tt> parses the command line:<ul>
<li>If <tt class="docutils literal"><span class="pre">--os-cloud</span></tt> is present read the named configuration from <tt class="docutils literal"><span class="pre">~/.config/openstack/clouds.yaml</span></tt> and create a <tt class="docutils literal">CloudConfig</tt> object<ul>
<li><tt class="docutils literal">CloudConfig</tt> also handles picking up the matching environment variables for the options</li>
</ul>
</li>
<li>The remaining global command line options are merged into the new <tt class="docutils literal">CloudConfig</tt></li>
</ul>
</li>
<li>A new <tt class="docutils literal">ClientManager</tt> is created and provided with the <tt class="docutils literal">CloudConfig</tt>:<ul>
<li>If <tt class="docutils literal"><span class="pre">--os-auth-type</span></tt> is provided and is a valid and available plugin it is used.</li>
<li>If <tt class="docutils literal"><span class="pre">--os-auth-type</span></tt> is not provided select an authentication plugin based on the existing options.  This is a short-circuit evaluation, first match wins.<ul>
<li>If <tt class="docutils literal"><span class="pre">--os-endpoint</span></tt> and <tt class="docutils literal"><span class="pre">--os-token</span></tt> are both present <tt class="docutils literal">token_endpoint</tt> is selected</li>
<li>If <tt class="docutils literal"><span class="pre">--os-username</span></tt> is present <tt class="docutils literal">password</tt> is selected</li>
<li>If <tt class="docutils literal"><span class="pre">--os-token</span></tt> is present <tt class="docutils literal">token</tt> is selected</li>
<li>If no selection has been made by now exit with error</li>
</ul>
</li>
<li>Load the selected plugin class.</li>
</ul>
</li>
<li><tt class="docutils literal">ClientManager</tt> waits until an operation that requires authentication is attempted to make the initial request to the Identity service.<ul>
<li>if <tt class="docutils literal"><span class="pre">--os-auth-url</span></tt> is not present for any of the types except
Token/Endpoint, exit with an error.</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="destinations-of-consequences">
<h1>Destinations of Consequences</h1>
<p>The changes that began with utilizing Keystone client's <tt class="docutils literal">Session</tt> are nearly complete and have added the <tt class="docutils literal">openstackclient.api.auth</tt> module and drastically restructured the <tt class="docutils literal">openstackclient.shell</tt> and <tt class="docutils literal">openstackclient.clientmanger</tt> modules.  One result is that the <tt class="docutils literal">ClientManager</tt> is now nearly self-contained with regard to its usability apart from the OSC shell.  At this time I can neither confirm nor deny that <tt class="docutils literal">ClientManager</tt> could be used as a single-point client API.  While it works (<a class="reference external" href="https://review.openstack.org/127873">one example</a>) it is not yet a stable API because it only unifies the session and auth components, passing the real work down to either the project libraries or OSC's internal API objects.  So don't go and do that.  Yet...</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Currently only in master branch, to be included in the next release.</td></tr>
</tbody>
</table>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[First Principles - Glossary]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/10/08/first-principles-glossary" />
    <id>http://hackstack.org/x/blog/2014/10/08/first-principles-glossary</id>
    <updated>2014-10-08T10:08:00Z</updated>
    <published>2014-10-08T10:08:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <summary type="html"><![CDATA[First Principles - Glossary]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/10/08/first-principles-glossary"><![CDATA[<div class="document" id="openstack-terms-and-you">
<h1 class="title">OpenStack Terms and You</h1>
<p>I think we need to start from the beginning to know a bit about where we really are...and to do that we need to speak the same language.  This doesn't mean we must agree on the actual terms used, but that their definition and evidence of need are spelled out and can inform the following steps and discussion.</p>
<p>In some cases, the bikeshed will simply be in primer with placeholders until we converge an appropriate terms.</p>
<p>The Technical Committee <a class="reference external" href="https://wiki.openstack.org/wiki/Governance/Foundation/TechnicalCommittee">Charter</a> refers to some terms that should be clarified early in this process.</p>
<ul class="simple">
<li><strong>Official Programs</strong> - The current unit of organization, called a <strong>program</strong>, logically groups people and code for purposes of achieving the OpenStack project mission.  The current programs are generally teams of people woring on a common project or set of related projects.  These teams elect a Program Team Lead (PTL) who has management responsibilities to the program.  <a class="reference external" href="https://review.openstack.org/#/c/125783/">A proposal</a> to change this terminology to <strong>project team</strong> is in part intended to reinforce the idea that these are groups of people first and code repositories second.  The proposal is part of a series to re-structure the organization of OpenStack projects and by itself is mostly for clarity.  It does not appear to be controversial based on the comments so far so I will adopt it here.</li>
</ul>
<ul class="simple">
<li><strong>Program Leads</strong> - We've all grown to love the term <strong>PTL</strong> and the change from program to team can be used to re-redefine PTL as <strong>project team lead</strong> as it once was back in the day.</li>
<li><strong>Oversight</strong> - The TC charter includes <em>&quot;[the TC] still has oversight over team decisions, especially when they affect other programs or go contrary to general OpenStack project goals&quot;</em>.  There is concern about the balance between project rights and responsibilities changing.  There may also be concerns over what that balance is today.</li>
</ul>
<p>I'm looking for the baked-in connection between governance organization and deliverables.  Today we have a 1:1 between official teams (programs) and their participation in the &quot;integrated release&quot;.  Many of the proposals floating about disconnect those.</p>
<ul>
<li><p class="first"><strong>Coordinated Release Cycle</strong> - The time period roughly starting at or just before the semi-annual Design Summit and ending approximately six months later, again just before the next Design Summit.</p>
</li>
<li><p class="first"><strong>Release Artifacts</strong> - The output of the process formerly known as <em>integrated release</em>.  These are the major output of the <em>coordinated release cycle</em> and typically what distributions would consume in building their periodic releases.</p>
</li>
<li><p class="first"><strong>Project Groups</strong> - Project groups are simply groups of related projects and/or project teams that are dependent on each other and it is often useful to consider the projects together for certain matters.  For example <strong>Group C</strong> includes Nova, Glance, Cinder and Neutron.  Other groups might include <strong>Group I</strong> (Keystone and plugins), <strong>Group O</strong> (Swift), <strong>Group W</strong> (<a class="reference external" href="http://en.wikipedia.org/wiki/Alice's_Restaurant">litterbugs</a>), <strong>Group H</strong> (Ironic and other hypervisor drivers), <strong>Group D</strong> (Trove), <strong>Group M</strong> (Zaqar), and so on.  Many groups may only have one member if there are not multiple projects with a strongly set of similar or mutual dependencies.</p>
<p>One use of project groups is in simplifying the relationship diagram between projects; the diagram between projects within the group can focus on that subset and the diagram between groups can minimize the effect of the intra-group relationships.  These diagrams should be useful in informing testing policy and requirements.</p>
<p>These groups map into my <a class="reference external" href="/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit/">layers description</a> with layer one comprised of Groups I and most of C and layer 2 comprised of Groups O, H and the remainder of C (Cinder).  Remaining groups of official OpenStack projects make up layer 3.</p>
</li>
<li><p class="first"><strong>Integrated Release</strong> - The first step is to change the term <em>integrated release</em> to something better defined and less overloaded.  This is highly contentious as you might expect.  I had intended on using <strong>nucleus</strong> to describe this chewy middle of OpenStack because it has all sorts of metaphorical nooks and crannies to be mined, but then I saw <strong>nuclear release</strong> in writing and decided to just use the rather archaic term <strong>bindle</strong> here instead, as in &quot;the OpenStack Bindle contains the basic cloud necessities&quot;.</p>
<p>The key part of the existing definition, <em>&quot;The OpenStack bindle is a subset of the official OpenStack projects whose member teams submit to stricter policies and oversight&quot;</em>, divides the official OpenStack projects into at least two tiers.  dhellmann describes it like this:</p>
<pre class="literal-block">
size(ecosystem) &gt; size(official-projects) &gt; size(bindle)
</pre>
<p>Inclusion in the integrated release has become the high-value attribute for many corporations judging project worthiness for contribution and/or use.  Removing this attribute is one of the major driving forces for re-considering the organizational structure in general and in renaming and clearly defining the <em>bindle</em>.</p>
<p>The makeup of the bindle defines what release artifacts are published as a result of the release process.  These projects are held to a higher bar in regards to testing, documentation in involvement of team members in horizontal OpenStack activities.</p>
</li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[A Funny Thing Happened On The Way To The Summit]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit" />
    <id>http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit</id>
    <updated>2014-10-03T10:03:00Z</updated>
    <published>2014-10-03T10:03:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <summary type="html"><![CDATA[A Funny Thing Happened On The Way To The Summit]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/10/03/a-funny-thing-happened-on-the-way-to-the-summit"><![CDATA[<div class="document">
<p>So back in <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">the old days</a> I started throwing around different terminology to describe some of the technical relationships between OpenStack projects because it was useful to sort out things like startup order requirements in DevStack and other semi-obvious stuff.</p>
<p>And wow have things happened since then.  To recap, oh nevermind, I'm just going to take back the term <strong>layer</strong> for technical use and propose anything else other than <strong>layer 1</strong> (there is no other layer?) for the rest of the conversation. The various alternate approaches all boil down to a <strong>nucleus</strong> with a cloud (heh) of projects with probabilistic locations.  I wasn't a physics major but I do know that doesn't sound like that Q-word that shall not be spoken.</p>
<p>I think it is important to remember that one of the primary purposes of OpenStack is to enable the creation of <strong>useful clouds</strong>.  In my dictionary what makes a useful cloud is described as &quot;the set of services that enable useful work to be done&quot;.  In a cloud.</p>
<p>The original layers idea has been picked up, painted, folded, carved and whitewashed to a shadow of its original.  Even so, in the end all of the ideas still end up looking similar.  Now seems like a good time to see how the orignal layers have held up.</p>
<div class="section" id="layer-1">
<h1>Layer 1</h1>
<p><em>We're still the one...</em></p>
<p>We open with the addition of Neutron as a viable alternative to Nova Network, and the likelihood of it becoming the default configuration in DevStack early in the Juno cycle.</p>
<blockquote>
<ul class="simple">
<li>Identity (Keystone)</li>
<li>Image (Glance)</li>
<li>Network (Neutron)</li>
<li>Compute (Nova)</li>
</ul>
</blockquote>
<p>What really stands out to me now is the realization that all of these were originally part of Nova itself (plus the cinder volume service, more on that later).  They were broken apart or re-implemented to scale the development as Nova kept growing.  In fact, there is talk again of need to break out more simply because Nova continues to expand.</p>
<p>This is the smallest working set for a compute-enabled cloud.  Realistic useful clouds of course offer more than this, so we have...</p>
</div>
<div class="section" id="layer-2">
<h1>Layer 2</h1>
<p>Layer 2 services are optional in a useful compute cloud but some are also useful in their own right as non-compute cloud services.</p>
<p>So the current Layer 2 still contains:</p>
<blockquote>
<ul class="simple">
<li>Volume (Cinder)</li>
<li>Object (Swift)</li>
<li>Bare-metal (Ironic)</li>
</ul>
</blockquote>
<p>These all build on the Layer 1 nucleus and get us a practical useful cloud.  They also all have the characteristic of having dependency arrows pointing <em>out</em> of Layer 1 when used with a compute cloud, such as Glance using Swift as its backend store.  This is a defining characteristic that brings a project in to Layer 2.</p>
<p>Even though Cinder was literally carved out of the Nova code base it stays in Layer 2 because it is an optional service to a Layer 1 cloud.  Manila will also fit here for the same reasons.</p>
<p>I neglected to mention last time the ability of Swift to stand alone as a useful cloud service as it has maintained its own authentication capability.  However, using it with any other OpenStack services requires Swift to use Keystone.</p>
<p>I also think it is worth considering the direction of the trademark usage constraints the board is refining with the DefCore work.  The current DefCore capability proposal is satisfied using only Layer 1 and 2 projects.  Also, the stand-alone services currently would not be able to qualify for trademark usage when deployed alone.</p>
</div>
<div class="section" id="layer-3">
<h1>Layer 3</h1>
<p>Do something useful.  Host services for paying customers.  Provide Lego blocks for them to build awesome cloud apps.  Warn about runaway <tt class="docutils literal">while true; done</tt> loops.  Count cycles burned and bits sent so paying customers know what to pay.  Communicate with your useful cloud.</p>
<p>The rest of the OpenStack-affiliated projects (for some value of <em>affiliated</em>) go in Layer 3 to populate the <strong>big tent</strong>.  If we've done our job right the majority of everything else should be able to be accomplished without special consideration from Layers 1 and 2.  Broad categories of Layer 3 projects include:</p>
<blockquote>
<ul class="simple">
<li>User Interfaces - You need one but a feature of well documented REST APIs is allowing the client side to be easily replaceable.<ul>
<li>Orchestration (Heat) (it is basically a smart automated cloud client, no?)</li>
<li>Web UI (Horizon)</li>
<li>&lt;insert-one-of-the-other-CLI-or-web-clients-here&gt;</li>
</ul>
</li>
<li>Something-as-a-Service - These are all services a deployer may choose to offer.<ul>
<li>Database (Trove)</li>
<li>Message Passing (Zaqar)</li>
</ul>
</li>
<li>Tracking Snooping and Counting - Keeping an eye on the useful cloud<ul>
<li>Telemetry (Ceilometer)</li>
</ul>
</li>
</ul>
</blockquote>
<p>Why is Heat in Layer 3???  Heat is essentially a smart automated cloud client and should be treated as one.   It needs to meet the same requirements for API compatibility to be useful over time.</p>
</div>
<div class="section" id="layer-4">
<h1>Layer 4</h1>
<p>Layer 4 is everything else that is not OpenStack-affiliated but might be a part of an especially useful OpenStack cloud.  Things like Ceph or Apache jclouds are useful with and as part of OpenStack clouds, but they also have a life of their own and we should respect that and not call late at night.</p>
</div>
<div class="section" id="what-about-layer-0">
<h1>What About Layer 0?</h1>
<p>Ah, right, where the Libraries live.  The last year has seen significant changes to how OpenStack-related libraries are integrated with a number of Oslo libraries being released stand-alone.  In most cases these can and should be thought of as dependencies just as any non-OpenStack project dependency (like <tt class="docutils literal">SQLAlchemy</tt> or <tt class="docutils literal">requests</tt>) that happen to live in either the <tt class="docutils literal">openstack</tt> or <tt class="docutils literal">stackforge</tt> namespaces in our Git repositories.</p>
<p>It also seems appropriate to add the client API libraries and SDKs to Layer 0 as the dependency model and release schedule is very similar to the other libraries.  I am specifically not including command-line interfaces here as I think those belong in Layer 3 but the project client libraries have an embedded CLI so the'll straddle the boundaries no matter what.</p>
</div>
<div class="section" id="so-how-do-we-govern-and-test-all-this">
<h1>So How Do We Govern and Test All This?</h1>
<p>OK, I lied.  I said I would skip this, but anyone still reading must think this is has a thread of merit, right?  I choose to make that assumption going forward, and those of you still reading for a laugh, here is your cue.</p>
<p>I'll lay out an overview of a developers perspective because I am primarily an OpenStack developer and I need the world to know what I think.  However, I am also an application developer and cloud end-user so those perspectives are not lost.  I have not managed to add cloud deployer to my CV, yet.</p>
<div class="section" id="releases">
<h2>Releases</h2>
<p>If you turn your head sideways and squint, the Layer picture can also be grouped according to release-able/deploy-able units with decently defined and documented interfaces between them.</p>
<p>Maintaining the current notion of an Integrated Release the layers fall out like this:</p>
<blockquote>
<ul class="simple">
<li>Layers 1 and 2 <em>are</em> the Integrated Release.  The services required to meet DefCore are currently a subset of these layers.</li>
<li>Layer 3 projects treat the Integrated Release as a dependency like any other they may have so they can have the freedom to iterate at a pace that suits the service being provided.  Trove probably needs fewer releases in the next year than Zaqar.</li>
</ul>
</blockquote>
<p>Switching to a more modularized set of released units the first 'natural' groupings are:</p>
<blockquote>
<ul class="simple">
<li>Layer 1 plus the semi-tightly coupled Nova projects like Cinder (and Manila) comprise a Compute Release.</li>
<li>Swift comprises an Object Store release</li>
<li>Ironic comprises an (insert-catchy-name-here) release and not in the Compute Release as it can also stand alone (right?)</li>
<li>Actually, everything else is on its own because Independence From Tyranny!  Things that need to talk to each other or to the Integrate projects need to correctly identify and handle the documented APIs available to them.</li>
</ul>
</blockquote>
<p>Basically, this alternative splits the Integrated Release into a Compute Release and two stand-alone releases for Swift and Ironic.  The Release Management team may reconsider the criteria required for them to continue to handle other project releases or allow (force?) the projects to handle their own.</p>
<p>Note how the difference in those two approaches to releases is exactly two things, pulling Swift and Ironic out of the Integrated Release bundle so they can stand alone.</p>
</div>
<div class="section" id="testing">
<h2>Testing</h2>
<p>As current work is showing, the actual detailed relationships between OpenStack services is very complex.  Describing it to a level of detail that can drive a test matrix is not simple.  We can, however, reduce the problem space by re-thinking at a higher level what needs to be tested together.</p>
<p>Layers 1 and 2 are really where the work needs to be done. By changing the perspective of Layer 3 projects we can reduce the piling-on of additional projects that are currently in our Test All The Things check/gate jobs.  Individual project relationships across that boundary may be important enough to warrant specific test jobs but those are considered exceptions and not the rule.</p>
<p>A significant amount of the gains to me made here are contingent on the projects developing comprehensive functional tests.</p>
</div>
</div>
<div class="section" id="horizontal-projects">
<h1>Horizontal Projects</h1>
<p>While it feels like I'm saving the best for last, in reality much of the above has to have some structure to know the scope that Infrastructure, Docs and QA need to be able to support.  Focusing these on Layers 1 and 2 provides a clear limit to the scope required.  This is not to say that other projects are not going to be accommodated, particularly those already in the current release, but it does say that it is not assured.</p>
</div>
<div class="section" id="now-what-smart-guy">
<h1>Now What Smart Guy?</h1>
<p>With my thoughts on Layers updated to include the governance and testing considerations it is time to match up other perspectives, flesh out the above with the new information and catch up on the plethora of other posts on this topic.</p>
<p>Film at eleven...</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStack Low Level API]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api" />
    <id>http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api</id>
    <updated>2014-09-15T09:15:00Z</updated>
    <published>2014-09-15T09:15:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="api" />
    <category scheme="http://hackstack.org/x/blog" term="client" />
    <summary type="html"><![CDATA[OpenStack Low Level API]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/09/15/openstack-low-level-api"><![CDATA[<div class="document">
<p>The current Python library situation for OpenStack is, sorry to say, a mess.  Cleaning it up requires essentially starting over and abstracting the individual REST APIs to usable levels.  With OpenStackClient I started from the top and worked down to make the CLI a better experience.  I think we have proved that to be a worthwhile task.  Now it is time to start from the bottom and work up.</p>
<p>The existing libraries utilize a Manager/Resource model that may be suitable for application work, but every project's client repo was forked and changed so they are all similar but maddeningly different.  However, a good idea or two can be easily extracted and re-used in making things as simple as possible.</p>
<p>I originally started with no objects at all and went straight to top-level functions, as seen in the current <tt class="docutils literal">object.v1.lib</tt> APIs in OSC.  That required passing around the session and URLs required to complete the REST calls, which OSC already has available, but it is not a good general-purpose API.</p>
<p>I've been through a number of iterations of this and have settles on what is described here, a low-level API for OSC and other applications that do not require an object model.</p>
<div class="section" id="api-baseapi">
<h1>api.BaseAPI</h1>
<p>We start with a <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> object that contains the common operations.  It is pretty obvious there are only a couple of ways to get a list of resources from OpenStack APIs so the bulk of that and similar actions are here.</p>
<p>It is also very convenient to carry around a couple of other objects so they do not have to be passed in every call.  <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> contains a <tt class="docutils literal">session</tt>, <tt class="docutils literal">service type</tt> and <tt class="docutils literal">endpoint</tt> for each instance.  The <tt class="docutils literal">session</tt> is a <tt class="docutils literal">requests.session.Session</tt>-compatible object.  In this implementation we are using the <tt class="docutils literal">keystoneclient.session.Session</tt> which is close enough.  We use the ksc Session to take advantage of keystoneclient's authentication plugins.</p>
<p>The <tt class="docutils literal">service type</tt> and <tt class="docutils literal">endpoint</tt> attributes are specific to each API.  <tt class="docutils literal">service type</tt> is as it is used in the Service Catalog, i.e. <tt class="docutils literal">Compute</tt>, <tt class="docutils literal">Identity</tt>, etc.  <tt class="docutils literal">endpoint</tt> is the base URL extracted from the service catalog and is prepended to the passed URL strings in the <tt class="docutils literal">API</tt> method calls.</p>
<p>Most of the methods in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> also are meant as foundational building blocks for the service APIs.  As such they have a pretty flexible list of arguments, many of them accepting a <tt class="docutils literal">session</tt> to override the base <tt class="docutils literal">session</tt>.  This layer is also where the JSON decoding takes place, these all return a Python <tt class="docutils literal">list</tt> or <tt class="docutils literal">dict</tt>.</p>
<p>The derived classes from <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> will contain all of the methods used to access their respective REST API.  Some of these will grow quite large...</p>
</div>
<div class="section" id="api-object-store-apiv1">
<h1>api.object_store.APIv1</h1>
<p>While this is a port of the existing code from OpenStackClient, <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/object_store.py#L26">object_store.APIv1</a> is still essentially a greenfield implementation of the <tt class="docutils literal"><span class="pre">Object-Store</span></tt> API.  All of the path manipulation, save for prepending the base URL, is done at this layer.</p>
</div>
<div class="section" id="api-compute-apiv2">
<h1>api.compute.APIv2</h1>
<p>This is one of the big ones.  At this point, only <tt class="docutils literal">flavor_list()</tt>, <tt class="docutils literal">flavor_show()</tt> and <tt class="docutils literal">key_list()</tt> have been implemented in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/compute.py#L19">compute.APIv2</a>.</p>
<p>Unlike the <tt class="docutils literal"><span class="pre">object-store</span></tt> API, the rest of the OpenStack services return resources wrapped up in a top-level dict keyed with the base name of the resource.  This layer shall remove that wrapper so the returned values are all directly lists or dicts.  This removed the variations in server implementations where some wrap the list object individually and some wrap the entire list once.  Also, Keystone's tendency to insert an additional <tt class="docutils literal">values</tt> key into the return.</p>
</div>
<div class="section" id="api-identity-vx-apivx">
<h1>api.identity_vX.APIvX</h1>
<p>The naming of <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/identity_v2.py#L19">identity_v2.APIv2</a> and <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/identity_v3.py#L19">identity_v3.APIv3</a> is a bit repetitive but putting the version into the module name lets us break down the already-long files.</p>
<p>At this point, only <tt class="docutils literal">project_list()</tt> is implemented in an effort to work out the mechanics of supporting multiple API versions.  In OSC, this is already handled in the ClientManager and individual client classes so there is not much to see here.  It may be different otherwise.</p>
</div>
<div class="section" id="osc-usage">
<h1>OSC Usage</h1>
<p>To demonstrate how this API is used, I've added an <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/blob/low-level-api/openstackclient/api/api.py#L22">BaseAPI</a> instance to the existing client objects that get stored in the <tt class="docutils literal">ClientManager</tt>.  For example, the addition for <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-9d500da5511aec08e46397bc7a4b25bdR75">compute.client</a> is one object instantiation and an import.  Now in OSC, <tt class="docutils literal">clientmanager.compute.api</tt> has all of the (implemented) <tt class="docutils literal">Compute</tt> API methods.</p>
<p>Using it in the flavor commands is a <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-1be98e03ae4586b73d8ad5f62f0dc578L163">simple change</a> to call <tt class="docutils literal">compute.api</tt> methods rather than the <tt class="docutils literal">compute.flavor.XXX</tt> methods.</p>
<p>Setting up for multiple API versions took a bit more work, as shown in <a class="reference external" href="https://github.com/dtroyer/python-openstackclient/commit/2bfc9e1b722cb89670ba4878f10fb07d9c68519f#diff-f7023c81f38d2c70e77da533164db4b6L31">identity.client</a>.  A parallel construction to the client class lookup is required, and would totally replace the existing version lookup once the old client is no longer required.</p>
</div>
<div class="section" id="fluff">
<h1>Fluff</h1>
<p>One other cool feature is utilizing <tt class="docutils literal">requests_mock</tt> for testing from the start.  It works great and has not the problems that rode along with <tt class="docutils literal">httpretty</tt>.</p>
</div>
<div class="section" id="now-what">
<h1>Now What?</h1>
<p>Many object models could be built on top of this API design.  The <tt class="docutils literal">API</tt> object hierarchy harkens back to the original client lib <tt class="docutils literal">Manager</tt> classes, except that they encompass an entire REST API and not one for each resource type.</p>
</div>
<div class="section" id="but-you-said-sanity-earlier">
<h1>But You Said 'Sanity' Earlier!</h1>
<p>Sanity in terms of coalescing the distinct APIs into something a bit more common?  Yes.  However, this isn't going to fix everything, just some of the little things that application developers really shouldn't have to worry about.  I want the project REST API docs to be usable, with maybe a couple of notes for the differences.</p>
<p>For example, OSC and this implementation both use the word <tt class="docutils literal">project</tt> in place of <tt class="docutils literal">tenant</tt>.  Everywhere.  Even where the underlying API uses <tt class="docutils literal">tenant</tt>.  This is an easy change for a developer to remember.  I think.</p>
<p>Also, smoothing out the returned data structures to not include the resource wrappers is an easy one.</p>
</div>
<div class="section" id="duplicating-work">
<h1>Duplicating Work?</h1>
<p>&quot;Doesn't this duplicate what is already being done in the OpenStack Python SDK?&quot;</p>
<p>Really, no.  This is meant to be the low-level SDK API that the Resource model can utilize to provide the back-end to its object model.  Honestly, most applications are going to want to use the Resource model, or an even higher API that makes easy things really easy, and hard things not-so-hard, as long as you buy in to the assumptions baked in to the implementation.</p>
<p>Sort of like OS/X or iOS.  Simple to use, as long as you don't want to anything different.  Maybe we should call that top-most API <tt class="docutils literal">iOSAPI</tt>?</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenWRT Images for OpenStack]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack" />
    <id>http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack</id>
    <updated>2014-08-17T08:17:00Z</updated>
    <published>2014-08-17T08:17:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="openwrt" />
    <summary type="html"><![CDATA[OpenWRT Images for OpenStack]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/08/17/openwrt-images-for-openstack"><![CDATA[<div class="document">
<p>I've been playing with <a class="reference external" href="http://openwrt.org">OpenWRT</a> since &lt;mumble&gt;-&lt;mumble&gt; and have enjoyed building some of the smallest Linux images around.  While targeted at low-end home router platforms, it also runs on a wide variety of small SoC boards including the near-ubiqutious Raspberry Pi and my fave BeagleBone Black.</p>
<p>I've also been using an incredibly tiny OpenWRT instance on my laptop for years now to work around the 'interesting' network configuration of VirtualBox.  Building a set of VMs that need to talk to each other and to the outside world shouldn't be hard, so I added a router just like I have at home in a 48Mb VM.</p>
<p>While OpenStack typically doesn't have that need (but you never know how Neutron might be configured!) there are plenty of other purposes for such a small single-purpose VM.  So let's build one!</p>
<p>The magic in this cloud build is having an analogue to smoser's <a class="reference external" href="https://launchpad.net/cloud-init">cloud-init</a>.  The original is written in Python and has a lot of very useful features, but requiring the Python stdlib and <tt class="docutils literal"><span class="pre">cloud-init</span></tt> dependencies to be installed expands the size of the root image considerably.   My version, called <a class="reference external" href="https://github.com/dtroyer/openwrt-packages/tree/master/rc.cloud">rc.cloud</a>, is a set of shell scripts that implement a small subset of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> capabilities.  [Note: I 'borrowed' the original scripts from somewhere over three years ago and for the life of me can't find out where now.  Pointers welcome.]</p>
<p>One of the most important features of <tt class="docutils literal"><span class="pre">cloud-init</span></tt> and <tt class="docutils literal">rc.cloud</tt> is configuring a network interface and enabling remote access.  OpenWRT defaults to no root password so I have to telnet to 192.168.1.1 to set root's password before Dropbear (a tiny ssh2 implementation) allows logins. Doing it with <tt class="docutils literal"><span class="pre">cloud-init</span></tt> or <tt class="docutils literal">rc.cloud</tt> instead allows automation and is a Wonderful Thing(TM).</p>
<p>This isn't a detailed How-To on building OpenWRT, there are a lot of <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">good docs</a> covering that topic.  It _is_ however, the steps I use plus some additional tweaks useful in a KVM-based OpenStack cloud.</p>
<div class="section" id="build-image-from-source">
<h1>Build Image From Source</h1>
<p>The basic build is straight out of the <a class="reference external" href="http://wiki.openwrt.org/doc/howto/build">OpenWRT wiki</a>.  I could have used the Image Builder, but I have some additional packages to include and like having control over the build configuration, such as either making sure IPv6 is present, or making sure it isn't.  And so on.</p>
<p>Configuring the OpenWRT buildroot can be a daunting task so starting with a minimal configuration is very helpful.  For a guest VM image there are a few things to consider:</p>
<ul class="simple">
<li>the VM target (Xen, KVM, etc)</li>
<li>root device name (vda2 for KVM, sda2 for others like VirtualBox)</li>
</ul>
<p>Traditionally OpenWRT has used Subversion for source control.  A move (or mirror?) on GitHub makes things easier for those of us who it it regualrly in other projects.  The <a class="reference external" href="http://wiki.openwrt.org/doc/howto/buildroot.exigence">buildroot doc</a> uses GitHub as the source so I've followed that convention.</p>
<ul>
<li><p class="first">Clone the repo:</p>
<pre class="literal-block">
git clone git://git.openwrt.org/openwrt.git
cd openwrt
</pre>
</li>
<li><p class="first">Install custom feed:</p>
<pre class="literal-block">
echo &quot;src-git dtroyer https://github.com/dtroyer/openwrt-packages&quot; &gt;&gt;feeds.conf.default
</pre>
</li>
<li><p class="first">Install packages:</p>
<pre class="literal-block">
./scripts/feeds update -a
./scripts/feeds install -a
</pre>
</li>
<li><p class="first">Check for missing packages:</p>
<pre class="literal-block">
make defconfig
</pre>
</li>
</ul>
<div class="section" id="configuration">
<h2>Configuration</h2>
<ul>
<li><p class="first">Configure:</p>
<pre class="literal-block">
make menuconfig
</pre>
</li>
<li><p class="first">Enable the following:</p>
<ul class="simple">
<li>Target System: x86</li>
<li>Subtarget: KVM guest</li>
<li>Target Images<ul>
<li><tt class="docutils literal">[*] ext4</tt></li>
<li><tt class="docutils literal">(48)</tt> Root filesystem partition size (in MB)</li>
<li><tt class="docutils literal">(/dev/vda2)</tt> Root partition on target device</li>
</ul>
</li>
<li>Base System<ul>
<li><tt class="docutils literal">{*} <span class="pre">block-mount</span></tt>  (not sure, if yes to support root fs, parted too)</li>
<li><tt class="docutils literal">&lt;*&gt; rc.cloud</tt></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Notes:</p>
<ul class="simple">
<li>Increase the root filesystem size if you do not intend to move root to another partition or increase the existing one to fit the flavor's disk size.</li>
</ul>
</div>
<div class="section" id="build">
<h2>Build</h2>
<p>It's pretty simple:</p>
<pre class="literal-block">
make -j 4
</pre>
<p>Adjust the argument to <tt class="docutils literal"><span class="pre">-j</span></tt> as appropriate for the number of CPUs on your build system.</p>
<p>When build errors occur, you'll need to run with output turned on:</p>
<pre class="literal-block">
make V=99
</pre>
</div>
</div>
<div class="section" id="configuring-the-image-for-openstack">
<h1>Configuring the Image for OpenStack</h1>
<p>As I mentioned earlier, there are a handful of changes to make to the resulting image that makes it ready for an OpenStack cloud.</p>
<ul class="simple">
<li>Set a root password - Without a root password your newly minted VM is vulnerable to a password-less telnet login if your security group rules allow that.  But more importantly, Dropbear will not allow an ssh login without a rot password.  Edit <tt class="docutils literal">/etc/shadow</tt> to set a root password</li>
<li>Configure a network interface for DHCP - This allows the first interface to obtain its IP automatically for OpenStack clouds that provide it.  Otherwise...</li>
<li>Configure <tt class="docutils literal">/etc/opkg.conf</tt> to my package repo - Packages usually need to be matched for not only their architecture but also other build flags.  Kernel modules are particularly rigid about how they can be loaded.</li>
</ul>
<div class="section" id="image-update">
<h2>Image Update</h2>
<p>All of the interesting parts below must be done as root.  So be careful.</p>
<ul>
<li><p class="first">Uncompress and copy the original image to a workspace, mount it and chroot into it:</p>
<pre class="literal-block">
gzip -dc bin/x86/openwrt-x86-kvm_guest-combined-ext4.img.gz &gt;openwrt-x86-kvm_guest-combined-ext4.img
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
mkdir -p imgroot
sudo mount -o loop /dev/mapper/loop0p2 imgroot
sudo chroot imgroot
</pre>
</li>
<li><p class="first">Make the desired changes:</p>
<ul>
<li><p class="first">Set root password:</p>
<pre class="literal-block">
sed -e '/^root/ s|^root.*$|root:\!:16270:0:99999:7:::|' -i /etc/shadow
</pre>
</li>
<li><p class="first">Configure DHCP:</p>
<pre class="literal-block">
uci set network.lan.proto=dhcp; uci commit
</pre>
</li>
<li><p class="first">Configure opkg:</p>
<pre class="literal-block">
sed -e &quot;s|http.*/x86/|http://bogus.hackstack.org/openwrt/x86/|&quot; -i /etc/opkg.conf
</pre>
</li>
</ul>
</li>
<li><p class="first">Unwind the mounted image:</p>
<pre class="literal-block">
sudo umount imgroot
sudo kpartx -av openwrt-x86-kvm_guest-combined-ext4.img
</pre>
</li>
<li><p class="first">Upload it into Glance:</p>
<pre class="literal-block">
openstack image create --file openwrt-x86-kvm_guest-combined-ext4.img --property os-distro=OpenWRT OpenWRT

# Glance CLI
glance image-create --file openwrt-x86-kvm_guest-combined-ext4.img --name OpenWRT
</pre>
</li>
</ul>
</div>
</div>
<div class="section" id="additional-modifications">
<h1>Additional Modifications</h1>
<div class="section" id="extending-root-filesystem">
<h2>Extending Root Filesystem</h2>
<p>Even the smallest flavor gets a root disk a good bit larger than the typocal OpenWRT disk image.  One way to use that space is to increase the root filesystem.  OpenWRT has something called <tt class="docutils literal">extroot</tt> that is currently experimental and semi-undocumented, so I just took the radical move of partitioning the unused space and moving the root filesystem to the new partition.</p>
<p>Of course a real root expansion should be automated and added to <tt class="docutils literal">rc.cloud</tt> to mirror the <tt class="docutils literal"><span class="pre">cloud-init</span></tt> functionality.  Someday...</p>
<ul>
<li><p class="first">Install required packages if they're not part of the base build:</p>
<pre class="literal-block">
opkg update
opkg install block-mount parted
</pre>
</li>
<li><p class="first">Create a filesystem on the remaining disk and mount it:</p>
<pre class="literal-block">
parted /dev/vda -s -- mkpart primary  $(parted /dev/vda -m print | tail -1 | cut -d':' -f3) -0
mkfs.ext4 -L newroot /dev/vda3
mkdir -p /tmp/newroot
mount /dev/vda3 /tmp/newroot
</pre>
</li>
<li><p class="first">Copy the root filesystem:</p>
<pre class="literal-block">
mkdir -p /tmp/oldroot
mount --bind / /tmp/oldroot
tar -C /tmp/oldroot -cvf - . | tar -C /tmp/newroot -xf -
umount /tmp/oldroot
umount /tmp/newroot
</pre>
</li>
<li><p class="first">Update the GRUB bootloader to use the new partition:</p>
<pre class="literal-block">
mkdir -p /tmp/boot
mount /dev/vda1 /tmp/boot
sed -e 's/vda2/vda3/' -i /tmp/boot/boot/grub/grub.cfg
umount /tmp/boot
</pre>
</li>
<li><p class="first">Reboot</p>
</li>
</ul>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[More Notes on Windows Images]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images" />
    <id>http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images</id>
    <updated>2014-07-13T07:13:00Z</updated>
    <published>2014-07-13T07:13:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="windows" />
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <summary type="html"><![CDATA[More Notes on Windows Images]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/07/13/more-notes-on-windows-images"><![CDATA[<div class="document">
<p>This is a follow-up to <a class="reference external" href="/x/blog/2014/07/07/windows-images-for-openstack/">Windows Images for OpenStack</a> that includes some of the notes accumulated along the way.</p>
<div class="section" id="other-docs">
<h1>Other Docs</h1>
<p>Building Windows VM images is a topic that has been done to death, but the working consensus of those I've talked to is that <a class="reference external" href="http://www.florentflament.com/blog/windows-images-for-openstack.html">Florent Flament's post</a> is one of the best guides through this minefield.</p>
</div>
<div class="section" id="metadata-server-curl-commands">
<h1>Metadata Server Curl Commands</h1>
<p>Instance UUID:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;uuid&quot;]'</blockquote>
<p>Instance Name:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/openstack/latest/meta_data.json">http://169.254.169.254/openstack/latest/meta_data.json</a> | python -c 'import sys, json; print json.load(sys.stdin)[&quot;name&quot;]'</blockquote>
<p>Fixed IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/local-ipv4">http://169.254.169.254/latest/meta-data/local-ipv4</a></blockquote>
<p>Floating IP:</p>
<blockquote>
curl <a class="reference external" href="http://169.254.169.254/latest/meta-data/public-ipv4">http://169.254.169.254/latest/meta-data/public-ipv4</a></blockquote>
</div>
<div class="section" id="building-on-an-openstack-cloud">
<h1>Building on an OpenStack Cloud</h1>
<p>One of the changes to the base instructions is to perform the build in an OpenStack cloud.  The compute node must have nested virtualization enabled so KVM will run, otherwise Qemu would be used and we just don't have time for that.</p>
<p>I'm going to use <a class="reference external" href="https://github.com/cloudenvy/cloudenvy">Cloudenvy</a> to manage the build VM.  It is similar to Vagrant in automating the grunt work of provisioning the VM.  The VM needs to have at least 4Gb RAM and 40Gb disk available in order to boot the seed Windows image.  This is an <tt class="docutils literal">n1.medium</tt> flavor on the private cloud I am using.</p>
<p>I am also using Ubuntu 14.04 because much of my tooling already assumes an Ubuntu build environment.  There is no technical reason that Fedora 20 could not be used, appropriate adjustments would need to be made, of course.</p>
<div class="section" id="build-vm">
<h2>Build VM</h2>
<p>I am not going to spend much time here explaining Cloudenvy's configuration, but there are two things required to not have a bad time with it.</p>
<p>Configure your cloud credentials in <tt class="docutils literal"><span class="pre">~/.cloudenvy</span></tt>:</p>
<pre class="literal-block">
cloudenvy:
    keypair_name: dev-key
    keypair_location: ~/.ssh/id_rsa-dev-key.pub
    clouds:
        cloud9:
            os_auth_url: https://cloud9.slackersatwork.com:2884/v2.0/
            os_tenant_name: demo
            os_username: demo
            os_password: secrete
</pre>
<pre class="literal-block">
project_config:
    name: imagebuilder
    image: Ubuntu 14.04
    remote_user: ubuntu
    flavor_name: n1.medium

sec_groups: [
    'tcp, 22, 22, 0.0.0.0/0',
    'tcp, 5900, 5919, 0.0.0.0/0',
    'icmp, -1, -1, 0.0.0.0/0'
]

files:
    Makefile: '~'
    ~/.cloud9.conf: '~'

provision_scripts:
    - install-prereqs.sh
</pre>
<p>The <tt class="docutils literal"><span class="pre">~/.cloud9.conf</span></tt> file is a simple script fragment that sets the <tt class="docutils literal">OS_*</tt> environment variable credentials required to authenticate using the OpenStack CLI tools.  It looks something like:</p>
<pre class="literal-block">
export OS_AUTH_URL=https://cloud9.slackersatwork.com:2884/v2.0/
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=secrete
</pre>
<p>Why do we need two sets of credentials?  Because we haven't taught Cloudenvy to read the usual environment variables yet.  I smell a pull request in my future...</p>
<p>Fire it up and log in:</p>
<pre class="literal-block">
envy up
envy ssh
</pre>
<p>At this point we can switch over to Flament's process.</p>
<p>Or we can use the cloudbase auto-answer template</p>
<p>Get the ISO:</p>
<pre class="literal-block">
&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
for i in aa ab ac ad ae af ag ah; do \
    swift download windows7 en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i; \
    cat en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso-$i &gt;&gt;en_windows_7_professional_with_sp1_x64_dvd_u_676939.iso
done
</pre>
<p>sudo ./make-floppy.sh</p>
<hr class="docutils" />
<pre class="literal-block">
# add keypair if not already there
os keypair create --public-key ~/.ssh/id_rsa.pub $(hostname -s)

# Create VM
os server create \
  --image &quot;Ubuntu 14.04&quot; \
  --flavor n1.tiny \
  --key-name bunsen \
  --user-data cconfig.txt \
  --wait \
  dt-1

export IP=$(os server show dt-1 -f value -c addresses | cut -d '=' -f2)

# Go to there
ssh ubuntu&#64;$IP
</pre>
<hr class="docutils" />
<p>Now on to Florent's steps</p>
<ul>
<li><p class="first">Create a virtual disk</p>
<blockquote>
<p>qemu-img create -f qcow2 Windows-Server-2008-R2.qcow2 9G</p>
</blockquote>
</li>
<li><p class="first">Boot the install VM</p>
</li>
</ul>
<pre class="literal-block">
kvm \
    -m 2048 \
    -cdrom &lt;WINDOWS_INSTALLER_ISO&gt; \
    -drive file=Windows-Server-2008-R2.qcow2,if=virtio \
    -drive file=&lt;VIRTIO_DRIVERS_ISO&gt;,index=3,media=cdrom \
    -net nic,model=virtio \
    -net user \
    -nographic \
    -vnc :9 \
    -k fr \
    -usbdevice tablet
</pre>
<p>Connect via VNC to :9</p>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[Windows Images for OpenStack]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack" />
    <id>http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack</id>
    <updated>2014-07-07T07:07:00Z</updated>
    <published>2014-07-07T07:07:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="windows" />
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="virtualbox" />
    <summary type="html"><![CDATA[Windows Images for OpenStack]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/07/07/windows-images-for-openstack"><![CDATA[<div class="document">
<p>There is no shortage of articles online about building Windows images for use in various clouds.  What there is a shortage of are working articles on building these images unattended.  The Windows unattended install process has been basically solved, even if still a bit arcane.  But finding more than a trivial example of doing it in a cloud is sparse.</p>
<p>Cloudbase has <a class="reference external" href="https://github.com/cloudbase/windows-openstack-imaging-tools">shared the tooling</a> they created for building their Windows images.  That makes a good base for an automated build process that can be tailored to your particular needs.  in addition to being the authors of cloudbase-init, their GitHub account is a trove of resources for Windows admins.</p>
<p>Since I had a Windows 7 Professional ISO handy I used that for the example...</p>
<div class="section" id="requirements">
<h1>Requirements</h1>
<p>The resulting image must:</p>
<ul class="simple">
<li>have RedHat's VirtIO drivers installed</li>
<li>use <tt class="docutils literal"><span class="pre">cloudbase-init</span></tt> for metadata handling</li>
</ul>
</div>
<div class="section" id="build-in-virtualbox">
<h1>Build In VirtualBox</h1>
<p>The Cloudbase process is designed to perform the build using KVM.  Ideally, it would be possible to boot a VM in an OpenStack cloud from the install ISO and let it go, but it turns out this is hard.  The unattended install process  requires two ISO images and a floppy image attached to the VM in addition to the target disk device. OpenStack currently has no way to do all of these attachments.  The alternative is to stash autounattend.xml and the virtio drivers in the Windows install ISO, but this requires a rebuild/upload for _every_ change to the install scripts.</p>
<p>So normally this means a Linux on bare-metal install is required.  How hard is it to dig up a laptop with Trusty on it?  Hard, if you're me.</p>
<p>I've heard that using VirtualBox doesn't work for some reason, but these reasons haven't been made clear to me so I didn't know I couldn't do what I'm describing here.</p>
<div class="section" id="auto-answer-changes">
<h2>Auto Answer Changes</h2>
<p>One of the main change to Cloudbase's setup is to put the PowerShell scripts on the floppy with Autounattend.xml.  This ensures that the files are matched together and changes in the repo doesn't break our working setup.</p>
<p>The Autounattend.xml file has a couple of changes other than those required to run the script from the floppy:</p>
<ul class="simple">
<li>Add the MetaData value for Win7</li>
<li>Since this is Win7, we need to enable the account stanza</li>
<li>Install a public product key</li>
<li>Fix a spacing error in the Microsoft-Windows-International-Core-WinPE component element</li>
</ul>
</div>
<div class="section" id="powershell-script-changes">
<h2>PowerShell Script Changes</h2>
<p>The primary change to the PowerShell scripts is to remove the file downloads and retrieve them from the floppy instead.</p>
</div>
<div class="section" id="make-the-floppy-image">
<h2>Make the Floppy Image</h2>
<p>I used <a class="reference external" href="/x/files/make-floppy.sh">this script</a> to create the floppy image, it builds a new image, mounts it, copies the appropriate Autounattend.xml and PowerShell scripts and other files, then umnounts the image.</p>
</div>
<div class="section" id="build-vm-configuration">
<h2>Build VM Configuration</h2>
<p>Automating a VBox build includes creating the VM to be used.  The <tt class="docutils literal">VBoxManage</tt> tool is the simple way to do this from a script and that's exactly what I've done here.</p>
<p>It turns out that 16Gb is not enough for Windows 7 installation once all of the updates are installed.  There are a LOT of them, 157 at this writing.  Even though this only needs to be done once, it takes a long time to apply them and it might be worthwhile to obtain media with the updates pre-applied.</p>
<p>The commands here are taken from the <tt class="docutils literal"><span class="pre">build-vb.sh</span></tt> script.</p>
<p>Create a new empty VM and disk:</p>
<pre class="literal-block">
BASE_NAME='win-build'
# Create a new empty VM
VBoxManage createvm --name &quot;$BASE_NAME&quot; --ostype &quot;$OS_TYPE&quot; --register
VBoxManage createhd --filename &quot;$VM_DIR/$BASE_NAME.vdi&quot; --size $DISK_SIZE
</pre>
<p>The disk configuration is an important part of this process so everything is found as required.  In addition to the install disk and install ISO a second ISO must be mounted containing the VirtIO drivers and a floppy image with the Autounattend.xml and Powershell scripts:</p>
<pre class="literal-block">
# SATA Controller
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;SATA&quot; --add sata
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;SATA&quot; --type hdd \
    --port 0 --device 0 --medium &quot;$VM_DIR/$BASE_NAME.vdi&quot;

# Make IDE disks
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;IDE&quot; --add ide
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 0  --device 0 --medium &quot;$WIN_ISO&quot;
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;IDE&quot;  --type dvddrive \
    --port 1  --device 0 --medium &quot;$VIRTIO_ISO&quot;

# Floppy disk image
VBoxManage storagectl &quot;$BASE_NAME&quot; --name &quot;Floppy&quot; --add floppy
VBoxManage storageattach &quot;$BASE_NAME&quot; --storagectl &quot;Floppy&quot; --type fdd \
    --port 0 --device 0 --medium &quot;$FLOPPY&quot;
</pre>
<p>Do the remaining basic configuration, including a virtio NIC to tickle Windows to install the drivers:</p>
<pre class="literal-block">
# General Config
VBoxManage modifyvm &quot;$BASE_NAME&quot; --cpus 2
VBoxManage modifyvm &quot;$BASE_NAME&quot; --memory $RAM_SIZE --vram 24
VBoxManage modifyvm &quot;$BASE_NAME&quot; --ioapic on

VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic1 nat --bridgeadapter1 e1000g0
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nic2 nat
VBoxManage modifyvm &quot;$BASE_NAME&quot; --nictype2 virtio

VBoxManage modifyvm &quot;$BASE_NAME&quot; --boot1 dvd --boot2 disk --boot3 none --boot4 none
</pre>
<p>Kick off the build process:</p>
<pre class="literal-block">
VBoxManage startvm &quot;$BASE_NAME&quot; --type gui
</pre>
<p>Convert the disk from VDI to QCOW2 format for uploading into the image store:</p>
<pre class="literal-block">
qemu-img convert -p -O qcow &quot;$VM_DIR/$BASE_NAME.vdi&quot; &quot;$VM_DIR/$BASE_NAME.qcow&quot;
</pre>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[Client Layers]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/02/11/client-layers" />
    <id>http://hackstack.org/x/blog/2014/02/11/client-layers</id>
    <updated>2014-02-11T02:11:00Z</updated>
    <published>2014-02-11T02:11:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstackclient sdk" />
    <summary type="html"><![CDATA[Client Layers]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/02/11/client-layers"><![CDATA[<div class="document">
<p>[Work In Progress, you have been warned]</p>
<p>So clients are like pie.  Creamy, gooey, butterscotch cream pie with meringue.  Known at the in-laws house as Baby Bear Pie for reasons unknown-to-me.  Meringue is yummy but not much by itself.  Pie crust does its job most of the time without being noticed, unless it is sub-par.  It's the cream filling that gets all of the attention.  Butterscotch, lemon or chocolate, that's where the glory is.</p>
<p>REST clients are like pie, what with their multiple layers of communication handlers, data marshallers and output formatters and all.  So lets talk client crust.  It is the least sexy of the layers, going about its job, semi-appreciated when it is right, scorned when it is bad and otherwise taken for granted.  In OpenStack we have a large number of client projects that all talk to REST APIs.  Without going too far into the history, most of these are forks of forks and all have been independently enhanced and updated and none of them have more than a familial resemblance to each other.</p>
<p>Alessio Ababilov tried to fix this, actually making a working common REST layer for the (at the time) 5 or so client libraries.  This was proposed to oslo-incubator and has gained some traction of late.  Some things that did get merged early on were the change to use the Requests package to replace httplib2, but that did nothing to unify the low-level internal API.</p>
<p>Rather than try to fix the legacy clients the right solution here is to define some requirements and build a solid common foundation that API libraries can build on.  Rather than call it crust, let's use the even-less-sexy 'transport layer' name, totally mis-appropriated from the OSI stack.</p>
<p>Independent of the Oslo apiclient work, Jamie Lennox rebuilt the transport layer of keystoneclient as part of a refactor of the authentication bits into pluggable classes.  This happens to be excitingly close to what I had been prototyping in OpenStackClient and was possibly the biggest highlight of the week in Hong Kong.</p>
<p>So lets see if we can't turn that into a generic SDK-style transport layer for our clients.  On top of that we will layer the basic OpenStack API version discovery, authentication and service catalog.</p>
<div class="section" id="layer-1-transport-layer">
<h1>Layer 1: Transport Layer</h1>
<p>The Transport Layer includes the basic components that implement the REST API client and essentially is a wrapper around the Python Requests package with some additional header handling and logging built in.</p>
<div class="section" id="design-notes">
<h2>Design Notes</h2>
<p>The rationale for some of the differences from the 'traditional' client structure:</p>
<ul class="simple">
<li>There is only 1 client (HTTPClient) instance.  This takes the role similar to OpenStackClient's ClientManager class.  It handles the authentication for all APIs one time and contains the instances of the specific API client objects, which now are little more than containers for their Manager class instances.</li>
</ul>
</div>
<div class="section" id="namespace">
<h2>Namespace</h2>
<p>Everything lives under the top-level <tt class="docutils literal">openstack</tt> namespace</p>
<ul class="simple">
<li><tt class="docutils literal">openstack.restapi</tt> - The layer 1 transport and base classes (session, exceptions) and  base layer 2 classes (discovery, base clients, service catalog)</li>
<li><tt class="docutils literal">openstack.restapi.auth</tt> - The base authentication classes</li>
<li><tt class="docutils literal">openstack.restapi.identity</tt>  - API-specific classes required for layer 2 operation (identity client)</li>
<li><tt class="docutils literal">openstack.client.identity</tt> - The layer 2 classes for the Identity API (<tt class="docutils literal">.v2_0</tt>, <tt class="docutils literal">.v3</tt>)</li>
<li><tt class="docutils literal"><span class="pre">openstack.client.&lt;api&gt;</span></tt> - Other layer 2 API classes</li>
</ul>
</div>
<div class="section" id="openstack-restapi-session-session">
<h2>openstack.restapi.session.Session</h2>
<p>Session is the lowest layer, basically a wrapper that adds the following to requests.Session:</p>
<ul class="simple">
<li>create a new requests.Session instance if one is not supplied (using requests.Session implies the TLS control lies here and is one reason for passing in an existing Session)</li>
<li>populate the X-Auth-Token header from an auth object contained by the Session that implements a get_token() method</li>
<li>populate headers as required: User-Agent, Forwarded, Content-Type</li>
<li>change requests' redirect handling to be more appropriate for an API</li>
<li>include wrappers for the REST methods: head(), get(), post(), put(), delete(), patch()</li>
<li>debug logging of request/response data</li>
</ul>
</div>
<div class="section" id="openstack-restapi-baseclient-client">
<h2>openstack.restapi.baseclient.Client</h2>
<p>The base Client class defines the methods that reflect into the Session.</p>
<ul class="simple">
<li>create a new Session instance if one is not supplied</li>
<li>contains a ServiceCatalog instance (applications requiring multiple identity contexts at a time should use multiple Client instances)</li>
<li>performs the API version discovery (see ApiVersion class below)</li>
<li>define the cache interface for client-side caching of API data</li>
<li>include wrappers for the REST methods: head(), get(), post(), put(), delete(), patch()</li>
</ul>
</div>
<div class="section" id="openstack-restapi-base-baseauthplugin">
<h2>openstack.restapi.base.BaseAuthPlugin</h2>
<p>The abstract auth plugin class</p>
<blockquote>
<ul class="simple">
<li>handles the specifics of authenticating a user and providing a token to Session when requested via get_token()</li>
</ul>
</blockquote>
</div>
<div class="section" id="openstack-restapi-httpclient-httpclient-baseclient-client-base-baseauthplugin">
<h2>openstack.restapi.httpclient.HTTPClient(baseclient.Client, base.BaseAuthPlugin)</h2>
<p>HTTPClient is the primary interface used by the project API layers (gooey-creamy!).</p>
<ul class="simple">
<li>creates a ServiceCatalog from the token received from Identity</li>
<li>uses <tt class="docutils literal">keyring</tt> to cache tokens</li>
<li>authenticate() calls get_raw_token_from_identity_service()</li>
</ul>
</div>
<div class="section" id="openstack-restapi-access-accessinfo">
<h2>openstack.restapi.access.AccessInfo</h2>
<p>Base class for auth plugins</p>
<ul class="simple">
<li>defines the basic auth interface</li>
<li>AccessInfoV2</li>
<li>AccessInfoV3</li>
</ul>
</div>
</div>
<div class="section" id="layer-2-discovery">
<h1>Layer 2: Discovery</h1>
<p>Discovery rides just above the transport layer and is the logic used to determine the best API version available between those support by the server and the client.</p>
<div class="section" id="openstack-restapi-api-discovery-apiversion">
<h2>openstack.restapi.api_discovery.ApiVersion</h2>
<p>A resource class for API versions used by BaseVersion</p>
<ul class="simple">
<li>normalizes version information</li>
</ul>
</div>
<div class="section" id="openstack-restapi-api-discovery-baseversion">
<h2>openstack.restapi.api_discovery.BaseVersion</h2>
<p>The root class for API version discovery.</p>
<ul class="simple">
<li>queries API server for supported version information</li>
<li>normalizes both server and client versions</li>
<li>select the appropriate version from those availalble (if possible)</li>
</ul>
</div>
<div class="section" id="openstack-restapi-identity-client-identityversion-api-discovery-baseversion">
<h2>openstack.restapi.identity.client.IdentityVersion(api_discovery.BaseVersion)</h2>
<p>A Version discovery class that handles the peculiarities of Keystone</p>
<ul class="simple">
<li>optionally removes 'v2.0' from the auth_url to do proper discovery on old-style deployment configurations</li>
<li>normalizes the returned dict to remove the <tt class="docutils literal">values</tt> key</li>
</ul>
</div>
</div>
<div class="section" id="layer-2-authentication">
<h1>Layer 2: Authentication</h1>
</div>
<div class="section" id="layer-2-service-catalog">
<h1>Layer 2: Service Catalog</h1>
</div>
<div class="section" id="examples">
<h1>Examples</h1>
<div class="section" id="create-a-session-with-private-ca-certificates">
<h2>Create A Session With Private CA Certificates</h2>
<pre class="literal-block">
session = api_session.Session(
    verify=ca_certificate_file,
    user_agent=USER_AGENT,
)
</pre>
</div>
<div class="section" id="add-a-base-client">
<h2>Add A Base Client</h2>
<pre class="literal-block">
client = httpclient.HTTPClient(
    session=session,
    auth_url=&quot;https://localhost:5000&quot;,
    project_name=&quot;sez-me-street&quot;,
    username=&quot;bert&quot;,
    password=&quot;pidgeon&quot;,
)
</pre>
</div>
<div class="section" id="identity-version-discovery">
<h2>Identity Version Discovery</h2>
<pre class="literal-block">
# Supported Identity client classes
API_VERSIONS = {
    '2.0': 'keystoneclient.v2_0.client.Client',
    '3': 'keystoneclient.v3.client.Client',
}

ver = identity.client.IdentityVersion(
    clients=API_VERSIONS.keys(),
    auth_url=&quot;https://localhost:5000&quot;,
)
print &quot;client class: %s=%s&quot; % (ver.client_version.id, API_VERSIONS[ver.server_version.id])
</pre>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStackClient Plugins]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2014/01/14/openstackclient-plugins" />
    <id>http://hackstack.org/x/blog/2014/01/14/openstackclient-plugins</id>
    <updated>2014-01-14T01:14:00Z</updated>
    <published>2014-01-14T01:14:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstackclient" />
    <summary type="html"><![CDATA[OpenStackClient Plugins]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2014/01/14/openstackclient-plugins"><![CDATA[<div class="document">
<p>OpenStackClient (OSC) has been in my project queue for almost two years now.  It was Feb 2012 that I stayed up all night mucking about with something called DrStack with the goal of combining the then four OpenStack CLI binaries into a single command set.</p>
<p>OSC is the second major realization of that goal having a greatly improved internal command architecture courtesy of dhellmann's Cliff framework.  It also somehow got an informal blessing without becoming an official project, a status that it still carries.  We have a roadmap of where to go with that but that is a topic for another day.</p>
<p>Today we talk plugins!</p>
<p>Yes, I know, that is an overused term in OpenStack, where everything seems to be a plugin or an extension or optional in some way.  But I don't have anything better at the moment so plugin it is.</p>
<div class="section" id="openstackclient-plugins">
<h1>OpenStackClient Plugins</h1>
<p>OSC development has been a series of quiet periods interspersed with bouts of furious activity.  Fast-forward to last month (Dec 2013) and the introduction of a viable command plugin system for OSC first released in version 0.3.0.  The OSC side is documented in the <a class="reference external" href="http://docs.openstack.org/developer/python-openstackclient/plugins.html">OpenStackClient Developer Documentation</a>.</p>
<div class="section" id="goals">
<h2>Goals</h2>
<p>The previous OSC plugin mechanism was too naive and did not allow for adding client objects to the ClientManager. We needed to:</p>
<ul class="simple">
<li>define an initialization to add global options for API versions and whatnot (parser phase)</li>
<li>define an initialization function(s) to select an API version add an appropriate client to the ClientManager (client phase)</li>
</ul>
</div>
<div class="section" id="implementation">
<h2>Implementation</h2>
<p>As an exercise to validate the completeness of the plugin mechanism, the Compute, Image and Volume API commands were converted to initialize via the plugin mechanism.  The only difference from an external plugin is that they are included in the OSC repo.</p>
<p>The new plugin mechanism builds on the use of <a class="reference external" href="https://pypi.python.org/pypi/cliff‎">Cliff</a> to dynamically load the command classes and modifies the existing OSC <tt class="docutils literal">ClientManager</tt> to define the client objects to be instantiated at run-time.  This allows additional clients to insert themselves into the <tt class="docutils literal">ClientManager</tt>.</p>
<p>OSC looks for plugins that register their client class under <tt class="docutils literal">openstack.cli.extension</tt>.  The key is the API name and must be unique for all plugins, the value is the module name that contains the public initialization functions.</p>
<p>The initialization module is typically names <tt class="docutils literal"><span class="pre">&lt;project-name&gt;.client</span></tt>, although there is no technical requirement to follow this convention.  It was adopted as that was already the name of the modules used by the built-in API classes.</p>
<p>The initialization module must implement a set of constants that are used to identify the plugin and two functions that instantiate the actual API client class and define any global options required.</p>
</div>
</div>
<div class="section" id="python-oscplugin">
<h1>python-oscplugin</h1>
<p>Since most actual OSC plugins are not going to be part of the repo, we created a sample plugin in a stand-alone project to demonstrate the bits required.  <a class="reference external" href="https://github.com/dtroyer/python-oscplugin">python-oscplugin</a> began life as a <a class="reference external" href="https://github.com/openstack-dev/cookiecutter">cookiecutter</a> project (worth using to bootstrap a project in the OpenStack-way) and expanded to become a simple demonstration of an OSC command plugin.</p>
<p>So let's walk through the sample plugin to see how this works...</p>
<div class="section" id="plugin-initialization">
<h2>Plugin Initialization</h2>
<p>It all starts with the initialization module, here named <tt class="docutils literal">oscplugin.plugin</tt>, defining the rest of the identifiers used to locate plugin bits.  Naming this module is flexible, it just needs to be specified in the <tt class="docutils literal">openstack.cli.extension</tt> entry point group.</p>
<ul class="simple">
<li><tt class="docutils literal">API_NAME</tt> - A short string describing the API or command set name.  It is used in the entry point group name and is the key name in the <tt class="docutils literal">openstack.cli.extension</tt> group to identify the plugin.  Must be a valid Python identifier string.</li>
<li><tt class="docutils literal">API_VERSION_OPTION</tt> - An optional name of the API version attribute used in the global command-line options to specify an API version.  It will be used in <tt class="docutils literal">build_option_parser()</tt> if setting an API version is required.  Must be a valid Python identifier string.</li>
<li><tt class="docutils literal">API_VERSIONS</tt> - A dict mapping version strings to client class names.</li>
</ul>
<p>Two functions are required that perform the initialization work for the plugin.</p>
<ul class="simple">
<li><tt class="docutils literal">build_option_parser()</tt> - The top-level parser object is passed in and available to add plugin-specific options, usually an API version selector.</li>
<li><tt class="docutils literal">make_client()</tt> - Instantiate the actual client object taking in to consideration any version that may be specified.  The mapping of version to client class is handled here.  Also, any authentication or service selection the specific client requires is passed in here.</li>
</ul>
</div>
<div class="section" id="client-api">
<h2>Client API</h2>
<p><tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> contains its own equivalent to a client API object.  In this case it is just a placeholder as the <tt class="docutils literal">plugin</tt> commands do not have an external client library.  For most API clients this is the actual client class, such as <tt class="docutils literal">glanceclient.v2.client.Client</tt> for the Image v2 API.</p>
<p>There are cases where the API client class is insufficient for some reason and adaptations are required.  The Image v1 client is a good example.  The ImageManager class in <tt class="docutils literal">glanceclient</tt> does not have a <tt class="docutils literal">find()</tt> method so we implemented one in <tt class="docutils literal">openstackclient.image.client.Client_v1</tt> that uses <tt class="docutils literal">openstackclient.image.client.ImageManager_v1</tt> with added <tt class="docutils literal">find()</tt> and <tt class="docutils literal">findall()</tt> methods.</p>
</div>
<div class="section" id="commands">
<h2>Commands</h2>
<p>The commands implemented in <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> are in <tt class="docutils literal">oscplugin.v1.plugin</tt> and follow the basic pattern used by the other OSC command classes.  Again they are mostly placeholders here.</p>
</div>
<div class="section" id="tests">
<h2>Tests</h2>
<p>The structure of the tests also follows that of the existing OSC API commands.  They use <tt class="docutils literal">mock</tt> and fakes to perform unit tests on the command classes.</p>
</div>
<div class="section" id="project-stuff">
<h2>Project Stuff</h2>
<p>Ad <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> was created using <a class="reference external" href="https://github.com/openstack-dev/cookiecutter">cookiecutter</a> it includes the usual OpenStack features such as <tt class="docutils literal">pbr</tt> and friends.  The specific bits pertaining to an OSC plugin:</p>
<ul>
<li><p class="first"><tt class="docutils literal">setup.cfg</tt> - All of the plugin-specific content is in the <tt class="docutils literal">[entry_points]</tt> section:</p>
<pre class="literal-block">
[entry_points]
openstack.cli.extension =
    oscplugin = oscplugin.plugin

openstack.oscplugin.v1 =
    plugin_list = oscplugin.v1.plugin:ListPlugin
    plugin_show = oscplugin.v1.plugin:ShowPlugin
</pre>
</li>
</ul>
<p>Note that OSC defines the group name as <tt class="docutils literal"><span class="pre">openstack.&lt;api-name&gt;.v&lt;version&gt;</span></tt>
so the version should not contain the leading 'v' character.</p>
<ul class="simple">
<li><tt class="docutils literal">requirements.txt</tt> - We've added  <tt class="docutils literal">openstackclient</tt> as <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> is useless without it.  <tt class="docutils literal">keystoneclient</tt> is here too, while <tt class="docutils literal"><span class="pre">python-oscplugin</span></tt> does not require it, most OpenStack API clients will.  <tt class="docutils literal">cliff</tt> is also needed here.</li>
<li><tt class="docutils literal"><span class="pre">test-requirements.txt</span></tt> - <tt class="docutils literal">mock</tt> is required for testing.</li>
</ul>
</div>
<div class="section" id="a-note-about-versions">
<h2>A Note About Versions</h2>
<p>Internally OSC uses the convention <tt class="docutils literal">vXXX</tt> for version identifiers, where <tt class="docutils literal">XXX</tt> is a valid Python identifier in its own right (i.e., uses '_' rather than '.' internally).  OSC adds the leading 'v' so versions expressed in constant declarations should not include it.</p>
</div>
</div>
<div class="section" id="eot">
<h1>EOT</h1>
<p>The plugin structure should allow any base install of OSC to be extended simply by installing the desired client package.  Af of right now there are no other clients that implement the plugin, but that will be changing soon.  Film at eleven...</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <author>
      <name>dtroyer</name>
      <uri>http://hackstack.org/x/blog</uri>
    </author>
    <title type="html"><![CDATA[OpenStack Icehouse Developer Summit]]></title>
    <link rel="alternate" type="text/html" href="http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit" />
    <id>http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit</id>
    <updated>2013-11-10T11:10:00Z</updated>
    <published>2013-11-10T11:10:00Z</published>
    <category scheme="http://hackstack.org/x/blog" term="openstack" />
    <category scheme="http://hackstack.org/x/blog" term="devstack" />
    <summary type="html"><![CDATA[OpenStack Icehouse Developer Summit]]></summary>
    <content type="html" xml:base="http://hackstack.org/x/blog/2013/11/10/openstack-icehouse-developer-summit"><![CDATA[<div class="document">
<p>OpenStack has had a global reach since the early days but the Design Summits
have always been a US-based affair.  Last week we finally took the every-six-month
roadshow off-continent and ventured out to Hong Kong.
Of course the Conference is co-located and concurrent but I didn't make it to
any of those sessions this time and only knew it was there by going to lunch in
the expo hall and seeing some familiar vendor faces.</p>
<p>We begin with the projects most subject to my attention, DevStack, Grenade and
OpenStackClient.</p>
<div class="section" id="devstack">
<h1>DevStack</h1>
<p>This is the first summit where DevStack has program status and thus its own
track of two back-to-back sessions.  I hear russellb is jealous...</p>
<div class="section" id="new-bits">
<h2>New Bits</h2>
<p>The DevStack 'New Bits' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstacks-new-bits">EtherPad</a>)
was spent talking about a couple of the significant additions
to DevStack late in the Havana cycle.  I wrote about the
<a class="reference external" href="/x/blog/2013/09/07/devstack-local-config">local config</a>
work as it was being developed, the discussion in the session was primarily a Q&amp;A.
One bit that was covered was converting devstack-gate to use this form rather
than <tt class="docutils literal">localrc</tt>.</p>
<p>The other major DevStack addition is a plugin mechanism
to configure and start additional services without requiring changes to DevStack.
This is partially intended for new projects to be able to use DevStack for their
Jenkins testing without requiring them to be added to the DevStack repo.</p>
<p>This is an expansion of the existing hook into <tt class="docutils literal">extras.d</tt> that automagically
ran scripts at the end of <tt class="docutils literal">stack.sh</tt>.  These scripts are essentially
dispatchers as they are called multiple times from <tt class="docutils literal">stack.sh</tt>, <tt class="docutils literal">unstack.sh</tt> and
<tt class="docutils literal">clean.sh</tt>.  <a class="reference external" href="http://devstack.org/plugins.html">devstack.org</a> has an example of an
<tt class="docutils literal">extras.d</tt> dispatch script.</p>
<p>Savanna and Tempest have been converted to the plugin format with Marconi in progress.
Most of the remaining <a class="reference external" href="/x/blog/2013/09/05/openstack-seven-layer-dip-as-a-service/">layer 4</a>
projects should also be able to be converted to the plugin format.</p>
<p>Other highlights, some of which I intend to cover here in the future:</p>
<ul class="simple">
<li>bash8 - style testing for Bash scripts similar to hacking/pep8/flake8;
there is interest in this becoming a stand-alone project if it proves to be useful</li>
<li>DevStack tests - the addition of <tt class="docutils literal">run_tests.sh</tt> provides a familiar, if
deprecated, interface to running <tt class="docutils literal">bash8</tt> and other tests</li>
<li>exercises - the DevStack exercises are now unused in all gate testing except
Grenade's <em>base</em> phase.  As they are still generally useful outside the
gate test environment a new Jenkins job needs to be added to check them for bit rot.</li>
</ul>
</div>
<div class="section" id="distro-support">
<h2>Distro Support</h2>
<p>sdague led the DevStack 'Distro Support' session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-devstack-support">EtherPad</a>)
discussing distro supported status and what we need to do to bring the current
ones up to snuff and what might be required of new additions.</p>
<p>The primary requirement to adding the support tag is the ability to have it tested
in the DevStack gate.  Unfortunately, neither of the clouds that provide
test resources to our CI infrastructure
(HP Cloud and Rackspace Cloud Servers)
allow arbitrary images to be uploaded so only distros that have supported images
are able to be tested.  The third-party testing hooks might be able to be used
to mitigate some of this but the resources for that testing will need to be supplied.</p>
<p>There was also some discussion around projects getting supported status from DevStack.
A lot of this is a timing and process issue for incubation/integration process wanting
to see testing before graduation from those steps but not wanting to add projects
to DevStack that are not on that track.  The addition of the extras.d capability
for projects to be easily added to DevStack without modifying it goes a long way
toward setting up the needed testing to demonstrate the capability of the project and
team before actually adding it to the repo.</p>
<p>The flow will look like:</p>
<ul class="simple">
<li>third-party testing in StackForge will utilize the extras.d plugins to do the
required pre-incubation testing</li>
<li>after incubation, the project gets added to the DevStack repo (still utilizing the
plugin mech) and added to the gate as a requirement for graduation to integrated status.</li>
</ul>
</div>
</div>
<div class="section" id="grenade">
<h1>Grenade</h1>
<p>The Grenade session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-summit-qa-grenade">EtherPad</a>) focused mostly on expanding the test matrix
of <tt class="docutils literal">base</tt> and <tt class="docutils literal">target</tt> releases that need testing.  This includes tests from
stable releases to trunk and stable release updates as well as from
stable release updates to next stable or trunk.</p>
<p>A couple of new control variables need to be added:</p>
<ul class="simple">
<li>Need to be able to turn off the <tt class="docutils literal">db_sync</tt> operation for rolling upgrade
testing that is not able to do the long-running sync operation.</li>
<li>Need to designate services to not be upgraded, i.e. test everything new with the
old nova-compute (<tt class="docutils literal"><span class="pre">n-cpu</span></tt>).</li>
</ul>
<p>Adding more projects to Grenade is desirable, the conclusion on the initial set:</p>
<ul class="simple">
<li>Neutron is not ready; will not be considered for Grenade at least until it
is voting in the gate.</li>
<li>Ceilometer has no Tempest tests; in order to be added to Grenade it will also
need tests backported to Tempest <tt class="docutils literal">stable/havana</tt>.</li>
<li>Heat has few Tempest tests; is considered out of scope at this time.</li>
<li>Trove needs to have Tempest tests and a Grenade plan by graduation from incubation.</li>
</ul>
<p>There has also been some desire expressed to be able to use the upgrade scripts
outside of Grenade itself.  Right now they rely heavily on DevStack components,
the work to separate that is low priority, but contributions welcome as always.</p>
</div>
<div class="section" id="openstackclient">
<h1>OpenStackClient</h1>
<p>My favorite project returned to the regular session schedule in Hong Kong.  I
conducted our talk in Portland as an Unconference session partly because I really
just wanted to talk to the group of regular comitters to sort out a plan.  That
may have been short-sighted as the level of interest and contribution dropped off
sharply.  Oops.</p>
<p>This time around dhellmann offered an Oslo slot for OSC and I snapped it up as
that is probably the least ill-fitting track for it.  That had the side
effect of prompting the question of putting OSC under Oslo organizationally.
I am OK with that even though Oslo has traditionally been focused on
libraries and reusable code.  Another that has come up before would be to
treat it as a distinct project like Horizon.  We passed on that initially
in San Francisco as the consensus was that it was not large enough to warrant
that status, and that is still the case in my view.</p>
<p>In the session (<a class="reference external" href="https://etherpad.openstack.org/p/icehouse-oslo-openstack-client-update">EtherPad</a>) I reviewed the recent activities including the
0.2 release last July and the addition of unit test templates.</p>
<p>Implementation of the Objet API has begun, utilizing a new <tt class="docutils literal">restapi.py</tt> module
to perform the low-level <tt class="docutils literal">requests</tt> interface.  Why not just use swiftclient?
Good question, and at the time I was looking for an excuse to try out a
thinner approach to implementing the REST APIs.</p>
<p>I also have started work on API version detection, in parallel with a couple other
projects.  I see this as mostly a platform for testing approaches and to
free the client from requiring versions in the service catalog.</p>
<p>Future work will look into Jamie's Keystone auth refactor and leverage that
as the common REST library.  Segue...</p>
</div>
<div class="section" id="keystone-client">
<h1>Keystone Client</h1>
<p>The Keystone core devs were in the OSC session and strongly suggested I come to
their Keystone client sessions on Wednsday afternoon, which I was planning to do
anyway so my arm remained undamanged.  I finally met Jamie Lennox, who has been
doing a lot of work refactoring the auth bits of the client lib and absorbing
much of the bits Alessio started a whil eback and proposed to Oslo last May.</p>
<p>I liked most of what I heard and liked it even better after Jamie straightened out
some of my confusion-because-of-lack-of-source-code-reading at dinner Friday night.
I think we are on the same page to create the one client to rule them all and just
need to tune some details that are likely to appear after the post-summit haze clears.
And while this space doesn't officially speak for the projects I am core on, because
this is essentially my brain-dump space you, dear reader, get an advance look at
what is likely to be proposed sooner than later.</p>
<p>One CLI, one core^H^H^H^Hintegrated^H^H^H^H^H^H^Hbasic API lib, user-pluggable additional
API libs.  I see it like this:</p>
<ul class="simple">
<li>python-openstackclient - continues to be a single project focused on an ultra-consistent
command line interface; directly consumes:</li>
<li>python-os-identityclient - a new Identity API library born out of Jamie's refactoring
auth/session work with a new library API that doesn't even try to be compatible with
the old stuff.  No cli, speaks Identity v2 and v3, directly usable by all other libraries and
projects to handle authenticated communication to openStack APIs.</li>
<li>python-XXXclient - TBD how the division of the other API libraries fall out.  I want
to minimize the number of moving parts for most users and not have the higher-level
optional projects impose an undue burden on dependencies.</li>
</ul>
</div>
<div class="section" id="other-bits">
<h1>Other Bits</h1>
<p>All-in-all it was a good week, including multiple trips into the city for
sight-seeing, street-level eating, parties, 102nd story eating, death-marches down
Nathan Road in search of (open) Starbucks,
you know, all the usual stuff.  Breakfast in the airport every morning (Maxim's Deluxe
sticky-top cheese buns rule).  Catching up with team-mates over non-IRC channels.
Wondering WTF happened to jeblair's hat (my bet is HK customs impounded it,
even though afazekas managed to smuggle in his red fedora).  Wondering if Vishy and Termie
survived Macau without going broke the first night.</p>
<p>The OSF board finalized the intent to agree on an agreement on the definition
of <tt class="docutils literal">core</tt> and how it is a totally overloaded word in the OpenStack world.
Wait, I may have dreamed part of that...or all of it.  Anyway, the usage of
<a class="reference external" href="20130905-open-stack-layers.rst">layers</a> when describing
the technical relationships of the projects seems to be catching on, I heard
it at least once outside the sessions where I used it.</p>
<p>And so the OpenStack March on Atlanta begins.  I have a hunch the city will
fare better next May than it did when General Sherman came for a visit back
in the day.  And I will forever hope that there will be more carbonated
caffiene.  I think Pepsi would be a fine choice given the locale, Mountain Dew
even.  In Coke's back yard, yeah, right.</p>
<p>It is too bad we're not
coming up to the 'S' release, I'd lobby for calling it Savannah just to enjoy
watching people trying to keep track of the Savanna Savannah release.  Or
would that be the Savannah Savanna release?  See, the fun we could have!</p>
<p>'J': Not Jacksonville, they are both in the wrong state and I don't want to
type that many letters.  Let's start a campaign for 'Joyland'!</p>
</div>
</div>
]]></content>
  </entry>
</feed>
